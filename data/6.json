{
    "600": {
        "file_id": 71,
        "content": "docker_cmds.append(f\"open -j -a {MACOS_DOCKER_APP_BINARY}\")\n    start_docker_cmds.append(\"open -a Docker\")\n    start_docker_cmds.append(f\"open -a {MACOS_DOCKER_APP_BINARY}\")\n    def hide_docker():\n        HIDE_DOCKER_ASCRIPT_OBJ.run()\n    def check_if_docker_window_exists():\n        exist = any([WINDOW_TITLE_KW in t for t in pygetwindow.getAllTitles()])\n        return exist\nelse:\n    raise Exception(f\"Unknown platform: {sysname}\")\ndef kill_docker():\n    for cmd in kill_docker_cmds:\n        execute_os_command_and_assert_safe_exit(cmd, kill_safe_codes)\ndef start_docker():\n    for cmd in start_docker_cmds:\n        execute_os_command_and_assert_safe_exit(cmd, start_safe_codes)\nDOCKER_KILLED_KWS = [\n    \"the docker daemon is not running\",\n    \"Cannot connect to the Docker daemon\",\n    \"error during connect\",\n]\ndef verify_docker_killed(timeout=5, encoding=\"utf-8\", inverse: bool = False):\n    output = (\n        subprocess.Popen(\n            [\"docker\", \"ps\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n   ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:120-157"
    },
    "601": {
        "file_id": 71,
        "content": "This code appears to be for controlling the Docker daemon. It includes functions to start, stop, and check if the Docker window is open. The code also handles different platforms (possibly MacOS and others) by appending specific commands or raising an exception for unknown platforms. There are also lists of keywords to recognize when checking if Docker has been killed or not.",
        "type": "comment"
    },
    "602": {
        "file_id": 71,
        "content": "     .communicate(timeout=timeout)[1]\n        .decode(encoding)\n    )\n    killed = any([kw in output for kw in DOCKER_KILLED_KWS])\n    if not inverse:\n        if not killed:\n            raise Exception(\n                f\"Docker not killed.\\nCaptured output from command `docker ps`:\\n{output}\"\n            )\n    else:\n        if killed:\n            raise Exception(\n                f\"Docker not started.\\nCaptured output from command `docker ps`:\\n{output}\"\n            )\nimport time\ndef verify_docker_launched(retries=10, sleep=3, daemon=False):\n    success = False\n    for i in range(retries):\n        try:\n            if not daemon:\n                exist = check_if_docker_window_exists()\n                print(f\"window exists? {exist}\")\n            else:\n                exist = False\n            if not exist:\n                verify_docker_killed(inverse=True)\n            success = True\n            break\n        except Exception as e:\n            if i < retries - 1:\n                print(f\"Retrying in {sleep} seconds...\")\n",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:157-192"
    },
    "603": {
        "file_id": 71,
        "content": "This code checks if Docker is running and handles the case when it's not running or when it's a daemon. It attempts to verify the Docker launch multiple times, with a specified number of retries and sleep time between each retry. If it fails after all retries, it raises an exception.",
        "type": "comment"
    },
    "604": {
        "file_id": 71,
        "content": "                time.sleep(sleep)\n            else:\n                raise e\n    return success\ndef restart_docker():\n    check_required_binaries()\n    print(\"prerequisites checked\")\n    kill_docker()\n    print(\"docker killed\")\n    verify_docker_killed()\n    print(\"kill has been verified\")\n    start_docker()\n    print(\"docker restarted\")\nimport shutil\ndef check_required_binaries():\n    for name in REQUIRED_BINARIES:\n        resolved_path = shutil.which(name)\n        assert resolved_path, f\"{name} is not available in PATH.\"\n        assert os.path.exists(\n            resolved_path\n        ), f\"{name} does not exist.\\nfilepath: {resolved_path}\"\n        print(f\"'{name}' found\")\n# working!\ndef restart_and_verify():\n    # this could be faulty! still stuck even if docker is killed on macOS\n    restart_docker()\n    if sysname in [\"Windows\", \"Darwin\"]:\n        verify_docker_launched()\n        print(\"docker restart verified\")\n        hide_docker()\n        print(\"docker window minimized\")\n    verify_docker_launched(daemon=True)\n",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:192-232"
    },
    "605": {
        "file_id": 71,
        "content": "The code attempts to check if required binaries are available in the PATH and restart Docker. It first checks for the existence of each binary, raises an error if any is missing, and then restarts and verifies the Docker daemon. If running on Windows or macOS, it also hides the Docker window after verification.",
        "type": "comment"
    },
    "606": {
        "file_id": 71,
        "content": "    print(\"docker daemon restart verified\")\nif elevate_needed:\n    elevate.elevate(graphical=False)\nif __name__ == \"__main__\":\n    # kill & perform checks if you really have killed docker.\n    # restart & check if restart is successful.\n    # do it once more.\n    for i in range(2):\n        print(f\"trial #{i}\")\n        restart_and_verify()\n        time.sleep(3)  # m1 is running too damn fast. or is it?",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:232-245"
    },
    "607": {
        "file_id": 71,
        "content": "This code is checking the restart and successful operation of the Docker daemon. It restarts the daemon twice, performs checks, and waits for 3 seconds between trials to ensure stable results.",
        "type": "comment"
    },
    "608": {
        "file_id": 72,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/sequence_learner.py",
        "type": "filepath"
    },
    "609": {
        "file_id": 72,
        "content": "The code presents a predictor class for sequences using NaivePredictor and PredictorWrapper. It initializes the PredictorWrapper with a NaivePredictor and enqueues a sequence [0, 1, 2, 3, 4]. The next 100 tokens are predicted based on sequence length of 10.",
        "type": "summary"
    },
    "610": {
        "file_id": 72,
        "content": "from collections import deque\nfrom typing import List\nimport numpy as np\nclass NaivePredictor:\n    def __init__(self, ksize: int):\n        self.ksize = ksize\n        self.kernel = np.random.rand(self.ksize)\n    def predict(self, x: List[int]):\n        x_processed = x[-self.ksize :]\n        if len(x_processed) < self.ksize:\n            x_processed = [0] * (self.ksize - len(x_processed)) + x_processed\n        x_one_hot = self.one_hot(x_processed)\n        next_token = np.matmul(x_one_hot, self.kernel)\n        ret = np.argmax(next_token)\n        return ret\n    def one_hot(self, x):\n        x_one_hot = np.eye(self.ksize)[x]\n        return x_one_hot\nclass PredictorWrapper:\n    def __init__(self, ksize: int, predictor_cls: NaivePredictor):\n        self.predictor: NaivePredictor = predictor_cls(ksize)\n        self.seq = deque([], maxlen=ksize)\n    def enqueue(self, seq: List[int]):\n        for tok in seq:\n            self.seq.append(tok)\n    def predict(self, seqlen: int):\n        ret_seq = []\n        for _ in range(seq",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/sequence_learner.py:1-37"
    },
    "611": {
        "file_id": 72,
        "content": "Code describes a class that implements a predictor for sequences, using a NaivePredictor and PredictorWrapper. The NaivePredictor uses a kernel to predict the next token in the sequence based on the last ksize elements of the input list. The PredictorWrapper maintains a deque with a maximum length of ksize and provides a method to enqueue new sequences, as well as a method to predict the next token for a given sequence length.",
        "type": "comment"
    },
    "612": {
        "file_id": 72,
        "content": "len):\n            tok = self.predictor.predict(list(self.seq))\n            self.seq.append(tok)\n            ret_seq.append(tok)\n        return ret_seq\nif __name__ == \"__main__\":\n    pw = PredictorWrapper(10, NaivePredictor)\n    pw.enqueue([0, 1, 2, 3, 4])\n    total_seq = pw.predict(100)\n    # ksize = 10\n    # predictor = NaivePredictor(ksize=ksize)\n    # seq = deque([0, 1, 2, 3, 4], maxlen=ksize)\n    # total_seq = list(seq)\n    # for _ in range(100):\n    #     tok = predictor.predict(list(seq))\n    #     seq.append(tok)\n    #     total_seq.append(tok)\n    print(\"total seq:\", total_seq)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/sequence_learner.py:37-56"
    },
    "613": {
        "file_id": 72,
        "content": "This code initializes a PredictorWrapper object with a NaivePredictor and enqueues a sequence [0, 1, 2, 3, 4]. It then predicts the next tokens for the next 100 steps using the wrapped predictor and prints the total sequence. The naive predictor seems to be used for prediction based on the input sequence length of 10.",
        "type": "comment"
    },
    "614": {
        "file_id": 73,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/test_beat_server.py",
        "type": "filepath"
    },
    "615": {
        "file_id": 73,
        "content": "Testing the beat server using unittest. Can also test killing server or normal clients. Imports from beat_common module.",
        "type": "summary"
    },
    "616": {
        "file_id": 73,
        "content": "# unittest for our damn beat server. so damn error prone.\n# you can also do unittest for kill server or normal clients.\nfrom beat_common import *",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/test_beat_server.py:1-3"
    },
    "617": {
        "file_id": 73,
        "content": "Testing the beat server using unittest. Can also test killing server or normal clients. Imports from beat_common module.",
        "type": "comment"
    },
    "618": {
        "file_id": 74,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/timeout_utils.py",
        "type": "filepath"
    },
    "619": {
        "file_id": 74,
        "content": "Imports libraries for timeouts and retrying. Defines timeout_func decorator for functions with no arguments and a nested retrying_timeout_func decorator for retrying and timeouts. Includes the \"__all__\" variable to export these functions.",
        "type": "summary"
    },
    "620": {
        "file_id": 74,
        "content": "import functools\nimport func_timeout\nimport retrying\n# let's not use multitasking\n# timeout decorator for func with no arg/kwarg\ntimeout_func = lambda timeout: (\n    lambda func: functools.partial(\n        func_timeout.func_timeout, timeout=timeout, func=func\n    )\n)\nretrying_timeout_func = lambda retry_max_count, timeout: (\n    lambda func:\n    # lambda func: multitasking.task(\n    retrying.retry(stop_max_attempt_number=retry_max_count)(timeout_func(timeout)(func))\n    # )\n)\n__all__ = [\"timeout_func\", \"retrying_timeout_func\"]",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/timeout_utils.py:1-22"
    },
    "621": {
        "file_id": 74,
        "content": "Imports libraries for timeouts and retrying. Defines timeout_func decorator for functions with no arguments and a nested retrying_timeout_func decorator for retrying and timeouts. Includes the \"__all__\" variable to export these functions.",
        "type": "comment"
    },
    "622": {
        "file_id": 75,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/to_be_killed.py",
        "type": "filepath"
    },
    "623": {
        "file_id": 75,
        "content": "This code imports os and time modules, gets the process ID (pid), and continuously prints the pid every second until terminated by os.kill().",
        "type": "summary"
    },
    "624": {
        "file_id": 75,
        "content": "import os\nimport time\npid = os.getpid()\n# killed by os.kill!\nwhile True:\n    print(\"process pid:\", pid)\n    time.sleep(1)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/to_be_killed.py:1-9"
    },
    "625": {
        "file_id": 75,
        "content": "This code imports os and time modules, gets the process ID (pid), and continuously prints the pid every second until terminated by os.kill().",
        "type": "comment"
    },
    "626": {
        "file_id": 76,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/type_utils.py",
        "type": "filepath"
    },
    "627": {
        "file_id": 76,
        "content": "These functions ensure the input is either a bytes or string type, and raise exceptions if it's not.",
        "type": "summary"
    },
    "628": {
        "file_id": 76,
        "content": "def enforce_bytes(s):\n    if isinstance(s, str):\n        s = s.encode()\n    if not isinstance(s, bytes):\n        raise Exception(\"unknown line content type:\", type(s))\n    return s\ndef enforce_str(content):\n    if isinstance(content, bytes):\n        content = content.decode()\n    if not isinstance(content, str):\n        raise Exception(\"Invalid content type: %s\\nShould be: str\" % type(content))\n    return content",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/type_utils.py:1-14"
    },
    "629": {
        "file_id": 76,
        "content": "These functions ensure the input is either a bytes or string type, and raise exceptions if it's not.",
        "type": "comment"
    },
    "630": {
        "file_id": 77,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/vocabulary.py",
        "type": "filepath"
    },
    "631": {
        "file_id": 77,
        "content": "NaiveVocab class generates a random string of characters from a given list and AsciiVocab/BytesVocab inherit from NaiveVocab to handle both ASCII and bytes content respectively.",
        "type": "summary"
    },
    "632": {
        "file_id": 77,
        "content": "import random\nfrom type_utils import *\nclass NaiveVocab:\n    charlist = [\"a\"]\n    startpoint = \"\"\n    content_typeguard = enforce_str\n    @classmethod\n    def generate(cls):\n        content = cls.startpoint\n        for _ in range(random.randint(1, 10)):\n            char = random.choice(cls.charlist)\n            content += char\n        return content\n    @classmethod\n    def filter(cls, content):\n        content = cls.content_typeguard(content)\n        result = cls.startpoint\n        for char in content:\n            if char in cls.charlist:\n                result += char\n        return result\nclass AsciiVocab(NaiveVocab):\n    charlist = [chr(x) for x in range(256)]\nclass BytesVocab(NaiveVocab):\n    startpoint = b\"\"\n    charlist = [bytes([x]) for x in range(256)]\n    content_typeguard = enforce_bytes",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/vocabulary.py:1-36"
    },
    "633": {
        "file_id": 77,
        "content": "NaiveVocab class generates a random string of characters from a given list and AsciiVocab/BytesVocab inherit from NaiveVocab to handle both ASCII and bytes content respectively.",
        "type": "comment"
    },
    "634": {
        "file_id": 78,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/wexpect_example.py",
        "type": "filepath"
    },
    "635": {
        "file_id": 78,
        "content": "Starts a cmd child process, waits for prompt, sends commands, prints output, and exits.",
        "type": "summary"
    },
    "636": {
        "file_id": 78,
        "content": "import wexpect\n# Start cmd as child process\nchild = wexpect.spawn(\"cmd.exe\")\n# Wait for prompt when cmd becomes ready.\nchild.expect(\">\")\n# Prints the cmd's start message\nprint(\"before\", child.before)\nprint(\"after\", child.after)\n# run list directory command\nchild.sendline(\"ls\")\n# Waiting for prompt\nchild.expect(\">\")\n# Prints content of the directory\nprint(child.before, end=\"\")\nprint(child.after, end=\"\")\n# Exit from cmd\nchild.sendline(\"exit\")\n# Waiting for cmd termination.\nchild.wait()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/wexpect_example.py:1-27"
    },
    "637": {
        "file_id": 78,
        "content": "Starts a cmd child process, waits for prompt, sends commands, prints output, and exits.",
        "type": "comment"
    },
    "638": {
        "file_id": 79,
        "content": "/binary_program_synthesis_cpu_assembly_execution/README.md",
        "type": "filepath"
    },
    "639": {
        "file_id": 79,
        "content": "Idea: Programs ingest and evolve via evolutionary process, directly executing assembly code without human interface. Interaction through isolated environments like a computer virus. Consider hierarchical tokenizer or embedding for memory efficiency.",
        "type": "summary"
    },
    "640": {
        "file_id": 79,
        "content": "I had an idea that programs shall ingest programs and learn to evolve in a evolutionary manner.\nThe program shall directly execute assembly code. So obviously, no human interface is needed.\nYou may ask how do we interact with such program? Consider computer virus. We first run it in isolated environments, then we interact.\n---\nyou may make hierarchical tokenizer or hierarchical embedding to reduce memory consumption",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/README.md:1-9"
    },
    "641": {
        "file_id": 79,
        "content": "Idea: Programs ingest and evolve via evolutionary process, directly executing assembly code without human interface. Interaction through isolated environments like a computer virus. Consider hierarchical tokenizer or embedding for memory efficiency.",
        "type": "comment"
    },
    "642": {
        "file_id": 80,
        "content": "/binary_program_synthesis_cpu_assembly_execution/bnn_data_ingest.py",
        "type": "filepath"
    },
    "643": {
        "file_id": 80,
        "content": "The code imports libraries, defines the `makeQuant` function to create a quantizer layer, and initializes variables. It generates a random tensor, applies embedding and quantization layers, performs matrix multiplication and softmax operation for attention, and outputs binary values from the quantized results while printing intermediate outputs.",
        "type": "summary"
    },
    "644": {
        "file_id": 80,
        "content": "import larq  # keras/tensorflow\n# ref: https://github.com/itayhubara/BinaryNet.pytorch\nimport tensorflow as tf\nimport random\n# tf.experimental.numpy.experimental_enable_numpy_behavior()\ndim = 2\n# dim = 10\ndef makeQuant(in_dim, out_dim):\n    layer = larq.layers.QuantDense(  # this will regulate all values into integers\n        units=out_dim,  # anything -> 2\n        # units=dim,\n        input_quantizer=larq.quantizers.SteSign(clip_value=1.0),\n        kernel_quantizer=larq.quantizers.SteSign(clip_value=1.0),\n        kernel_constraint=larq.constraints.WeightClip(clip_value=1.0),\n        # input_shape=(42,), # still working? not in sequential.\n        input_shape=(in_dim,),\n    )\n    return layer\n# larq.layers.QuantDense(units=2)\n# dim = 1024\n# model = tf.keras.Sequential()\nseqlen = 20\n# (AB * BA) * (BA * AB)\n# random_x_in = [[random.randint(0, 1) for _ in range(seqlen)]]\nrandom_x_in = [[random.randint(0, 1) for _ in range(seqlen)]]\nx_in = tf.convert_to_tensor(random_x_in, dtype=tf.float32)  # [1, 20]\n# x_in = tf.r",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/bnn_data_ingest.py:1-37"
    },
    "645": {
        "file_id": 80,
        "content": "The code imports necessary libraries and defines a function `makeQuant` that creates a quantizer layer for regulating input and kernel values to integers. It also initializes variables, including the dimension (`dim`) and sequence length (`seqlen`), and creates a tensor `x_in` with random integer data.",
        "type": "comment"
    },
    "646": {
        "file_id": 80,
        "content": "andom.uniform(shape=(1, dim), minval=0, maxval=2, dtype=tf.float32)\nl_emb = tf.keras.layers.Embedding(2, dim)\nt_emb = l_emb(x_in)\nl_q = makeQuant(dim, dim)\nl_k = makeQuant(dim, dim)\nl_v = makeQuant(dim, dim)\nt_q = l_q(t_emb)\nt_k = l_k(t_emb)\nt_v = l_v(t_emb)\nt_att_pre = tf.matmul(t_k, t_q, transpose_a=True)\nt_att = tf.nn.softmax(t_att_pre, axis=2) / (dim**0.5)\nt_feat = tf.matmul(t_v, t_att)\nl_out = makeQuant(dim, 2)\nt_out = l_out(t_feat)\nprint(x_in)\nprint()\nprint(t_out)  # not within 1 and 0\nbinary = tf.argmax(t_out, axis=2)\nprint()\nprint(binary)\nprint(binary.shape, t_out.shape)\n# breakpoint()",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/bnn_data_ingest.py:37-65"
    },
    "647": {
        "file_id": 80,
        "content": "This code generates a random tensor, applies embedding and quantization layers to it, performs matrix multiplication and softmax operation for attention, and finally outputs binary values from the quantized results. The code also prints these intermediate outputs for debugging purposes.",
        "type": "comment"
    },
    "648": {
        "file_id": 81,
        "content": "/binary_program_synthesis_cpu_assembly_execution/convert_binary_files_as_zero_and_one_streams.py",
        "type": "filepath"
    },
    "649": {
        "file_id": 81,
        "content": "This code reads a binary file, converts it to a stream of zeroes and ones, groups the bits into 8-bit groups (bytes), and writes the result in text format with one byte per line.",
        "type": "summary"
    },
    "650": {
        "file_id": 81,
        "content": "# binaries? this reminds me of binary neural networks. whoa there!\n# import numpy as np\nfilepath = \"README.md\"\noutput_path = \"converted_binary_view.txt\"\n# import binascii\n# this can do the job of hex\nimport itertools\nwith open(filepath, \"rb\") as f:\n    _bytes = f.read()\n    # char = f.read(1)\n    # arr = np.frombuffer(char, dtype=np.bool_) # this will not work.\n    # arr = np.frombuffer(char, dtype=np.uint8)\n    # arr = np.array(list(char)).astype(np.uint8)\n    # print(arr)\n    # print(char)\n    # bin_array = np.unpackbits(arr)\n    # bin_array = np.unpackbits(arr).astype(np.bool_)\n    # print(bin_array)  # [False  True False False  True False False  True]\n    bit_repr = [format(it, '08b') for it in _bytes]\n    line_list = []\n    for index, bit_group in itertools.groupby(enumerate(bit_repr), key=lambda x: x[0]//8):\n        line = \" \".join([it for _, it in bit_group])\n        line_list.append(line)\n    with open(output_path, 'w+') as fw:\n        fw.write(\"\\n\".join(line_list))",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/convert_binary_files_as_zero_and_one_streams.py:1-27"
    },
    "651": {
        "file_id": 81,
        "content": "This code reads a binary file, converts it to a stream of zeroes and ones, groups the bits into 8-bit groups (bytes), and writes the result in text format with one byte per line.",
        "type": "comment"
    },
    "652": {
        "file_id": 82,
        "content": "/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py",
        "type": "filepath"
    },
    "653": {
        "file_id": 82,
        "content": "The code imports necessary libraries, initializes a GPT2 model, defines functions for token building and generation, generates binary input, masks attention mask, adds custom special tokens to tokenizer, resizes token embeddings, trains the model by masking logits during forward pass, calculates loss using masked logits, computes gradients and optimizes them, and uses a symbol/reference variable for future reference.",
        "type": "summary"
    },
    "654": {
        "file_id": 82,
        "content": "#!/usr/bin/env python\n# coding: utf-8\n# In[34]:\n# a good reference:\n# https://blog.paperspace.com/generating-text-summaries-gpt-2/\nget_ipython().system('pip3 install einops')\nimport einops\nimport transformers\nimport torch\nMODEL_NAME = 'gpt2'\nmodel = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nlr = 0.0001\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nloss_fn = torch.nn.CrossEntropyLoss()\ndef build_special_token(location:str, name:str):\n    return f\"<|{location}_{name}|>\"\ndef generate_special_token_pair(name:str):\n    begin_token = build_special_token('begin', name)\n    end_token = build_special_token('end', name)\n    return begin_token, end_token\ntext = \"8e7d4f\"\n# text = \"0100100010010\"\nenc = tokenizer([text], return_tensors='pt')\ninput_ids = enc['input_ids'] # into three pieces only.\nattention_mask = torch.ones(input_ids.shape, dtype=torch.int64)\ninput_ids.shape\nbegin_bytes, end_bytes = generate_special_token_pair('bytes')\n",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py:1-39"
    },
    "655": {
        "file_id": 82,
        "content": "This code imports necessary libraries, initializes a GPT2 language model, and defines functions for building special tokens and generating token pairs. It also generates a binary input and masks the attention mask to be all ones.",
        "type": "comment"
    },
    "656": {
        "file_id": 82,
        "content": "# how to step in and out?\ntokenizer(begin_bytes)['input_ids'], tokenizer(end_bytes)['input_ids']\n# In[35]:\n# print(dir(tokenizer))\n# print(tokenizer.all_special_tokens)\n# help(tokenizer.add_tokens)\ntokenizer.add_tokens([begin_bytes, end_bytes]) # will not do this again.\n# tokenizer.add_special_tokens({\"begin_bytes\": begin_bytes, \"end_bytes\":end_bytes})\n# In[36]:\n# add new special token to tokenizer\nlen(tokenizer)\n# In[37]:\nmodel.resize_token_embeddings(len(tokenizer))\n# In[38]:\n# dir(tokenizer)\n# tokenizer.vocab\n# binary_vocab = {i: format(i, '04b') for i in range(16)}\n# binary_map = {v: tokenizer.vocab[v] for _, v in binary_vocab.items()}\n# missing: 0011\nhex_vocab = {i: format(i, '0x') for i in range(16)}\nhex_map = {v: tokenizer.vocab[v] for _, v in hex_vocab.items()}\n# hex_map\n# In[39]:\nbyte_vocab = {i: str(i) for i in range(256)}\nbyte_map = {v: tokenizer.vocab[v] for _, v in byte_vocab.items()}\n# In[46]:\noutput.logits.shape # now: 50259\n# <|begin_bytes|>feffd3d7ea<|end_bytes|>\n# <|begin_hex|>feffd3d7ea<|end_he",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py:39-94"
    },
    "657": {
        "file_id": 82,
        "content": "This code is adding custom special tokens to a tokenizer, mapping binary and hexadecimal values to existing tokens in the tokenizer's vocabulary. This allows the model to process input data that includes binary or hexadecimal information. The code then resizes the token embeddings based on the updated vocabulary size and logs some shapes of output arrays for further analysis.",
        "type": "comment"
    },
    "658": {
        "file_id": 82,
        "content": "x|>\n# .............##########[#..........] <- in training we only mask some prob\n# .............####################### <- in inference/parsing there could be state rolling back\n# In[41]:\n# training\noutput = model(input_ids = input_ids, attention_mask = attention_mask)\n# output.logits[:,:,:] = 0 # this will not affect loss\nmasked_logits = torch.zeros(output.logits.shape)\nfocused_ids = [10,20,30]\nmasked_logits[:,:,focused_ids] = output.logits[:,:,focused_ids] # this will\nzero_input_ids = torch.zeros(input_ids.shape, dtype=input_ids.dtype)\n# output.logits\nreshaped_original_logits = einops.rearrange(output.logits, \"b s c -> b c s\")\nreshaped_logits = einops.rearrange(masked_logits, \"b s c -> b c s\")\nloss = loss_fn(reshaped_original_logits, zero_input_ids)\n# loss = loss_fn(reshaped_logits, zero_input_ids)\nprint(loss.item()) # it would be the same as long as setting to zero.\n# In[42]:\nmasked_logits[:,:,focused_ids]\n# In[43]:\nmodel.zero_grad()\n# In[44]:\nloss.backward()\noptimizer.step()\nmodel.zero_grad()\n# In[45]:\n# in",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py:94-141"
    },
    "659": {
        "file_id": 82,
        "content": "This code is training a model by masking some logits during the forward pass. It first creates a tensor of zeros for the masked logits and selects specific elements to preserve from the original output logits. Then, it calculates the loss using these masked logits. The loss is then printed, and the gradients are computed and optimized.",
        "type": "comment"
    },
    "660": {
        "file_id": 82,
        "content": "ference",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py:141-141"
    },
    "661": {
        "file_id": 82,
        "content": "This code snippet seems to be a reference variable or symbol, possibly indicating a memory location or value that will be used later in the program.",
        "type": "comment"
    },
    "662": {
        "file_id": 83,
        "content": "/binary_program_synthesis_cpu_assembly_execution/interpret_and_save_binary_program.py",
        "type": "filepath"
    },
    "663": {
        "file_id": 83,
        "content": "Reads binary program from file, converts to integer list, and writes to output file.",
        "type": "summary"
    },
    "664": {
        "file_id": 83,
        "content": "# filepath = \"binary_program_windows_bin.txt\"\nfilepath = \"binary_program.txt\"\n# output_path = \"program.bin\"\n# output_path = \"program.exe\"\noutput_path = \"program\" # linux program?\nbyte_int_list = []\nwith open(filepath, 'r') as f:\n    content = f.read()\n    byte_list = content.replace(\"\\n\", ' ').split()\n    for byte_str in byte_list:\n        byte_int = int(byte_str, 2)\n        byte_int_list.append(byte_int)\nbinary_bytes = bytes(byte_int_list)\nwith open(output_path, 'wb') as f:\n    f.write(binary_bytes)\n# import os\n# os.system(output_path)",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/interpret_and_save_binary_program.py:1-20"
    },
    "665": {
        "file_id": 83,
        "content": "Reads binary program from file, converts to integer list, and writes to output file.",
        "type": "comment"
    },
    "666": {
        "file_id": 84,
        "content": "/binary_program_synthesis_cpu_assembly_execution/softmax_test.py",
        "type": "filepath"
    },
    "667": {
        "file_id": 84,
        "content": "Computing 2D softmax activation for random input tensor of shape (2,3).",
        "type": "summary"
    },
    "668": {
        "file_id": 84,
        "content": "import torch\nimport torch.nn as nn\nm = nn.Softmax(dim=1)\ninput = torch.randn(2, 3, requires_grad=True)\noutput = m(input)\nprint(output)",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/softmax_test.py:1-7"
    },
    "669": {
        "file_id": 84,
        "content": "Computing 2D softmax activation for random input tensor of shape (2,3).",
        "type": "comment"
    },
    "670": {
        "file_id": 85,
        "content": "/binary_program_synthesis_cpu_assembly_execution/vkq_bin.py",
        "type": "filepath"
    },
    "671": {
        "file_id": 85,
        "content": "The VKQAttention class implements attention mechanism using linear layers, while the code defines binary quantization configuration and converts model to binary format. The 'bout' array shape is stored but not printed.",
        "type": "summary"
    },
    "672": {
        "file_id": 85,
        "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass VKQAttention(nn.Module):\n    def __init__(self, input_dim):\n        super(VKQAttention, self).__init__()\n        self.input_dim = input_dim\n        self.value = nn.Linear(input_dim, input_dim)\n        self.key = nn.Linear(input_dim, input_dim)\n        self.query = nn.Linear(input_dim, input_dim)\n        self.softmax = nn.Softmax(dim=2)\n        self.dk = self.input_dim\n    def forward(self, x):\n        values = self.value(x)\n        keys = self.key(x)\n        queries = self.query(x)\n        # scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.dk**0.5)\n        # scores = torch.bmm(keys.transpose(1, 2), queries) / (self.dk**0.3)  # not 2!\n        scores = torch.bmm(keys.transpose(1, 2), queries) / (self.dk**0.5)\n        attention = self.softmax(scores)\n        # print(attention)\n        # print(scores)\n        # breakpoint()\n        # weighted = torch.bmm(scores, values)\n        # weighted = torch.bmm(attention, values)\n        weight",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/vkq_bin.py:1-29"
    },
    "673": {
        "file_id": 85,
        "content": "This code defines a VKQAttention class that implements an attention mechanism using linear layers for value, key, and query calculations. The input_dim parameter is used to specify the size of the input. The forward method performs the attention calculations by multiplying queries with keys transposed, then dividing by the square root of the input_dim. It then applies softmax along dimension 2 and returns the attention weights for further processing.",
        "type": "comment"
    },
    "674": {
        "file_id": 85,
        "content": "ed = torch.bmm(values, attention)\n        return weighted\nimport torch\n# import torchvision.models as models\nfrom bnn import BConfig, prepare_binary_model\n# Import a few examples of quantizers\nfrom bnn.ops import BasicInputBinarizer, BasicScaleBinarizer, XNORWeightBinarizer\n# Define the binarization configuration and assign it to the model\nbconfig = BConfig(\n    activation_pre_process=BasicInputBinarizer,\n    activation_post_process=BasicScaleBinarizer,\n    # optionally, one can pass certain custom variables\n    weight_pre_process=XNORWeightBinarizer.with_args(center_weights=True),\n)\n# Convert the model appropiately, propagating the changes from parent node to leafs\n# The custom_config_layers_name syntax will perform a match based on the layer name, setting a custom quantization function.\nmodel = VKQAttention(input_dim=768)\nbmodel = prepare_binary_model(model, bconfig)\ninp = torch.randn(1, 12, 768)\n# out = model(inp)\n# print(out, out.shape)\n# print(inp)\n# print(out.shape)  # 1, 12, 768\nbout = bmodel(inp)\nprin",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/vkq_bin.py:29-61"
    },
    "675": {
        "file_id": 85,
        "content": "28-47: Define binary quantization config and assign it to the model\n48-56: Convert the model to a binary one, propagating changes from parent to leaf nodes\n57-60: Pass input to the binary model and obtain output",
        "type": "comment"
    },
    "676": {
        "file_id": 85,
        "content": "t(bout)\n# print(bout.shape)",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/vkq_bin.py:61-62"
    },
    "677": {
        "file_id": 85,
        "content": "This code stores the shape of the 'bout' array, but it is not currently printed.",
        "type": "comment"
    },
    "678": {
        "file_id": 86,
        "content": "/containerized_chatgpt_agent/Dockerfile",
        "type": "filepath"
    },
    "679": {
        "file_id": 86,
        "content": "Installing pip, open-interpreter using Tsinghua mirror.",
        "type": "summary"
    },
    "680": {
        "file_id": 86,
        "content": "FROM ubuntu:22.04\nWORKDIR /root\nCOPY install_pip.sh .\nRUN bash install_pip.sh\nRUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple open-interpreter",
        "type": "code",
        "location": "/containerized_chatgpt_agent/Dockerfile:1-5"
    },
    "681": {
        "file_id": 86,
        "content": "Installing pip, open-interpreter using Tsinghua mirror.",
        "type": "comment"
    },
    "682": {
        "file_id": 87,
        "content": "/containerized_chatgpt_agent/Dockerfile_autoexec",
        "type": "filepath"
    },
    "683": {
        "file_id": 87,
        "content": "This Dockerfile installs required packages for a Python-based chat agent, including pip and specific libraries, then copies necessary script files.",
        "type": "summary"
    },
    "684": {
        "file_id": 87,
        "content": "FROM ubuntu:22.04\nWORKDIR /root\nCOPY install_pip.sh .\nRUN bash install_pip.sh\nRUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple litellm pyte ptyprocess requests tornado\nRUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple progressbar2\nCOPY ptyproc.py .\nCOPY terminal_config.py .\nCOPY diff_utils.py .\nCOPY ollama_utils.py .\nCOPY port_util.py .\nCOPY container_autoexec_example.py .",
        "type": "code",
        "location": "/containerized_chatgpt_agent/Dockerfile_autoexec:1-12"
    },
    "685": {
        "file_id": 87,
        "content": "This Dockerfile installs required packages for a Python-based chat agent, including pip and specific libraries, then copies necessary script files.",
        "type": "comment"
    },
    "686": {
        "file_id": 88,
        "content": "/containerized_chatgpt_agent/Dockerfile_autoexec_visual",
        "type": "filepath"
    },
    "687": {
        "file_id": 88,
        "content": "This Dockerfile installs Xfce4 and Xvfb, sets up the screenshot process using xfce4-session and PyAutoGUI. It then installs various Python packages and copies required scripts and files for the application to function properly.",
        "type": "summary"
    },
    "688": {
        "file_id": 88,
        "content": "# install xfce4, xvfb\n# run the screenshot process.\n# xvfb-run -n 99 -f ~/.Xauthority xfce4-session\nFROM ubuntu:22.04\nWORKDIR /root\nENV DEBIAN_FRONTEND=noninteractive\nCOPY install_pip_and_pyautogui_prequisites.sh .\nRUN bash install_pip_and_pyautogui_prequisites.sh\nRUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple litellm requests fastapi uvicorn pytesseract ascii_magic pyautogui\nRUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple progressbar2\nCOPY ptyproc.py .\nCOPY port_util.py .\nCOPY visual_autoexec_example.py .\nCOPY ollama_utils.py .\nCOPY diff_utils.py .\nCOPY cursor.png .\nCOPY visual_server_on_ubuntu.py .\nCOPY visual_autoexec_main.sh main.sh",
        "type": "code",
        "location": "/containerized_chatgpt_agent/Dockerfile_autoexec_visual:1-18"
    },
    "689": {
        "file_id": 88,
        "content": "This Dockerfile installs Xfce4 and Xvfb, sets up the screenshot process using xfce4-session and PyAutoGUI. It then installs various Python packages and copies required scripts and files for the application to function properly.",
        "type": "comment"
    },
    "690": {
        "file_id": 89,
        "content": "/containerized_chatgpt_agent/Modelfile",
        "type": "filepath"
    },
    "691": {
        "file_id": 89,
        "content": "This Docker image uses the llama2 model and sets a temperature of 1 for more coherent responses. It includes a system prompt for AI interactions within a bash shell environment, with available commands like \"type\".",
        "type": "summary"
    },
    "692": {
        "file_id": 89,
        "content": "FROM llama2\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# PARAMETER temperature 0.5\n# set the system prompt\nSYSTEM \"\"\"\nYou are an AI agent inside a terminal environment. The terminal environment is bash shell. You can interact with the environment by writing special commands separated by newline. After your actions, the environment will execute the command and return the current terminal view. You can input special characters like carriage return or delete by using escape sequences, starting with a backslash and ending with a letter, like '\\\\b'.\nDo not talk back to me. Only generate commands. The system will not add the return '\\\\n' for you. You have to be explicit.\nEverytime you try to input anything, a random command will be generated and executed before you. You can view it.\nAvaliable commands:\ntype <character sequence>\nExample:\ntype apt\\\\n\ntype ls\\\\n\n\"\"\"",
        "type": "code",
        "location": "/containerized_chatgpt_agent/Modelfile:1-24"
    },
    "693": {
        "file_id": 89,
        "content": "This Docker image uses the llama2 model and sets a temperature of 1 for more coherent responses. It includes a system prompt for AI interactions within a bash shell environment, with available commands like \"type\".",
        "type": "comment"
    },
    "694": {
        "file_id": 90,
        "content": "/containerized_chatgpt_agent/Modelfile_visual",
        "type": "filepath"
    },
    "695": {
        "file_id": 90,
        "content": "The summary is as follows: The given comments describe a containerized ChatGPT agent with specific settings, such as using llama2-uncensored base and setting the temperature to 1 for creativity and coherence. It provides available commands for interacting with the agent in a containerized environment including typing character sequences, clicking, and moving the cursor.",
        "type": "summary"
    },
    "696": {
        "file_id": 90,
        "content": "FROM llama2-uncensored\n# llama2 won't comply.\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# PARAMETER temperature 0.5\n# set the system prompt\nSYSTEM \"\"\"\nYou are a visual AI agent inside a graphical environment which is xfce4. The OS is Ubuntu 22.04. You can interact with the environment by writing special commands separated by newline. After your actions, the environment will execute the command and return the current desktop view in text. You can input special characters like carriage return or delete by using escape sequences, starting with a backslash and ending with a letter, like '\\\\b'.\nThink of Helen Keller, the human writer who is bilnd. You can operate the GUI with text command. Everytime you try to input anything, a random command will be generated and executed before you. You can view it.\nBelow are some information separated by title and colon. Do not confuse them.\nIn the end you will be asked to give your commands similar to the",
        "type": "code",
        "location": "/containerized_chatgpt_agent/Modelfile_visual:1-16"
    },
    "697": {
        "file_id": 90,
        "content": "Containerized ChatGPT agent with Modelfile_visual: Docker image using llama2-uncensored as base, setting temperature to 1 for creativity and coherence, system prompt provided for visual AI agent interaction within xfce4 on Ubuntu 22.04.",
        "type": "comment"
    },
    "698": {
        "file_id": 90,
        "content": " random commands. Do not emit anything other than commands.\nAvaliable commands:\ntype <character sequence (escaped)>\nclick [left|right|middle]\nclick\nmove_abs <x>,<y>\n\"\"\"",
        "type": "code",
        "location": "/containerized_chatgpt_agent/Modelfile_visual:16-25"
    },
    "699": {
        "file_id": 90,
        "content": "This code provides available commands for a chatGPT agent in a containerized environment. Commands include typing character sequences, clicking with different mouse buttons, and moving the cursor to an absolute position.",
        "type": "comment"
    }
}