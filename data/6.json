{
    "600": {
        "file_id": 74,
        "content": "/containerized_chatgpt_agent/test_image_to_ascii.py",
        "type": "filepath"
    },
    "601": {
        "file_id": 74,
        "content": "The code takes an image file path, converts it to ASCII art using the AsciiArt library, and then prints the ASCII art. It also extracts text from the image using pytesseract and prints that extracted text.",
        "type": "summary"
    },
    "602": {
        "file_id": 74,
        "content": "image_path = \"vscode_screenshot.png\"\nfrom ascii_magic import AsciiArt\ncolumns = 60\nmy_art = AsciiArt.from_image(image_path)\n# str_art = my_art.to_terminal(columns=columns, monochrome=True)\nstr_art = my_art._img_to_art(columns=columns, monochrome=True)\nprint('artwork:')\nprint()\nprint(str_art)\n# then we need to visualize it.\nimport pytesseract\nfrom PIL import Image\nimage = Image.open(image_path)\nextracted_text = pytesseract.image_to_string(image)\nprint(\"extracted text:\")\nprint(extracted_text)",
        "type": "code",
        "location": "/containerized_chatgpt_agent/test_image_to_ascii.py:1-23"
    },
    "603": {
        "file_id": 74,
        "content": "The code takes an image file path, converts it to ASCII art using the AsciiArt library, and then prints the ASCII art. It also extracts text from the image using pytesseract and prints that extracted text.",
        "type": "comment"
    },
    "604": {
        "file_id": 75,
        "content": "/containerized_chatgpt_agent/visual_autoexec_example.py",
        "type": "filepath"
    },
    "605": {
        "file_id": 75,
        "content": "The code imports utilities, defines variables, and outlines future development tasks for a multi-agent system. It initializes functions to interact with a visual model API, handles input actions and action types, escapes/unescapes text, and generates random commands like cursor movement. The program executes commands with error handling and interfaces with ChatGPT, continuously capturing screenshots, constructing prompts with random commands, sending them to ChatGPT for responses, executing the responses as commands, recording errors in lists.",
        "type": "summary"
    },
    "606": {
        "file_id": 75,
        "content": "from port_util import port\nimport ollama_utils\n# TODO: multi-agent infrastructure, help each other to earn bucks\n# TODO: train the model on some 'visual' datasets\n# TODO: diff/diffuse (memory fading) the input\n# TODO: limit the output. prevent the ai from going too far (talkative), if it is not doing any valid operation\n# TODO: make the model 'error-free', that is, interpreting & executing the output no matter what it is\n# TODO: zoom in/out & move around the focus area\n# TODO: focus on the mouse when it is moved\n# TODO: correct mistakes, force it to learn random commands if not doing anything right\n# TODO: use utils designed for blind people to operate the GUI\n# TODO: make statistics on ai execution frequency & overall performance (including random commands), record every execution error and cause (who did it)\nurlbase = f\"http://localhost:{port}\"\nimport functools\ngenerate_command_pool = lambda: {\n    \"executed\": [],\n    \"not_executed\": [],\n    \"error\": [],\n}\nprev_command_pool = generate_command_pool()\nEXE",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:1-25"
    },
    "607": {
        "file_id": 75,
        "content": "This code imports functions from \"port_util\" and \"ollama_utils\", and defines a URL base, lambda function for generating command pools, and variables to track executed, not executed, and error commands. The code has various TODO items for future development such as multi-agent infrastructure, model training, input/output limitations, error correction, and performance statistics.",
        "type": "comment"
    },
    "608": {
        "file_id": 75,
        "content": "C_DELAY = 0.5\n@functools.lru_cache()\ndef urlmake(path):\n    return f\"{urlbase}/{path}\"\nimport litellm\nimport requests\nmodel_name = \"ollama/autoexec_visual\"\nsess = requests.Session()\ndef perform_action(path: str, params: dict):\n    url = urlmake(path)\n    response = sess.get(url, params=params)\n    response_code = response.status_code\n    assert response_code == 200, f\"Error code: {response_code} {response.text}\"\n    return response\ndef get_info(path: str):\n    response = perform_action(path, {})\n    data = response.json()\n    return data\ndef get_resolution():\n    data = get_info(\"resolution\")\n    return data[\"width\"], data[\"height\"]\ndef get_position():\n    data = get_info(\"position\")\n    return data[\"x\"], data[\"y\"]\ndef get_text_screenshot():\n    data = get_info(\"text_screenshot\")\n    return data[\"text\"]\ndef move_abs_action(argument: str):\n    x, y = argument.split(\",\")\n    x, y = x.strip(), y.strip()\n    x, y = float(x), float(y)\n    perform_action(\"move_abs\", {\"x\": x, \"y\": y})\ndef type_action(argument: str):\n    argu",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:25-76"
    },
    "609": {
        "file_id": 75,
        "content": "Code snippet initializes functions to interact with a visual model API. It retrieves information such as resolution, position, and allows moving the cursor and typing text using API requests.",
        "type": "comment"
    },
    "610": {
        "file_id": 75,
        "content": "ment = unescape(argument)\n    perform_action(\"type\", {\"text\": argument})\ndef click_action(argument: str):\n    button = argument.strip()\n    params = {}\n    if argument:\n        params = {\"button\": button}\n    perform_action(\"click\", params)\naction_handlers = {\n    \"move_abs\": move_abs_action,\n    \"type\": type_action,\n    \"click\": click_action,\n}\nimport random\ndef move_abs_random_action(width, height):\n    x = random.randint(0, width)\n    y = random.randint(0, height)\n    action = f\"move_abs {x},{y}\"\n    return action\nimport ast\ndef unescape(text: str):\n    text = ast.literal_eval(repr(text).replace(\"\\\\\\\\\", \"\\\\\"))\n    return text\ndef escape(text: str):\n    text = text.encode(\"unicode_escape\").decode()\n    return text\ndef type_random_action(min_char=4, max_char=10):\n    text_length = random.randint(min_char, max_char)\n    text = \"\".join(chr(random.randint(0, 255)) for _ in range(text_length))\n    text = escape(text)\n    action = f\"type {text}\"\n    return action\ndef click_random_action():\n    button_choices = [\"left\", \"r",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:76-126"
    },
    "611": {
        "file_id": 75,
        "content": "The code defines functions for various input actions such as typing and clicking, and action handlers based on different action types. It also includes functions to escape and unescape text, and generate random actions like moving the cursor and typing or clicking with random parameters.",
        "type": "comment"
    },
    "612": {
        "file_id": 75,
        "content": "ight\", \"middle\", None]\n    button = random.choice(button_choices)\n    if button:\n        action = f\"click {button}\"\n    else:\n        action = \"click\"\n    return action\nrandom_action_generators = {\n    \"move_abs\": move_abs_random_action,\n    \"type\": type_random_action,\n    \"click\": click_random_action,\n}\ndef random_actor(width, height, min_action=1, max_action=3):\n    random_actions = []\n    action_count = random.randint(min_action, max_action)\n    for _ in range(action_count):\n        action_name, action_generator = random.choice(\n            list(random_action_generators.items())\n        )\n        args = []\n        if action_name == \"move_abs\":\n            args.extend([width, height])\n        action = action_generator(*args)\n        random_actions.append(action)\n    return random_actions\nimport traceback\ndef action_executor(action_text: str):\n    action_text = action_text.lstrip()\n    err = None\n    executed = False\n    for action, handler in action_handlers.items():\n        if action_text.startswith(action):\n     ",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:126-166"
    },
    "613": {
        "file_id": 75,
        "content": "This code defines a function `random_actor` that generates a list of random actions to be taken in a visual environment. The actions can be \"move_abs\", \"type\", or \"click\". The number of actions generated is randomly determined between the minimum and maximum values specified. The `action_executor` function takes an action text as input, strips any leading whitespace, and executes the action if it matches any defined action handlers. If an error occurs during execution, it will be stored in the `err` variable.",
        "type": "comment"
    },
    "614": {
        "file_id": 75,
        "content": "       argument = action_text[len(action) + 1 :]\n            print(\"excuting:\", action, argument)\n            try:\n                handler(argument)\n                prev_command_pool[\"executed\"].append(action_text)\n            except:\n                err = traceback.format_exc(limit=1)\n                print(\"err:\", err)\n                prev_command_pool[\"error\"].append(action_text)\n            executed = True\n            break\n    if not executed:\n        prev_command_pool[\"not_executed\"].append(action_text)\n    time.sleep(EXEC_DELAY)\n    return err\n# at the same time, how do we visualize the current display?\n# you need to name that container.\ndef execute_command_list(cmd_list):\n    err_list = []\n    for cmd in cmd_list:\n        err = action_executor(cmd)\n        if err:\n            err_list.append(err)\n    return err_list\nimport time\nSLEEP_TIME = 3\ndef construct_prompt(\n    data: str, width: int, height: int, random_err_list: list[str], err_list: list[str]\n):\n    random_commands = random_actor(width, height)\n    x",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:166-205"
    },
    "615": {
        "file_id": 75,
        "content": "1. Executes actions based on the given command list\n2. Handles success, error, and not executed commands separately\n3. Sleeps for a specified delay time after executing each action\n4. Returns a list of errors encountered",
        "type": "comment"
    },
    "616": {
        "file_id": 75,
        "content": ", y = get_position()\n    random_commands_str = \"\\n\".join(random_commands)\n    last_random_errors = \"\\n\".join(random_err_list)\n    last_errors = \"\\n\".join(err_list)\n    previous_executed_repr = \"\\n\".join(prev_command_pool[\"executed\"])\n    previous_error_repr = \"\\n\".join(prev_command_pool[\"error\"])\n    previous_not_executed_repr = \"\\n\".join(prev_command_pool[\"not_executed\"])\n    prompt = f\"\"\"\n{data}\nPointer location: {x}, {y}\nResolution: {width}x{height}\nLast random command errors:\n{last_random_errors}\nLast errors:\n{last_errors}\nPrevious executed successfully:\n{previous_executed_repr}\nPrevious executed with error:\n{previous_error_repr}\nPrevious not executed:\n{previous_not_executed_repr}\nNext random commands:\n{random_commands_str}\nYour commands:\n\"\"\"\n    return prompt, random_commands\ndef get_reply_from_chatgpt(content: str, max_tokens=50):\n    messages = [{\"content\": content, \"role\": \"system\"}]\n    print(\"sending:\")\n    print(messages)\n    response = litellm.completion(\n        model_name, messages, api_base=\"http://lo",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:205-253"
    },
    "617": {
        "file_id": 75,
        "content": "Line 204: Get the current position\nLine 205-209: Format random_commands, last_random_errors, last_errors, previous_executed_repr, and previous_error_repr into strings\nLine 210: Create a prompt for ChatGPT that includes the data, position, error logs, previous commands, and next commands\nLine 211-214: Return the prompt and random_commands\nLine 215-231: Send messages to OpenAI API, get reply from ChatGPT",
        "type": "comment"
    },
    "618": {
        "file_id": 75,
        "content": "calhost:11434\", max_tokens=max_tokens\n    )\n    choices = response[\"choices\"]\n    reply_content = choices[0][\"message\"][\"content\"]\n    print(\"reply:\")\n    print(reply_content)\n    return reply_content\ndef refresh_command_pool(command_pool, limit=3):\n    ret = {}\n    for k,v in command_pool.items():\n        new_v = v[-limit:]\n        ret[k] = new_v\n    return ret\nerr_list = []\nrandom_err_list = []\nwidth, height = get_resolution()\nwhile True:\n    data = get_text_screenshot()\n    prompt, random_commands = construct_prompt(\n        data.strip(), width, height, random_err_list, err_list\n    )\n    prev_command_pool = generate_command_pool()\n    # prev_command_pool = refresh_command_pool(prev_command_pool)\n    print(\"random commands:\", random_commands)\n    response = get_reply_from_chatgpt(prompt)\n    command_list = response.split(\"\\n\")\n    random_err_list = execute_command_list(random_commands)\n    err_list = execute_command_list(command_list)\n    time.sleep(SLEEP_TIME)",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_example.py:253-284"
    },
    "619": {
        "file_id": 75,
        "content": "This code is continuously capturing a text screenshot, constructing a prompt with random commands, and sending the prompt to ChatGPT for replies. The replies are then executed as commands, and any errors generated are recorded in error lists.",
        "type": "comment"
    },
    "620": {
        "file_id": 76,
        "content": "/containerized_chatgpt_agent/visual_autoexec_main.sh",
        "type": "filepath"
    },
    "621": {
        "file_id": 76,
        "content": "Starts Xvfb with 99 display, launches Ubuntu visual server, and then runs an example script",
        "type": "summary"
    },
    "622": {
        "file_id": 76,
        "content": "xvfb-run -n 99 -f ~/.Xauthority xfce4-session &\nsleep 2 && env DISPLAY=:99 python3 visual_server_on_ubuntu.py &\nsleep 5 && python3 visual_autoexec_example.py",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_autoexec_main.sh:1-3"
    },
    "623": {
        "file_id": 76,
        "content": "Starts Xvfb with 99 display, launches Ubuntu visual server, and then runs an example script",
        "type": "comment"
    },
    "624": {
        "file_id": 77,
        "content": "/containerized_chatgpt_agent/visual_server_on_ubuntu.py",
        "type": "filepath"
    },
    "625": {
        "file_id": 77,
        "content": "The code is a Python script using PyAutoGUI and Tesseract OCR, enabling users to capture screenshots with cursor overlay, perform actions like moving, clicking, typing, and offers API endpoints for controlling the cursor and inputting text on a computer.",
        "type": "summary"
    },
    "626": {
        "file_id": 77,
        "content": "# you can input commands here.\n# after input, you may take screenshot and get it as text.\n# now you can move, click, and type.\nimport fastapi\nimport uvicorn\nfrom port_util import port\nimport pyautogui\nfrom PIL import Image\nfrom diff_utils import diff_methods\nfrom typing import Literal\npyautogui.FAILSAFE = False\ncursor_image = \"cursor.png\"\ncur = Image.open(cursor_image)\ndef screenshot_with_cursor():\n    shot = pyautogui.screenshot()\n    pos = pyautogui.position()\n    shot.paste(cur, pos, cur)\n    return shot\nfrom ascii_magic import AsciiArt\nimport pytesseract\ndef image_to_ascii(img: Image, columns=60):\n    art = AsciiArt.from_pillow_image(img)\n    ascii_text = art._img_to_art(columns=columns, monochrome=True)\n    return ascii_text.strip()\ndef image_to_words(img: Image):\n    words = pytesseract.image_to_string(img)\n    return words.strip()\nprev_registry = {\n    'ascii_text':'',\n    'words': ''\n}\ndef image_to_ascii_and_words(img: Image, method):\n    procedure = diff_methods.get(method, lambda prev_text, next_text: next_",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_server_on_ubuntu.py:1-47"
    },
    "627": {
        "file_id": 77,
        "content": "This code is a Python script that allows the user to input commands, take screenshots with cursor overlay, perform actions like moving, clicking, and typing, converts images to ASCII art or extracts text from images using Tesseract OCR. It also keeps track of previous ascii_text and words values in prev_registry.",
        "type": "comment"
    },
    "628": {
        "file_id": 77,
        "content": "text)\n    ascii_text = image_to_ascii(img)\n    ascii_text_processed = process_and_update(procedure, ascii_text, 'ascii_text')\n    words = image_to_words(img)\n    words_processed = process_and_update(procedure, words, 'words')\n    text = f\"\"\"\nAscii image:\n{ascii_text_processed}\nText in image:\n{words_processed}\n\"\"\"\n    return text\ndef process_and_update(procedure, item, key):\n    output = procedure(prev_registry[key], item)\n    prev_registry[key] = item\n    return output\napp = fastapi.FastAPI()\n@app.get(\"/position\")\ndef get_position():\n    pos = pyautogui.position()\n    data = {\"x\": pos.x, \"y\": pos.y}\n    return data\n@app.get(\"/resolution\")\ndef get_resolution():\n    size = pyautogui.size()\n    data = {\"width\": size.width, \"height\": size.height}\n    return data\n@app.get(\"/text_screenshot\")\ndef get_text_screenshot(\n    method: Literal[\"git_style_diff\", \"char_diff\", \"line_indexed_diff\", 'no_diff'] = 'line_indexed_diff'\n):\n    shot = screenshot_with_cursor()\n    text = image_to_ascii_and_words(shot, method)\n    return {\"tex",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_server_on_ubuntu.py:47-95"
    },
    "629": {
        "file_id": 77,
        "content": "Code is converting an image to ASCII text and words, then combining them into a formatted string. It also provides API endpoints for getting the position of the mouse and the resolution of the screen.",
        "type": "comment"
    },
    "630": {
        "file_id": 77,
        "content": "t\": text}\n@app.get(\"/move_abs\")\ndef move_cursor_abs(x: int, y: int):\n    pyautogui.moveTo(x, y)\n@app.get(\"/move_rel\")\ndef move_cursor_rel(x: int, y: int):\n    pyautogui.moveRel(x, y)\nfrom typing import Literal\n@app.get(\"/click\")\ndef click_cursor(button: Literal[\"left\", \"right\", \"middle\"] = \"left\"):\n    params = {}\n    if button:\n        params = {\"button\": button}\n    pyautogui.click(**params)\n@app.get(\"/type\")\ndef type_text(text: str):\n    pyautogui.typewrite(text)\n@app.get(\"/write\")\ndef type_text(text: str):\n    pyautogui.write(text)\n@app.get(\"/scroll\")\ndef scroll_down(x: float, y: float, clicks: float):\n    pyautogui.scroll(clicks=clicks, x=x, y=y)\nif __name__ == \"__main__\":\n    host = \"0.0.0.0\"\n    print(\"gui server running at:\", f\"http://{host}:{port}\")\n    uvicorn.run(app, host=host, port=port)",
        "type": "code",
        "location": "/containerized_chatgpt_agent/visual_server_on_ubuntu.py:95-137"
    },
    "631": {
        "file_id": 77,
        "content": "This code defines several API endpoints that can be used to control the cursor and input text on a computer using Python's PyAutoGUI library. The endpoints include moving the cursor, clicking buttons, typing text, and scrolling down. The code also sets up an Uvicorn server to host these API endpoints.",
        "type": "comment"
    },
    "632": {
        "file_id": 78,
        "content": "/decorator_method_registry.py",
        "type": "filepath"
    },
    "633": {
        "file_id": 78,
        "content": "MyDecorator is a function decorator that executes code before and after the decorated function, while MyMeta adds code execution right after the 'def' block of the decorated function. A decorator function \"my_decorator\" is defined which executes code before and after the decorated function by using \"@\" syntax to decorate the function \"my_function\".",
        "type": "summary"
    },
    "634": {
        "file_id": 78,
        "content": "class MyDecorator:\n    def __init__(self, func):\n        self.func = func\n        self.invoke_decorated_function()\n    def invoke_decorated_function(self):\n        # Code to execute after the 'def' block of the decorated function\n        print(\"Executing code right after the 'def' block\")\n        self.func()\n# Decorate a function with the decorator\n@MyDecorator\ndef my_function():\n    print(\"Executing the decorated function\")\nprint(\"-\" * 40)\nclass MyMeta(type):\n    def __new__(cls, name, bases, attrs):\n        # Code to execute right after the 'def' block of the decorated function\n        print(\"Executing code right after the 'def' block\")\n        return super().__new__(cls, name, bases, attrs)\n# Define the metaclass for the decorator\nclass MyDecorator(metaclass=MyMeta):\n    def __call__(self, func):\n        print(\"Executing code before the function is invoked\")\n        def wrapper(*args, **kwargs):\n            # Code to execute before invoking the decorated function\n            # Invoke the decorated function\n",
        "type": "code",
        "location": "/decorator_method_registry.py:1-38"
    },
    "635": {
        "file_id": 78,
        "content": "MyDecorator class is a function decorator that executes code before and after the decorated function.\nMetaclass MyMeta adds code execution right after the 'def' block of the decorated function.",
        "type": "comment"
    },
    "636": {
        "file_id": 78,
        "content": "            return func(*args, **kwargs)\n        return wrapper\n# Decorate a function with the decorator\n@MyDecorator()\ndef my_function():\n    print(\"Executing the decorated function\")\nprint(\"-\" * 20)\ndef my_decorator(func):\n    print(\"Executing code before the function is invoked.\")\n    print(\"Function name:\", func.__name__)\n    def wrapper(*args, **kwargs):\n        # Code to execute before the invocation of the decorated function\n        # Invoke the decorated function\n        return func(*args, **kwargs)\n    # Return the wrapper function\n    return wrapper\n# Decorate a function with the decorator\n@my_decorator\ndef my_function():\n    print(\"Executing the decorated function.\")",
        "type": "code",
        "location": "/decorator_method_registry.py:38-68"
    },
    "637": {
        "file_id": 78,
        "content": "Defines a decorator function \"my_decorator\" that executes code before and after the decorated function. Uses the \"@\" syntax to decorate the function \"my_function\".",
        "type": "comment"
    },
    "638": {
        "file_id": 79,
        "content": "/directml_yolov5/test.py",
        "type": "filepath"
    },
    "639": {
        "file_id": 79,
        "content": "The code modifies torch.cat to handle zero-sized arrays and enables DirectML compatibility for inference mode. It processes a filepath with the model multiple times using a loop, sets it to specified device, prints output for 1000 iterations, and disables automatic differentiation using torch.no_grad().",
        "type": "summary"
    },
    "640": {
        "file_id": 79,
        "content": "import torch\nimport torch_directml\nimport contextlib\nimport copy\nfrom functools import reduce\n@contextlib.contextmanager\ndef null_inference_mode(*args, **kwargs):\n    try:\n        yield\n    finally:\n        pass\ntorch.inference_mode = null_inference_mode\nold_cat = copy.copy(torch.cat)\ndef smart_cat(arr, *args, **kwargs):\n    new_arr = []\n    for it in arr:\n        shape = it.shape\n        size = reduce(lambda x,y: x*y, shape)\n        if size >0:\n            new_arr.append(it)\n    ret = old_cat(new_arr, *args, **kwargs)\n    return ret\ntorch.cat = smart_cat\ndev = torch_directml.device()\nmodel = torch.hub.load(\"../yolov5\", \"yolov5m\", source='local')\n# get torch cache path?\n# model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5m\")\nfilepath = \"./zidane.jpg\"\n# two issues:\n# 1. directml doesn't work with inference mode (yet), you can nullify it.\n# 2. torch.cat is not working properly, because we are passing zero size arrays into it. however, the cpu executor supports it.\n# with torch.inference_mode(mode=False):\nwith torch.n",
        "type": "code",
        "location": "/directml_yolov5/test.py:1-41"
    },
    "641": {
        "file_id": 79,
        "content": "Code is modifying torch.cat to handle zero size arrays and nullifying inference mode for DirectML compatibility.",
        "type": "comment"
    },
    "642": {
        "file_id": 79,
        "content": "o_grad():\n    # for _ in range(1000):\n    print(model(filepath))\nmodel.to(dev)\n# with torch.inference_mode(mode=False):\nwith torch.no_grad():\n    # for _ in range(1000):\n    # amd yes!\n    print(model(filepath))",
        "type": "code",
        "location": "/directml_yolov5/test.py:41-49"
    },
    "643": {
        "file_id": 79,
        "content": "This code is using the \"model\" to process the \"filepath\" multiple times in a loop. It first sets the model to the specified device (dev), and then prints the output of the model for 1000 iterations. The \"torch.no_grad()\" context manager ensures that automatic differentiation is disabled, potentially improving performance during these model evaluations.",
        "type": "comment"
    },
    "644": {
        "file_id": 80,
        "content": "/directml_yolov5/test_concat.py",
        "type": "filepath"
    },
    "645": {
        "file_id": 80,
        "content": "Code snippet for testing concatenation using torch library with directml acceleration.",
        "type": "summary"
    },
    "646": {
        "file_id": 80,
        "content": "import torch\nimport torch_directml\ndev = torch_directml.device()\na = torch.ones((20, 20))\na.to(dev)\nprint(torch.cat([a, a]))\nprint(torch.cat([a, a], dim=1))\nprint(torch.cat([a, a], 1))\nprint(torch.cat((a, a), 1))\nprint(torch.cat((a, a, a), 1))\nprint(torch.cat((a, a, a, a), 1))\nprint(torch.cat((a, a, a, a, a), 1))",
        "type": "code",
        "location": "/directml_yolov5/test_concat.py:1-14"
    },
    "647": {
        "file_id": 80,
        "content": "Code snippet for testing concatenation using torch library with directml acceleration.",
        "type": "comment"
    },
    "648": {
        "file_id": 81,
        "content": "/directml_yolov5/test_nograd.py",
        "type": "filepath"
    },
    "649": {
        "file_id": 81,
        "content": "The code is importing necessary libraries and initializing a YOLOv5 model using the \"ultralytics/yolov5\" module. It then warns about two issues: DirectML not working with torch.inference_mode and potential problems with torch.cat due to passing zero-size arrays. The code finally runs inference on an image, first without and then with DirectML device.",
        "type": "summary"
    },
    "650": {
        "file_id": 81,
        "content": "import torch\nimport torch_directml\ndev = torch_directml.device()\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5m\")\n# two issues:\n# 1. directml doesn't work with torch.inference_mode. you can nullify it.\n# 2. torch.cat is not working properly, because we are passing zero size arrays into it. however, the cpu executor supports it.\nwith torch.no_grad():\n    print(model(\"C:/Users/z98hu/Desktop/zidane.jpg\"))\nmodel.to(dev)\nwith torch.no_grad():\n    print(model(\"C:/Users/z98hu/Desktop/zidane.jpg\"))",
        "type": "code",
        "location": "/directml_yolov5/test_nograd.py:1-14"
    },
    "651": {
        "file_id": 81,
        "content": "The code is importing necessary libraries and initializing a YOLOv5 model using the \"ultralytics/yolov5\" module. It then warns about two issues: DirectML not working with torch.inference_mode and potential problems with torch.cat due to passing zero-size arrays. The code finally runs inference on an image, first without and then with DirectML device.",
        "type": "comment"
    },
    "652": {
        "file_id": 82,
        "content": "/dynamic_plasticity_neural_networks/MCTS_NAS.md",
        "type": "filepath"
    },
    "653": {
        "file_id": 82,
        "content": "Monte Carlo Tree Search (MCTS) is used in Neural Architecture Search (NAS) to evaluate potential network structures. This process involves simulation, backpropagation, selection and repetition until a stopping criterion is met. Researchers continue to explore new methods for NAS efficiency and effectiveness.",
        "type": "summary"
    },
    "654": {
        "file_id": 82,
        "content": "Monte Carlo Tree Search (MCTS) is a heuristic search algorithm that's commonly used in decision-making processes, particularly in game-playing AI, where it evaluates possible moves and outcomes to make decisions. When it comes to using MCTS in neural architecture search (NAS), it typically involves using MCTS to explore the space of possible neural network architectures and evaluate their performance.\nHere's a high-level overview of how MCTS could be used in NAS:\n1. **Search Space Representation**: Define a representation of the neural network architecture space. This could involve defining different types of layers, their connections, hyperparameters, etc., that form the search space.\n2. **Tree Expansion**: Start with a root node representing the current state of the search (e.g., a randomly initialized neural network architecture). Use MCTS to iteratively expand the search tree by considering different possible architectures and their performance.\n3. **Simulation and Evaluation**: During",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/MCTS_NAS.md:1-7"
    },
    "655": {
        "file_id": 82,
        "content": "Monte Carlo Tree Search (MCTS) is used in neural architecture search to explore and evaluate possible network architectures based on their performance.",
        "type": "comment"
    },
    "656": {
        "file_id": 82,
        "content": " the selection and expansion phase of MCTS, simulate the performance of different architectures by training them on a subset of data or using a proxy measure of performance. This helps in estimating the potential value of exploring a particular architecture.\n4. **Backpropagation**: Propagate the simulated performance results back up the tree to update the value estimates of different architectures and guide the search towards promising areas of the architecture space.\n5. **Selection**: Use the updated value estimates to guide the selection of architectures for further exploration, focusing on those with potentially higher performance.\n6. **Repeat and Refine**: Iterate the process of selection, expansion, simulation, and backpropagation for a certain number of iterations or until a stopping criterion is met.\nIt's important to note that the specific implementation of MCTS in NAS can vary depending on the exact problem setting, the search space representation, and the performance evaluation m",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/MCTS_NAS.md:7-12"
    },
    "657": {
        "file_id": 82,
        "content": "This code describes the process of using Monte Carlo Tree Search (MCTS) for Neural Architecture Search (NAS). It involves simulating performance, backpropagation, selection, and repeating until a stopping criterion is met. The implementation can vary based on problem setting and search space representation.",
        "type": "comment"
    },
    "658": {
        "file_id": 82,
        "content": "ethods used. Additionally, MCTS is just one of many approaches to NAS, and researchers continue to explore different methods to efficiently and effectively search the space of neural network architectures.",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/MCTS_NAS.md:12-12"
    },
    "659": {
        "file_id": 82,
        "content": "This code discusses the use of MCTS (Monte Carlo Tree Search) in NAS (Neural Architecture Search), mentioning that it's just one approach and researchers continue to explore new methods for efficient and effective neural network architecture search.",
        "type": "comment"
    },
    "660": {
        "file_id": 83,
        "content": "/dynamic_plasticity_neural_networks/README.md",
        "type": "filepath"
    },
    "661": {
        "file_id": 83,
        "content": "This code introduces the concept of creating artificial neurons that can move around and change connections. It first focuses on replicating a static classic neural network before moving to dynamic ones.",
        "type": "summary"
    },
    "662": {
        "file_id": 83,
        "content": "We can create artificial neurons that can move around, form new connections, changing its overall shape.\nBut first we will replicate a common static classic neural network.",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/README.md:1-3"
    },
    "663": {
        "file_id": 83,
        "content": "This code introduces the concept of creating artificial neurons that can move around and change connections. It first focuses on replicating a static classic neural network before moving to dynamic ones.",
        "type": "comment"
    },
    "664": {
        "file_id": 84,
        "content": "/dynamic_plasticity_neural_networks/dnn_reference.py",
        "type": "filepath"
    },
    "665": {
        "file_id": 84,
        "content": "This code defines classes for connections and neurons in a neural network, including forward propagation, activation function, backpropagation, and derivative functions using a sigmoid function.",
        "type": "summary"
    },
    "666": {
        "file_id": 84,
        "content": "import numpy as np  # Import NumPy for mathematical operations\nclass Connection:\n    def __init__(self, connected_index, weights, bias):\n        self.connected_index = connected_index  # Index of the connected neuron\n        self.weights = weights  # Weights of the connection\n        self.bias = bias  # Bias of the connection\nclass Neuron:\n    def __init__(self, index, current_potential, input_connections, output_connections):\n        self.index = index  # Index of the neuron\n        self.current_potential = current_potential  # Current membrane potential\n        self.input_connections = input_connections  # List of input connections\n        self.output_connections = output_connections  # List of output connections\n    def forward_propagate(self, inputs):\n        # Perform weighted sum of inputs and apply activation function\n        weighted_sum = np.dot(inputs, [conn.weights for conn in self.input_connections]) + self.input_connections[0].bias\n        self.current_potential = self.activation_functio",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/dnn_reference.py:1-19"
    },
    "667": {
        "file_id": 84,
        "content": "The code defines two classes: \"Connection\" and \"Neuron\". The \"Connection\" class represents a connection between two neurons with properties like connected index, weights, and bias. The \"Neuron\" class represents a neuron with properties such as its index, current potential, input connections, and output connections. It also has a method called \"forward_propagate\" that performs weighted sum of inputs, adds the connection's bias, and applies an activation function to calculate the neuron's new potential.",
        "type": "comment"
    },
    "668": {
        "file_id": 84,
        "content": "n(weighted_sum)\n    def activation_function(self, x):\n        # For example, using a simple sigmoid activation function\n        return 1 / (1 + np.exp(-x))\n    def back_propagate(self, error):\n        # Update weights and biases based on backpropagated error\n        # This is a simplified example; a complete backpropagation algorithm would involve more steps\n        for conn in self.input_connections:\n            # Update weights using the error and the derivative of the activation function\n            conn.weights -= learning_rate * error * self.activation_function_derivative(self.current_potential) * conn.connected_index.current_potential\n            # Update bias using the error\n            conn.bias -= learning_rate * error * self.activation_function_derivative(self.current_potential)\n    def activation_function_derivative(self, x):\n        # Derivative of the sigmoid activation function\n        return self.activation_function(x) * (1 - self.activation_function(x))",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/dnn_reference.py:19-36"
    },
    "669": {
        "file_id": 84,
        "content": "This code defines a neural network class with activation function, backpropagation, and derivative functions. The activation function uses a simple sigmoid function, while the backpropagate method updates weights and biases based on the error. The activation_function_derivative calculates the derivative of the activation function.",
        "type": "comment"
    },
    "670": {
        "file_id": 85,
        "content": "/dynamic_plasticity_neural_networks/dynamic_neural_network.py",
        "type": "filepath"
    },
    "671": {
        "file_id": 85,
        "content": "The code defines neural network classes for creating bots capable of understanding human intentions and performing tasks, with a focus on decision-making using Monte Carlo tree search and batch training techniques.",
        "type": "summary"
    },
    "672": {
        "file_id": 85,
        "content": "import torch\n# let's use networkx to generate random graphs?\n# the method to train it? we only keep the input & output the same as the static model.\n# place some clock like neurons, just like the SNN\n# to ensure that neurons won't get far away from each other\n# if they want to have some spikes\n# now you might want to use solver for network assembly.\n# neurons that fire together, wire together\n# there are pending connections. if these connections has larger gradients, they will be welcomed.\n# calculate route to output. if there is no route to output, then no gradient.\n# it rolls out the monte carlo tree search in ppo. would you find some optimal neural network structure with that?\nclass Connection:\n    def __init__(self):\n        self.connected_index = ...\n        self.weights = ...\n        self.bias = ...\nclass Neuron:\n    def __init__(self): # you can choose like: 100x1 -> 1x50 (1 can be larger, like 2) or 10x10\n        # but 100x1, 1x50 can be perferred, since it includes the potential or hidden state\n",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/dynamic_neural_network.py:1-30"
    },
    "673": {
        "file_id": 85,
        "content": "The code defines classes for neurons and connections in a neural network. The neuron class has an output size of 1x50, which includes potential or hidden state. The connection class represents the connections between neurons and holds their weights and bias. The code mentions using Monte Carlo tree search to find optimal neural network structure, but does not explicitly define this algorithm.",
        "type": "comment"
    },
    "674": {
        "file_id": 85,
        "content": "        # we can also tune the decay factor inside the neuron.\n        self.index = ...\n        self.current_potential = ...\n        self.input_connection = ...\n        self.output_connection = ...\nclass NeuralPort:\n    def __init__(self):\n        self.index = ...\n        self.port_type = ...\n        self.current_potential = ...\n# so we would iterate through our connection pool, propagate\n# how to do batch training?\ninput_ports = [] # assign ten indices\noutput_ports = []\n# obviously they are not connected.\n# you can assess that by gradient. does it have gradient?\n# recurrent connection: (0, 1), (1, 2), (2, 0)\n# so we just do all the forwarding at once in random order. who cares the order? no one!\n# maybe you can take multiple random execution samples multiple times and average out to stablize the performance\n# so there are possibly two things to learn. the first is to learn the connectivity rule. the second is to learn the propagation rule.\n# does a regular/periodic propagation rule plausible in brain?\n# c",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/dynamic_neural_network.py:30-58"
    },
    "675": {
        "file_id": 85,
        "content": "This code defines a NeuralPort class with properties for index, port_type, and current_potential. It also initializes input_ports and output_ports lists to store indices. The code mentions batch training, recurrent connections, forwarding all inputs randomly, and learning connectivity and propagation rules.",
        "type": "comment"
    },
    "676": {
        "file_id": 85,
        "content": "alculate inter_neuron distance\n# we have to use sparse matrix\n# output_node_index, input_node_index\n# if you swap, you break the connection\n# you can compute the gradient, to decide if you want to break or keep\n# (2, 3)\n# (4, 1)\n# we will start with the input neurons, iterate over the network\n# it forms a loop.\n# output: spikes, movements per neuron\n# what determines the connectivity between neurons?\n# what forms new connectivities?\n# (in0, out0)\n# (in1, out1)\n# (in2, out2)\n# i think we do not need the coordinates.\n# i think we need some gcn, or recommendation engine?\n# just represent the connectivity in sparse matrix.\n# connection swapping?\n# minimize the number of unconnected neurons\n# cannot connect to themselves\n# you may have pre-programmed randomly initialized connectivity matrix\n# hierarchical grouping?\n# would you rather do some value swapping in weight matricies\n# liquid state machine\n# dynamic rewiring of neurons by upper/lower weight limit, or the free energy principle\n# or you can also look at the a",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/dynamic_neural_network.py:58-99"
    },
    "677": {
        "file_id": 85,
        "content": "Computing inter-neuron distance and creating sparse matrices for input/output nodes. Iterating over the network to form connections. Determining connectivity between neurons using potential methods like GCN, recommendation engine, or pre-programmed random initialization. Connection swapping to minimize unconnected neurons. Hierarchical grouping or adjusting weight matrices possible.",
        "type": "comment"
    },
    "678": {
        "file_id": 85,
        "content": "ctual energy consumption or battery consumption\n# by far multiple thesis have been stated, but yet unproven and unimplemented.\n# the primary objective of this project is to create computer operating bots that can understand human intentions and do everyday tasks on their own, including browsing, coding and searching.\n# it is not clear whether this project will create some artificial life, and it is not our objective. it might be a good research project for the bots, but i can say for sure it is not for me. i have limited capacity of knowledge and resources. i don't allocate that much memory biologically or physically. i choose to build the bot first. it does not have to be that complex that i cannot imagine or create.\n# decision matters. the executors of this 'life' project should be bots. i am the project manager. besides, i could participate in the 'cybergod' project which controls the computer by computer itself.\n# noise in, words out, recurse. that is dream.",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/dynamic_neural_network.py:99-109"
    },
    "679": {
        "file_id": 85,
        "content": "This code discusses the creation of computer bots that can understand human intentions and perform tasks, with a focus on browsing, coding, and searching. The author expresses their limited capacity to create an artificial life form but believes in using bots for decision making. They also mention participating in a 'cybergod' project that controls computers by themselves.",
        "type": "comment"
    },
    "680": {
        "file_id": 86,
        "content": "/dynamic_plasticity_neural_networks/neural_evolution.py",
        "type": "filepath"
    },
    "681": {
        "file_id": 86,
        "content": "This code is importing the NEAT (Nature-Inspired Evolutionary Algorithms Toolkit) module from evotorch library, and creating a NEProblem object for neural evolution problem.",
        "type": "summary"
    },
    "682": {
        "file_id": 86,
        "content": "# NEAT?\nimport evotorch\nevotorch.neuroevolution.NEProblem",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/neural_evolution.py:1-4"
    },
    "683": {
        "file_id": 86,
        "content": "This code is importing the NEAT (Nature-Inspired Evolutionary Algorithms Toolkit) module from evotorch library, and creating a NEProblem object for neural evolution problem.",
        "type": "comment"
    },
    "684": {
        "file_id": 87,
        "content": "/dynamic_plasticity_neural_networks/sparse_matrix_multiplication.py",
        "type": "filepath"
    },
    "685": {
        "file_id": 87,
        "content": "Creates a sparse matrix with random values and average synapses, then times the calculation of multiplying it by either another dense or sparse matrix.",
        "type": "summary"
    },
    "686": {
        "file_id": 87,
        "content": "import torch\nimport sparse\nimport time\nlarge_number = 1_000_000\naverage_synapses = 3 # 1.7s, 0.146s\n# average_synapses = 10 # 11.8s, 0.505s\n# average_synapses = 100 # ..., 13.258s\n# average_synapses = 1_000\nnp_sparse = sparse.random((large_number, large_number), nnz=large_number*average_synapses)\n# print(np_sparse.coords, np_sparse.data, np_sparse.shape)\n# breakpoint()\ntorch_sparse = torch.sparse_coo_tensor(np_sparse.coords, np_sparse.data, np_sparse.shape)\ntorch_dense = torch.randn(large_number, dtype=torch.double)\n# before = time.time()\n# result = torch_sparse@torch_sparse\n# after = time.time()\n# print(result)\n# print(f\"calculation time: {after-before:.3f}s\")\nbefore = time.time()\nresult_dense = torch_dense@torch_sparse\nafter = time.time()\nprint(result_dense)\nprint(f\"calculation time: {after-before:.3f}s\")",
        "type": "code",
        "location": "/dynamic_plasticity_neural_networks/sparse_matrix_multiplication.py:1-27"
    },
    "687": {
        "file_id": 87,
        "content": "Creates a sparse matrix with random values and average synapses, then times the calculation of multiplying it by either another dense or sparse matrix.",
        "type": "comment"
    },
    "688": {
        "file_id": 88,
        "content": "/external_reference/extra_init.sh",
        "type": "filepath"
    },
    "689": {
        "file_id": 88,
        "content": "Clone CogVLM and AppAgent repositories from their respective GitHub URLs.",
        "type": "summary"
    },
    "690": {
        "file_id": 88,
        "content": "git clone https://github.com/THUDM/CogVLM\ngit clone https://github.com/mnotgod96/AppAgent",
        "type": "code",
        "location": "/external_reference/extra_init.sh:1-2"
    },
    "691": {
        "file_id": 88,
        "content": "Clone CogVLM and AppAgent repositories from their respective GitHub URLs.",
        "type": "comment"
    },
    "692": {
        "file_id": 89,
        "content": "/external_reference/init.cmd",
        "type": "filepath"
    },
    "693": {
        "file_id": 89,
        "content": "Cloning necessary repositories for project setup.",
        "type": "summary"
    },
    "694": {
        "file_id": 89,
        "content": "git clone https://github.com/abhiprojectz/SingularGPT\ngit clone https://github.com/ddupont808/GPT-4V-Act\ngit clone https://github.com/openai/Video-Pre-Training\ngit clone https://github.com/Charmve/gpt-eyes\ngit clone https://github.com/ruvnet/q-star",
        "type": "code",
        "location": "/external_reference/init.cmd:1-5"
    },
    "695": {
        "file_id": 89,
        "content": "Cloning necessary repositories for project setup.",
        "type": "comment"
    },
    "696": {
        "file_id": 90,
        "content": "/external_reference/supercharge_init.sh",
        "type": "filepath"
    },
    "697": {
        "file_id": 90,
        "content": "The code is cloning four GitHub repositories: \"self-operating-computer\", \"QStarLearning.mojo\", \"open_qstar\", and \"gpt4v-browsing\".",
        "type": "summary"
    },
    "698": {
        "file_id": 90,
        "content": "git clone https://github.com/OthersideAI/self-operating-computer\ngit clone https://github.com/tairov/QStarLearning.mojo\ngit clone https://github.com/estill01/open_qstar\ngit clone https://github.com/unconv/gpt4v-browsing",
        "type": "code",
        "location": "/external_reference/supercharge_init.sh:1-4"
    },
    "699": {
        "file_id": 90,
        "content": "The code is cloning four GitHub repositories: \"self-operating-computer\", \"QStarLearning.mojo\", \"open_qstar\", and \"gpt4v-browsing\".",
        "type": "comment"
    }
}