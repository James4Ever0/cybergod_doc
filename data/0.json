{
    "0": {
        "file_id": 0,
        "content": "/Dockerfile",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "This Dockerfile uses x11docker/xserver as the base image and sets up a development server. It may be used with docker-compose.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "# FROM ubuntu:22.04\n# ref: https://gitlab.com/mviereck/x11docker#docker-desktop-or-docker-engine\nFROM x11docker/xserver:latest\n####### LAZERO DEVELOPMENT SERVER #######\n# docker compose?",
        "type": "code",
        "location": "/Dockerfile:1-5"
    },
    "3": {
        "file_id": 0,
        "content": "This Dockerfile uses x11docker/xserver as the base image and sets up a development server. It may be used with docker-compose.",
        "type": "comment"
    },
    "4": {
        "file_id": 1,
        "content": "/Makefile",
        "type": "filepath"
    },
    "5": {
        "file_id": 1,
        "content": "The code snippet adjusts environment variables according to the OS and prepares necessary files/directories for testing a project. The Makefile builds, executes the Python test project, syncs utilities, handles dependencies, and creates JSON files using tools like pytest, renderer, bash script, and Pip installer.",
        "type": "summary"
    },
    "6": {
        "file_id": 1,
        "content": ".PHONY: test\n# may borrow system check code from ies.\nPLATFORM := $(shell python -c \"import os; print(os.name)\")\nifeq (${PLATFORM}, )\nPLATFORM := $(shell python3 -c \"import os; print(os.name)\") # executed on macos\nendif\nifeq (${PLATFORM}, nt)\nOS_TYPE = windows\nelse\nOS_TYPE = macos\nendif\nPYTHON_ENV = -X utf8=1\nifeq (${OS_TYPE}, macos)\nCONDA_ENV = rosetta\nPYTHON = /usr/bin/python3\nPIP = ${PYTHON} -m pip\nelse\nCONDA_ENV = cplex\nPYTHON = python ${PYTHON_ENV}\nPIP = gsudo ${PYTHON} -m pip\nendif\n#### ALWAYS REMEMBER TO EXPORT USEFUL VARIABLES ####\nexport OS_TYPE PLATFORM PYTHON PYTHON_ENV CONDA_ENV\n#### ALWAYS REMEMBER TO EXPORT USEFUL VARIABLES ####\nRENDERED_CODE = conscious_struct.py hid_utils.py\nRENDER_UTILS = jinja_utils.py pyright_utils.py\nUTILS = log_utils.py ${RENDER_UTILS}\nUTILS_SYNC_DIR = ../jubilant-adventure2/microgrid_base/\n# shall you dump log to file, not to display it here\nexport RENDERED_CODE\ntest: ${UTILS} ${RENDERED_CODE} test/test_project.py\n\tcd test && ${PYTHON} -m pytest --lf --lfnf=all --capture=tee-sy",
        "type": "code",
        "location": "/Makefile:1-42"
    },
    "7": {
        "file_id": 1,
        "content": "Code snippet sets environment variables based on the operating system and defines necessary files and directories for testing a project.",
        "type": "comment"
    },
    "8": {
        "file_id": 1,
        "content": "s test_project.py\n\t# cd test && ${PYTHON} -m pytest --lf --lfnf=all --capture=tee-sys --log-level=DEBUG test_project.py\n${RENDERED_CODE}: $(addsuffix .j2, ${RENDERED_CODE}) ${RENDER_UTILS}\n\t${PYTHON} render_python_code.py $@\n${UTILS}: $(addprefix  ${UTILS_SYNC_DIR}, ${UTILS})\n\tbash sync_utils.sh $@ ${UTILS_SYNC_DIR}\nsetup:\n\t${PIP} install -r requirements.txt\nKL2XKS.json: hid_utils.py\n\t${PYTHON} hid_utils.py\nsoftware_interface: ${RENDERED_CODE}\n\t${MAKE} -e -C software_capture_hid_control\nhardware_interface: ${RENDERED_CODE}\n\t${MAKE} -e -C hardware_capture_hid_power_control",
        "type": "code",
        "location": "/Makefile:42-61"
    },
    "9": {
        "file_id": 1,
        "content": "This Makefile contains rules for building and executing a Python test project, rendering Python code, synchronizing utils, setting up dependencies, and creating JSON files. It involves various tools like pytest, Python renderer, bash script, and Pip installer.",
        "type": "comment"
    },
    "10": {
        "file_id": 2,
        "content": "/README.md",
        "type": "filepath"
    },
    "11": {
        "file_id": 2,
        "content": "Cybergod is a project utilizing GPT4 MoE architecture, targeting specific audiences with entertainment and utility features. It involves devcontainers, AI training, multiple services/agents, mouse calibration, learning approach, pre-trained data via random keystrokes, and explores AI limitations, visual grids, prompt engineering, different environments (Chromium/Firefox), Docker for isolated execution, and JPEG streaming services for visualization.",
        "type": "summary"
    },
    "12": {
        "file_id": 2,
        "content": "![Cybergod logo](propaganda/logos/cybergod_2.png)\nhttps://github.com/Significant-Gravitas/Auto-GPT/assets/103997068/8e1cd6fe-c49d-4d2b-835d-0ffc9a5a458e\n# Cybergod\n[join discord group](https://discord.gg/eM5vezJvEQ)\n[bilibili live streaming](http://live.bilibili.com/22228498)\n## Intro\nTrained on [The Frozen Forest](https://huggingface.co/datasets/James4Ever0/the_frozen_forest), a [dataset](https://modelscope.cn/datasets/james4ever0/the_frozen_forest/summary) containing random keystrokes, mouse clicks and screen recordings.\nthe openai [universe](https://github.com/openai/universe) is using VNC, almost doing the same thing.\nyou can find some demo models from [there](https://github.com/openai/universe-starter-agent).\ncheck out [SerpentAI](https://github.com/SerpentAI/SerpentAI)\nbut why bother? we can build these things in the same way.\nhuman demonstrations are limited, but random keystrokes are infinite.\ntry to obtain infinite data as pretrained data, then fine-tune on human demonstrations.\n---\n键鼠真神是一种",
        "type": "code",
        "location": "/README.md:1-29"
    },
    "13": {
        "file_id": 2,
        "content": "The code is displaying the Cybergod logo and providing links to various resources such as a discord group, live streaming site, and related datasets. The intro section explains the concept of using random keystrokes for pre-trained data instead of human demonstrations, mentioning similar projects like OpenAI's Universe and SerpentAI.",
        "type": "comment"
    },
    "14": {
        "file_id": 2,
        "content": "意识形态\ncybergod is an ideology.\n键鼠真神 又名cybergod 赛博真神\n训练数据集为the frozen forest 随机敲键盘点鼠标 录屏\n奖励函数 如果屏幕发生变化 奖励上一次行为\n避免把系统关机 被锁在屏幕外面\n避免机器卡死： 监测机器是否卡死 如果卡死那么自动外部重启 （重置状态，重新跑脚本）\n连着WEBDAV一起刷新 有filelock\n(直接取消lock权限)\n---\nlooking for using docker for automation, or using some tty-like things for automation.\ndisable ubuntu system authentication?\n---\nmake some server for vm to access to restart the webdav server when you have error.\n---\nagi_workspace happen to be in recycle bin. make sure we have the init files.\nmake sure we can restore our environments in every restart.\n---\nspice-protocol\nfound on utm.app, launch qemu and create spice unix socket.\nhttps://github.com/Shells-com/spice\nhttps://github.com/gnif/PureSpice\nhttps://github.com/citrix-openstack-build/spice-html5\nhttps://github.com/oetcxiaoliu/spice\nhttps://github.com/TotallWAR/spice_protocol\nremmina\n---\n掉盘问题： `cd .`\n(建议直接换个盘 或者换C口的数据线 A口不稳定 或者把硬盘取出来更新固件？)\nc口数据线观测中\n---\nto resolve the display resolution/mouse coordinate range matching issue, use pyautogui to get the resolutio",
        "type": "code",
        "location": "/README.md:29-91"
    },
    "15": {
        "file_id": 2,
        "content": "Ideology and system requirements for cybergod automation.",
        "type": "comment"
    },
    "16": {
        "file_id": 2,
        "content": "n then capture display using that resolution (resize to it)\n---\nGPT4 is using MoE as its architecture.\n---\nthe main objective of AGI is to create another version of itself.\n---\nways of connection:\nvnc, ssh, tty, tmux, hdmi capture & hid emulator, window capture and directed inputs (os specific)\n---\nthe point is not making this exhaustive. it is about making some standard i/o and adapt to every situation.\n---\n改变开发思路：将功能和娱乐相结合\n受众：游戏娱乐向 实用向\n发布程序到steam平台\n为此需要宣传、绘画设计等等\n---\n用elo进行打分 分高的可以在官网有较高的模型权重排名\n---\ntechnically this would not be a normal game. it is a metagame, which is the game of all games. it can play other games, play itself, even create itself.\n---\ndevcontainer is useful for creating reproducible environments locally (if of the same architecture, like x86) or remotely (different architecture, like Apple M1).\n---\nbecause setting this up properly during development is a pain in the ass (in most time), let's pack it up into a docker container, for your safety.\nif you want to release this and use it in prod",
        "type": "code",
        "location": "/README.md:91-137"
    },
    "17": {
        "file_id": 2,
        "content": "The code is providing information about the project's features and objectives. It mentions GPT4 using MoE architecture, different connection ways, adapting to every situation, combining entertainment and utility, target audience, elo scoring, being a metagame that can play other games, and the use of a devcontainer for development environments.",
        "type": "comment"
    },
    "18": {
        "file_id": 2,
        "content": "uction, you can refactor the code, configure platform specific dependencies and send it to devops.\n---\ndevcontainer won't work as expected on windows 11 as we put our repo on external disk\n---\nyour aim is too damn big! shall you begin to train some primitive neural network with functionality of only emitting and receiving ascii words, even just a single character like 'C'. get your hands dirty!\n---\nthe basic docker service is just like havoc. it does not contain anything 'intelligent'. only 'life support'.\nwe plan to containerize chatdev/open-interpreter/autogpt. after that, we will combine the two, and create some 'capitalism' among multiple containers.\nfinally we will create some ever-evolving agent and use that as the building block for the megasystem.\n---\nthe mouse calibration issue can be of major concern. we don't find it anywhere. \nuse active inference or reinforcement learning?\n---\nthanks to our pioneers that guided us some 'aimless' learning, i think it does not matter how we learn specifi",
        "type": "code",
        "location": "/README.md:137-164"
    },
    "19": {
        "file_id": 2,
        "content": "This code appears to contain various notes and comments related to development, containerization, and AI training. The author seems to be working on a project involving containers, AI, and potentially multiple services or agents interacting with each other. Some issues like mouse calibration and the approach for learning are also mentioned.",
        "type": "comment"
    },
    "20": {
        "file_id": 2,
        "content": "c things. when we learn things relevant to our personal goals, we are inevitably going to optimize the algorithm towards our desired values.\nif qstar is just about long term thinking, it could be useful since that is something currently missing in ai systems. when it comes to issues like calibration errors, fixing bugs, handling uncertainties, worse than human. they often get stuck into repetition, never seek for improvements and will not get bored in the loop, which is quite strange and unusual.\n---\ni think you are getting jealous over openai, since they are consisted of the world's smartest asses and come over new ideas every fucking day. but that does not matter. i think i had the idea before. i think everyone around the world had the same damn idea before. we do not need its mercy to teach us the dream we had via abstract symbols and formulas. we do our own. worst of all, they do not trust their ai systems, severely limited the ability of ai, left it overthinking and powerless.\n---\nthe ",
        "type": "code",
        "location": "/README.md:164-175"
    },
    "21": {
        "file_id": 2,
        "content": "This code discusses the limitations of current AI systems and their inability to adapt or optimize towards personal goals. It also expresses frustration with other organizations, such as OpenAI, believing that everyone has had similar ideas before and that they do not trust or fully utilize their AI systems.",
        "type": "comment"
    },
    "22": {
        "file_id": 2,
        "content": "`self-operating-computer` is using visual grid (just putting grid and text over screenshot, kinda like this hackish approach) for calibration. are you sure it is a good idea? do you need some extra channel over this to avoid information loss?\ndoes this work for games as well?\n---\nprompt engineering is a tweak around prior, in order to change posterior. they want to know better prior to get better posterior. kind like searching for the reason behind the decision, backtracking. so why not just use bidirectional or arbitrary directional language models instead of causal models?\n---\ni don't understand active inference. however there is a debate over whether to change the environment to fit prediction, or to change prediction to fit the environment. sounds like quantum entanglement.\n---\nthe reward function is part of the observation, usually not something life critical so it will not be so direct. it is the internal state that will be affected by the observation.\n---\nplay fps at: https://krunker.io/",
        "type": "code",
        "location": "/README.md:175-193"
    },
    "23": {
        "file_id": 2,
        "content": "The code seems to contain a mix of thoughts and questions related to various topics such as visual grids, calibration methods, prompt engineering, active inference, reward functions, and gaming. The author appears to be seeking insights into these areas and is open to suggestions for improvement or alternative approaches.",
        "type": "comment"
    },
    "24": {
        "file_id": 2,
        "content": " (in chromium, not firefox)\n---\nif want to run the program isolated, without interference, you use docker. if want to visualize, use jpeg streaming service and open in browser.\n## Star History\n<img src=\"https://api.star-history.com/svg?repos=james4ever0/agi_computer_control&Timeline\" style=\"filter: invert(100%);\"></img>",
        "type": "code",
        "location": "/README.md:193-201"
    },
    "25": {
        "file_id": 2,
        "content": "This code snippet describes how to run a program in different environments: within Chromium or Firefox, using Docker for isolated execution without interference, and utilizing JPEG streaming services for visualization by opening it in the browser. Additionally, it shows an image of star history from the \"james4ever0/agi_computer_control\" repository on the Star-History website.",
        "type": "comment"
    },
    "26": {
        "file_id": 3,
        "content": "/array_check_ast_parsing.py",
        "type": "filepath"
    },
    "27": {
        "file_id": 3,
        "content": "Reading code and storing its content into a string variable. Parsing the code using ast and iterating through the body elements, printing their attributes and checking for annotations.",
        "type": "summary"
    },
    "28": {
        "file_id": 3,
        "content": "code_path = \"array_static_typecheck.py\"\nimport ast\nimport rich\nwith open(code_path, \"r\") as f:\n    content = f.read()\n    tree = ast.parse(content)\n    for el in tree.body:\n        rich.print(el.__dict__)\n        # rich.print(dir(el))\n        print(ann:=getattr(el, \"annotation\", None))\n        if ann:\n            print(ast.unparse(el), el) # ast.AnnAssign",
        "type": "code",
        "location": "/array_check_ast_parsing.py:1-14"
    },
    "29": {
        "file_id": 3,
        "content": "Reading code and storing its content into a string variable. Parsing the code using ast and iterating through the body elements, printing their attributes and checking for annotations.",
        "type": "comment"
    },
    "30": {
        "file_id": 4,
        "content": "/array_shape_typecheck.py",
        "type": "filepath"
    },
    "31": {
        "file_id": 4,
        "content": "Code creates 2D arrays 'arr' and 'arr2', defines class 'Array' with addition and absolute value methods, and uses numpy and Literal casting. It retrieves annotations from 'myArr' and suggests using Jinja2 macros for type checking/annotation work to prevent errors.",
        "type": "summary"
    },
    "32": {
        "file_id": 4,
        "content": "from typing_extensions import reveal_type\nimport numpy as np\n# from typing import Tuple\n# from numpy.typing import NDArray, DTypeLike\nfrom nptyping import NDArray, Shape, Float  # type: ignore\nfrom typing import Any\nM = 5\nN = 10\narr: NDArray[Shape[\"5, 10\"], Any] = np.zeros((M, N))\narr2: NDArray[Shape[\"10, 5\"], Any] = np.zeros((N, M))\n# no annotation!\nimport beartype  # type:ignore\n@beartype.beartype  # check before run?\ndef add_arrays(\n    arr1: NDArray[Shape[\"5, 10\"], Any], arr2: NDArray[Shape[\"5, 10\"], Any]\n) -> NDArray[Shape[\"5, 10\"], Any]:\n    result = arr1 + arr2\n    return result\nmyarr = add_arrays(arr, arr)  # no issue?\n# myarr = add_arrays(arr, arr2)  # only beartype shows issue.\nreveal_type(myarr)\n# from jaxtyping import Array\n# not typechecking. import from jax.\n# from jax import Array  # type: ignore\n# import jaxtyping  # type: ignore\n# from typing_extensions import TypeAlias\nfrom typing import cast\n# mTypeAlias: TypeAlias = jaxtyping.Float[Array, \"dim1 dim2\"]\n# arr3 = cast(mTypeAlias, np.array([[1, 2,",
        "type": "code",
        "location": "/array_shape_typecheck.py:1-38"
    },
    "33": {
        "file_id": 4,
        "content": "Code is defining variables `arr` and `arr2` as numpy arrays with shapes 5x10 and 10x5 respectively.\nFunction `add_arrays` takes two numpy arrays of shape 5x10 as input and returns an output numpy array also of shape 5x10.\nVariable `myarr` is assigned the result of calling function `add_arrays` with arrays `arr` and `arr`.\nFunction `reveal_type(myarr)` reveals the actual type of variable `myarr`.",
        "type": "comment"
    },
    "34": {
        "file_id": 4,
        "content": " 3]]))\n# # arr3: mTypeAlias = np.array([[1, 2, 3]])\n# arr4: jaxtyping.Float[Array, \"dim1 dim3\"] = np.array([[1, 2, 3, 5]])\n# @beartype.beartype\n# def add2(a: mTypeAlias, b: mTypeAlias) -> mTypeAlias:\n#     return a + b\n# # arr5 = add2(arr3, arr4)\n# arr5 = add2(arr3, arr3)  # still not working.\n# from typing import TypeVar, Generic\n# from typing_extensions import TypeVarTuple, Unpack\n# DType = TypeVar(\"DType\")\n# Shape = TypeVarTuple(\"Shape\")\n# class Array(Generic[DType, Unpack[Shape]]):\n#     def __abs__(self) -> Array[DType, Unpack[Shape]]:\n#         ...\n#     def __add__(\n#         self, other: Array[DType, Unpack[Shape]]\n#     ) -> Array[DType, Unpack[Shape]]:\n#         ...\n# from typing import Literal\n# arr9 = cast(Array[int, Literal[1], Literal[3]], np.array([[1, 2, 3]]))\n# arr10 = cast(Array[int, Literal[1], Literal[4]], np.array([[1, 2, 3, 4]]))\n# arr11 = arr9 + arr10  # checked!\n# arr11 = arr9+arr9\nfrom typing_extensions import Annotated\nmyType = Annotated[np.ndarray, 20, 30]\nmyArr: myType = np.zeros((20",
        "type": "code",
        "location": "/array_shape_typecheck.py:38-80"
    },
    "35": {
        "file_id": 4,
        "content": "37-46: Define 2D array 'arr3' with type 'mTypeAlias'.\n47-59: Define 2D array 'arr4' with type 'jaxtyping.Float[Array, \"dim1 dim3\"]'.\n60-68: Define a function 'add2' that takes two 'mTypeAlias' arguments and returns 'mTypeAlias'.\n69-72: Add arrays 'arr3' and 'arr4' using the 'add2' function, but the result is not working as expected.\n75-80: Define generic class 'Array' with type variables for data type and shape tuple.\n81-85: Define '__abs__' method in 'Array' class to return an array of the same data type and shape.\n86-90: Define '__add__' method in 'Array' class to perform addition of two arrays with matching data type and shape.\n92-100: Define 'arr9' as a casted 2D array using numpy and 'Literal' from typing for specific dimensions.\n101-111: Define 'arr10' as a casted 2D array using numpy and 'Literal' from typing for specific dimensions.\n112-114: Add 'arr9' and 'arr10', and store the result in 'arr11'.\n115-117: Add 'arr9' to itself, and store the result in 'arr11'.\n118-123: Define a new type 'myType' using Annotated from typing_extensions for np.ndarray with specific dimensions.\n124-126: Create an instance of 'myType', initialize it with zeros, and store the result in 'myArr'.",
        "type": "comment"
    },
    "36": {
        "file_id": 4,
        "content": ", 30))\n# how to get that annotated value?\n# print('ANNOTATION?',myArr.__annotations__)\nprint(__annotations__)\n# {'arr': NDArray[Shape['5, 10'], Any], 'arr2': NDArray[Shape['10, 5'], Any], 'myArr': typing_extensions.Annotated[numpy.ndarray, 20, 30]}\n# one such way to prevent errors is to delegate some type checking/annotation work to jinja2. use macro to generate annotations.",
        "type": "code",
        "location": "/array_shape_typecheck.py:80-86"
    },
    "37": {
        "file_id": 4,
        "content": "The code is attempting to retrieve annotations from a variable 'myArr'. It first prints the current annotations using `__annotations__` and then mentions the idea of preventing errors by delegating type checking/annotation work to Jinja2 using macros for generating annotations.",
        "type": "comment"
    },
    "38": {
        "file_id": 5,
        "content": "/array_static_typecheck.py",
        "type": "filepath"
    },
    "39": {
        "file_id": 5,
        "content": "The code defines a generic class for arrays, performs operations like absolute value and addition, uses NewType to define specific types, and employs annotations for operations and shape constraints. It also includes a variable 'val4' that can be simplified using SymPy.",
        "type": "summary"
    },
    "40": {
        "file_id": 5,
        "content": "# link: https://taoa.io/posts/Shape-typing-numpy-with-pyright-and-variadic-generics\n# PEP 646: https://peps.python.org/pep-0646/\n# mypy --enable-incomplete-feature=Unpack --enable-incomplete-feature=TypeVarTuple  array_static_typecheck.py\nfrom typing import TypeVar, Generic, NewType, Literal\nfrom typing_extensions import TypeVarTuple, Unpack, Self, Annotated\nDType = TypeVar(\"DType\")\nShape = TypeVarTuple(\"Shape\")\nclass Array(Generic[DType, Unpack[Shape]]):\n    def __abs__(self) -> Self:\n        ...\n    def special_ops(self, a: Annotated[int, 2, 3]) -> Annotated[int, 1, 2]:\n        ...\n    def __add__(self, other: Self) -> Self:\n        ...\nHeight = NewType(\"Height\", int)\nWidth = NewType(\"Width\", int)\nx: Array[float, Height, Width] = Array()\ny: Array[float, Literal[1], Literal[1]] = Array()\nz = abs(y)\nh = x + y\na0: Annotated[float, 1, 2] = 1\nx.special_ops(a0)  # annotated will not be checked here.\nval: int\nval2: \"annotated_info\"  # <ast.Constant object at 0x101306290>\n# T = TypeVar(\"T\")\n# T2 = TypeVar(\"T2\")\nval3:",
        "type": "code",
        "location": "/array_static_typecheck.py:1-39"
    },
    "41": {
        "file_id": 5,
        "content": "This code is defining a generic class for arrays, implementing type checking using type variables, and performing array operations such as absolute value and addition. It also utilizes NewType to define specific types like Height and Width. The code includes annotations for specific operations and shape constraints.",
        "type": "comment"
    },
    "42": {
        "file_id": 5,
        "content": " Annotated[\n    int, T * T2\n]  # which can only be understood by some type checker. passing this to sympy will be much better.\nval4: Annotated[int, \"T*T2\"]\nval4: Annotated[int, \"T*T3\"] # this is valid, for now.",
        "type": "code",
        "location": "/array_static_typecheck.py:39-43"
    },
    "43": {
        "file_id": 5,
        "content": "The code defines a variable 'val4' annotated with types \"T*T2\" and later changes it to \"T*T3\". This variable is understood by some type checker and could be simplified using SymPy.",
        "type": "comment"
    },
    "44": {
        "file_id": 6,
        "content": "/autogui.py",
        "type": "filepath"
    },
    "45": {
        "file_id": 6,
        "content": "Both comments discuss Python scripts that utilize modules such as \"autogui\" or \"win32gui\" to capture and record GUI input, simulate keystrokes with a delay, and playback recorded events while allowing screenshot capturing.",
        "type": "summary"
    },
    "46": {
        "file_id": 6,
        "content": "import pyautogui, time, sys, os, win32api, win32gui, win32con, datetime, pyHook, pythoncom\nfrom optparse import OptionParser\n'''\nPython Automated Actions Script by Ian Mckay\nVersion 0.1 - 20151217\n'''\npyautogui.PAUSE = 0\npyautogui.FAILSAFE = True\nmain_thread_id = win32api.GetCurrentThreadId()\nevents = []\nrecording = False\ndef OnMouseEvent(event):\n\tglobal events\n\tglobal recording\n\tif (event.Message!=512): # 512 is mouse move\n\t\t'''\n\t\tprint('MessageName:',event.MessageName)\n\t\tprint('Message:',event.Message)\n\t\tprint('Time:',event.Time)\n\t\tprint('Window:',event.Window)\n\t\tprint('WindowName:',event.WindowName)\n\t\tprint('Position:',event.Position)\n\t\tprint('Wheel:',event.Wheel)\n\t\tprint('Injected:',event.Injected)\n\t\tprint('---')\n\t\t'''\n\t\tif (recording==True):\n\t\t\tevents.append([event.Position[1],event.Position[0],event.Message,event.Time,\"2\"])\n\treturn True\ndef OnKeyboardEvent(event):\n\tglobal hm\n\tglobal events\n\tglobal recording\n\tglobal starttime\n\tglobal main_thread_id\n\t'''\n\tprint('MessageName:',event.MessageName)\n\tprint('Message:',event",
        "type": "code",
        "location": "/autogui.py:1-45"
    },
    "47": {
        "file_id": 6,
        "content": "This code imports various libraries and defines several functions for automating user actions, such as capturing mouse and keyboard events. It appears to be part of an automated action script with the capability to record and playback user input.",
        "type": "comment"
    },
    "48": {
        "file_id": 6,
        "content": ".Message)\n\tprint('Time:',event.Time)\n\tprint('Window:',event.Window)\n\tprint('WindowName:',event.WindowName)\n\tprint('Ascii:', event.Ascii, chr(event.Ascii))\n\tprint('Key:', event.Key)\n\tprint('KeyID:', event.KeyID)\n\tprint('ScanCode:', event.ScanCode)\n\tprint('Extended:', event.Extended)\n\tprint('Injected:', event.Injected)\n\tprint('Alt', event.Alt)\n\tprint('Transition', event.Transition)\n\tprint('---')\n\t'''\n\tif (recording==True):\n\t\tif (event.Key==\"End\"):\n\t\t\thm.UnhookKeyboard()\n\t\t\thm.UnhookMouse()\n\t\t\twin32api.PostThreadMessage(main_thread_id, win32con.WM_QUIT, 0, 0);\n\t\t\tevents.append([\"0\",\"0\",\"1\",event.Time,\"0\"])\n\t\t\tprint(\"Ended recording\")\n\t\t\tprint('\\a')\n\t\t\treturn False\n\t\telse:\n\t\t\tevents.append([event.Extended,event.KeyID,event.Message,event.Time,\"1\"])\n\tif (recording==False):\n\t\tif (event.Key==\"Home\" and event.Message==257):\n\t\t\tstarttime = datetime.datetime.now().time()\n\t\t\trecording = True\n\t\t\tprint(\"Started recording\")\n\t\t\tprint('\\a')\n\t\t\tevents.append([\"0\",\"0\",\"0\",event.Time,\"0\"])\n\t\t\treturn False\n\treturn True\ndef record():\n\tglobal",
        "type": "code",
        "location": "/autogui.py:45-84"
    },
    "49": {
        "file_id": 6,
        "content": "This code is handling keyboard and mouse events. If the \"End\" key is pressed while recording, it will stop recording and end the program. If the \"Home\" key is pressed, it starts recording. The events are being appended to a list for future use.",
        "type": "comment"
    },
    "50": {
        "file_id": 6,
        "content": " hm\n\tprint(\"Hooking now...\")\n\thm = pyHook.HookManager()\n\thm.MouseAll = OnMouseEvent\n\thm.KeyAll = OnKeyboardEvent\n\thm.HookMouse()\n\thm.HookKeyboard()\n\tprint(\"Hooked\")\n\tpythoncom.PumpMessages()\n\tprint(\"Exporting...\")\n\tf = open('recording.txt', 'w+')\n\tfor event in events:\n\t\tf.write(str(event.pop()) + ',' + str(event.pop()) + ',' + str(event.pop()) + ',' + str(event.pop()) + ',' + str(event.pop()) + '\\n')\n\tf.close()\n\tprint(\"Ending...\")\ndef play():\n\tprint(\"Starting in 2 secs...\")\n\ttime.sleep(2)\n\tlasttime=False\n\twith open('recording.txt') as fp:\n\t\tfor line in fp:\n\t\t\telements = line.split(',')\n\t\t\teventdata2 = int(elements.pop().replace('\\n',''))\n\t\t\teventdata1 = int(elements.pop())\n\t\t\teventsubtype = int(elements.pop())\n\t\t\teventtime = int(elements.pop())\n\t\t\teventtype = int(elements.pop())\n\t\t\tif (lasttime==False):\n\t\t\t\tif (eventtype==0 and eventsubtype==0):\n\t\t\t\t\tlasttime=eventtime\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Source data error! (eventtype=\" + str(eventtype) + \", eventsubtype=\" + str(eventsubtype) + \")\")\n\t\t\t\t\tsys.exit(1)\n\t\t\telif (eventty",
        "type": "code",
        "location": "/autogui.py:84-122"
    },
    "51": {
        "file_id": 6,
        "content": "Hooking the mouse and keyboard events, recording them in a file named 'recording.txt', and then playing back the recorded events.",
        "type": "comment"
    },
    "52": {
        "file_id": 6,
        "content": "pe==1):\n\t\t\t\ttime.sleep(max((eventtime-lasttime)/1000,0.02)) # At least 20ms between everything\n\t\t\t\tif (eventdata1>32 and eventdata1<127 and eventdata2==0):\n\t\t\t\t\tkey = chr(eventdata1).lower()\n\t\t\t\telif (eventdata1==91 and eventdata2==1):\n\t\t\t\t\tkey = \"winleft\"\n\t\t\t\telif (eventdata1==9 and eventdata2==0):\n\t\t\t\t\tkey = \"tab\"\n\t\t\t\telif (eventdata1==20 and eventdata2==0):\n\t\t\t\t\tkey = \"capslock\"\n\t\t\t\telif (eventdata1==160 and eventdata2==0):\n\t\t\t\t\tkey = \"shiftleft\"\n\t\t\t\telif (eventdata1==162 and eventdata2==0):\n\t\t\t\t\tkey = \"ctrlleft\"\n\t\t\t\telif (eventdata1==164 and eventdata2==0):\n\t\t\t\t\tkey = \"altleft\"\n\t\t\t\telif (eventdata1==32 and eventdata2==0):\n\t\t\t\t\tkey = \"space\"\n\t\t\t\telif (eventdata1==165 and eventdata2==1):\n\t\t\t\t\tkey = \"altright\"\n\t\t\t\telif (eventdata1==163 and eventdata2==1):\n\t\t\t\t\tkey = \"ctrlright\"\n\t\t\t\telif (eventdata1==37 and eventdata2==1):\n\t\t\t\t\tkey = \"left\"\n\t\t\t\telif (eventdata1==40 and eventdata2==1):\n\t\t\t\t\tkey = \"down\"\n\t\t\t\telif (eventdata1==39 and eventdata2==1):\n\t\t\t\t\tkey = \"right\"\n\t\t\t\telif (eventdata1==161 and eventdata2==1):\n\t\t\t",
        "type": "code",
        "location": "/autogui.py:122-152"
    },
    "53": {
        "file_id": 6,
        "content": "This code handles USB HID events for keyboard inputs. It ensures at least 20ms between events and maps specific event data to corresponding keys or key combinations, such as \"winleft\" for the Windows key, \"capslock\", and various arrow/directional keys.",
        "type": "comment"
    },
    "54": {
        "file_id": 6,
        "content": "\t\tkey = \"shiftright\"\n\t\t\t\telif (eventdata1==38 and eventdata2==1):\n\t\t\t\t\tkey = \"up\"\n\t\t\t\telif (eventdata1==34 and eventdata2==1):\n\t\t\t\t\tkey = \"pgdn\"\n\t\t\t\telif (eventdata1==33 and eventdata2==1):\n\t\t\t\t\tkey = \"pgup\"\n\t\t\t\telif (eventdata1==8 and eventdata2==0):\n\t\t\t\t\tkey = \"backspace\"\n\t\t\t\telif (eventdata1==44 and eventdata2==1):\n\t\t\t\t\tkey = \"printscreen\"\n\t\t\t\telif (eventdata1==46 and eventdata2==1):\n\t\t\t\t\tkey = \"delete\"\n\t\t\t\telif (eventdata1==27 and eventdata2==0):\n\t\t\t\t\tkey = \"esc\"\n\t\t\t\telif (eventdata1==13 and eventdata2==0):\n\t\t\t\t\tkey = \"enter\"\n\t\t\t\telif (eventdata1==112 and eventdata2==0):\n\t\t\t\t\tkey = \"f1\"\n\t\t\t\telif (eventdata1==113 and eventdata2==0):\n\t\t\t\t\tkey = \"f2\"\n\t\t\t\telif (eventdata1==114 and eventdata2==0):\n\t\t\t\t\tkey = \"f3\"\n\t\t\t\telif (eventdata1==115 and eventdata2==0):\n\t\t\t\t\tkey = \"f4\"\n\t\t\t\telif (eventdata1==116 and eventdata2==0):\n\t\t\t\t\tkey = \"f5\"\n\t\t\t\telif (eventdata1==117 and eventdata2==0):\n\t\t\t\t\tkey = \"f6\"\n\t\t\t\telif (eventdata1==118 and eventdata2==0):\n\t\t\t\t\tkey = \"f7\"\n\t\t\t\telif (eventdata1==119 and eventdata2==0):\n\t\t\t\t\tkey = \"f8\"\n\t",
        "type": "code",
        "location": "/autogui.py:152-185"
    },
    "55": {
        "file_id": 6,
        "content": "This code is mapping different keyboard event combinations to their corresponding keys.",
        "type": "comment"
    },
    "56": {
        "file_id": 6,
        "content": "\t\t\telif (eventdata1==120 and eventdata2==0):\n\t\t\t\t\tkey = \"f9\"\n\t\t\t\telif (eventdata1==121 and eventdata2==0):\n\t\t\t\t\tkey = \"f10\"\n\t\t\t\telif (eventdata1==122 and eventdata2==0):\n\t\t\t\t\tkey = \"f11\"\n\t\t\t\telif (eventdata1==123 and eventdata2==0):\n\t\t\t\t\tkey = \"f12\"\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Skipping unknown keycode: \" + str(eventdata1))\n\t\t\t\t\tkey = False\n\t\t\t\tif (eventsubtype==256 or eventsubtype==260): # I think 260 is a \"virtual keystroke\"\n\t\t\t\t\tif (key!=False):\n\t\t\t\t\t\tpyautogui.keyDown(key)\n\t\t\t\telif (eventsubtype==257):\n\t\t\t\t\tif (key!=False):\n\t\t\t\t\t\tpyautogui.keyUp(key)\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Bad keyboard subtype!\")\n\t\t\t\t\tsys.exit(1)\n\t\t\t\tlasttime=eventtime\n\t\t\telif (eventtype==2):\n\t\t\t\ttime.sleep(max((eventtime-lasttime)/1000,0.02)) # At least 20ms between everything\n\t\t\t\tif (eventsubtype==513):\n\t\t\t\t\tpyautogui.mouseDown(x=eventdata1, y=eventdata2, button='left')\n\t\t\t\telif (eventsubtype==514):\n\t\t\t\t\tpyautogui.mouseUp(x=eventdata1, y=eventdata2, button='left')\n\t\t\t\telse:\n\t\t\t\t\tprint(\"Bad mouse subtype!\")\n\t\t\t\t\tsys.exit(1)\n\t\t\t\tlasttime=eventtime\n\t\t\tel",
        "type": "code",
        "location": "/autogui.py:185-220"
    },
    "57": {
        "file_id": 6,
        "content": "This code is part of an autogui module and appears to be processing keyboard and mouse events from a device. If the eventdata matches specific keycodes, it assigns corresponding keys (\"f9\"-\"f12\") to the variable \"key\". The code then checks if it's a virtual keystroke (eventsubtype 256 or 260) and triggers either pyautogui.keyDown() or pyautogui.keyUp() function accordingly. If the event is related to mouse movement, it uses pyautogui.mouseDown() or pyautogui.mouseUp() depending on the eventsubtype. It ensures there's at least a 20ms delay between events and handles unknown subtypes by exiting the program.",
        "type": "comment"
    },
    "58": {
        "file_id": 6,
        "content": "if (eventtype==0 and eventsubtype==1):\n\t\t\t\tprint(\"Done.\")\n\t\t\t\tsys.exit(0)\n\t\t\telse:\n\t\t\t\tprint(\"Bad source major type!\")\n\t\t\t\tsys.exit(1)\n\tprint(\"Done playing\")\n\tprint('\\a')\ndef main():\n\tusage = \"usage: %prog [options]\"\n\tparser = OptionParser(usage=usage)\n\tparser.add_option(\"-r\", \"--record\", action=\"store_true\", dest=\"do_record\", default=False, help=\"record a session of input\")\n\tparser.add_option(\"-p\", \"--play\", action=\"store_true\", dest=\"do_play\", default=False, help=\"play a session of input\")\n\t(options, args) = parser.parse_args()\n\tif (options.do_record==False and options.do_play==False):\n\t\tparser.print_help()\n\t\tsys.exit(0)\n\tif (options.do_record==True):\n\t\trecord()\n\tif (options.do_play==True):\n\t\tplay()\nif __name__ == \"__main__\":\n\tmain()\n'''\nPLAY AREA\nw=win32gui\ntitle=w.GetWindowText(w.GetForegroundWindow())\nim = None\ndef capture():\n\tglobal im\n\t#pyautogui.click(1200, 500)\n\t#pyautogui.typewrite('About to close window!')\n\t#time.sleep(2)\n\t#pyautogui.hotkey('alt', 'f4')\n\t#pyautogui.screenshot()\n\tposx, posy = pyautogui.positio",
        "type": "code",
        "location": "/autogui.py:220-259"
    },
    "59": {
        "file_id": 6,
        "content": "This code is part of a Python script for an auto-GUI (graphical user interface) tool. It allows the user to record input and playback sessions of input. The script uses the \"win32gui\" module, which provides GUI window handling functions.\n\nThe first part of the code checks if the command line arguments are correct and then proceeds with either recording or playing input sessions. If no action is specified, it displays help information and exits.\n\nIn the PLAY AREA section, there is a function called \"capture\" which captures a screenshot when a window with a specific title is active. However, the code seems incomplete as it does not include the necessary imports for the functions used within the code.",
        "type": "comment"
    },
    "60": {
        "file_id": 6,
        "content": "n()\n\tprint(str(posx) + \",\" + str(posy))\n\tim = pyautogui.screenshot('img.png',region=(posx-20,posy-20,40,40))\ndef replay():\n\tglobal im\n\tloc = pyautogui.locateOnScreen(im)\n\tlocx, locy = pyautogui.center(loc)\n\tpyautogui.click(locx, locy)\n#capture()\n#replay()\ndef post_keys(hwnd, i):\n\twin32api.SendMessage(hwnd, win32con.WM_KEYDOWN, i, 0)\n\twin32api.SendMessage(hwnd, win32con.WM_KEYUP, i, 0)\n'''",
        "type": "code",
        "location": "/autogui.py:259-272"
    },
    "61": {
        "file_id": 6,
        "content": "The code captures a screenshot of the current window and searches for the image within it. If found, it clicks on the location. The `post_keys()` function simulates key down and key up events.",
        "type": "comment"
    },
    "62": {
        "file_id": 7,
        "content": "/compose.yaml",
        "type": "filepath"
    },
    "63": {
        "file_id": 7,
        "content": "Creating two services, Xorg-VNC-Client and VNC-Server with specified images.",
        "type": "summary"
    },
    "64": {
        "file_id": 7,
        "content": "services:\n  xorg-vnc-client:\n    build: .\n  vnc-server:\n    image: \"dorowu/ubuntu-desktop-lxde-vnc:focal\"",
        "type": "code",
        "location": "/compose.yaml:1-5"
    },
    "65": {
        "file_id": 7,
        "content": "Creating two services, Xorg-VNC-Client and VNC-Server with specified images.",
        "type": "comment"
    },
    "66": {
        "file_id": 8,
        "content": "/config.py",
        "type": "filepath"
    },
    "67": {
        "file_id": 8,
        "content": "Sets the simulation timestep to 0.03 seconds and defines the file path for storing state data as \"states.jsonl\".",
        "type": "summary"
    },
    "68": {
        "file_id": 8,
        "content": "timestep = 0.03\nfilePath = \"states.jsonl\"",
        "type": "code",
        "location": "/config.py:1-2"
    },
    "69": {
        "file_id": 8,
        "content": "Sets the simulation timestep to 0.03 seconds and defines the file path for storing state data as \"states.jsonl\".",
        "type": "comment"
    },
    "70": {
        "file_id": 9,
        "content": "/conscious_struct.py",
        "type": "filepath"
    },
    "71": {
        "file_id": 9,
        "content": "The code processes HID actions, handles various inputs and formats, validates types, includes classes for data management and training, ensures proper shape and logging. It manages training data in an image processing program using SequentialEvalQueue class with LSTM, fixes bugs 9 and 13, optimizes data type bits, prepares input data for RNN, uses Einops library with ViTDecoder to process selected image bits, rearranges and copies data based on shared and exclusive indices, performs 1D convolution with filters, and generates output.",
        "type": "summary"
    },
    "72": {
        "file_id": 9,
        "content": "# TODO: design a way to \"observe\" current holding keys, current mouse location, encode that observation and feed into model input along with screen image data\n# import pynput\n# no such dependency when training.\nimport einops\nimport os\nimport numpy as np\nimport cv2\nimport ast\nfrom pydantic import BaseModel, validator\nfrom typing import Union, Mapping, List\n# import logging\nfrom log_utils import logger\nfrom pydantic_numpy import NDArray\nimport torch\ntry:\n    from typing import Literal\nexcept:\n    from typing_extensions import Literal  # this is a failsafe.\ntry:\n    from typing import TypeAlias\nexcept:\n    from typing_extensions import TypeAlias\n##############\n#  HID BASE  #\n##############\nclass HIDActionTypes:\n    keyboard_action_types: TypeAlias = Literal[\n        \"key_press\",\n        \"key_release\",\n    ]\n    mouse_action_types: TypeAlias = Literal[\n        \"mouse_move\",\n        \"mouse_click\",\n        \"mouse_scroll\",\n    ]\n    action_types: TypeAlias = Literal[\n        keyboard_action_types,\n        mouse_action_types,\n    ",
        "type": "code",
        "location": "/conscious_struct.py:1-46"
    },
    "73": {
        "file_id": 9,
        "content": "This code is importing necessary libraries and defining classes for handling HID (Human Interface Device) actions such as keyboard and mouse events. It also includes type hints for different action types.",
        "type": "comment"
    },
    "74": {
        "file_id": 9,
        "content": "]\n    mouse_buttons: TypeAlias = Literal[\n        \"Button.left\",\n        \"Button.middle\",\n        \"Button.right\",\n    ]\n    keys: TypeAlias = Literal[\n        \"\"\"','\"\"\",\n        \"\"\"'.'\"\"\",\n        \"\"\"'/'\"\"\",\n        \"\"\"';'\"\"\",\n        \"\"\"\\\"'\\\"\"\"\",\n        \"\"\"'['\"\"\",\n        \"\"\"']'\"\"\",\n        \"\"\"'\\\\'\"\"\",\n        \"\"\"'='\"\"\",\n        \"\"\"'-'\"\"\",\n        \"\"\"'0'\"\"\",\n        \"\"\"'9'\"\"\",\n        \"\"\"'8'\"\"\",\n        \"\"\"'7'\"\"\",\n        \"\"\"'6'\"\"\",\n        \"\"\"'5'\"\"\",\n        \"\"\"'4'\"\"\",\n        \"\"\"'3'\"\"\",\n        \"\"\"'2'\"\"\",\n        \"\"\"'1'\"\"\",\n        \"\"\"'`'\"\"\",\n        \"\"\"'a'\"\"\",\n        \"\"\"'b'\"\"\",\n        \"\"\"'c'\"\"\",\n        \"\"\"'d'\"\"\",\n        \"\"\"'e'\"\"\",\n        \"\"\"'f'\"\"\",\n        \"\"\"'g'\"\"\",\n        \"\"\"'h'\"\"\",\n        \"\"\"'i'\"\"\",\n        \"\"\"'j'\"\"\",\n        \"\"\"'k'\"\"\",\n        \"\"\"'l'\"\"\",\n        \"\"\"'m'\"\"\",\n        \"\"\"'n'\"\"\",\n        \"\"\"'o'\"\"\",\n        \"\"\"'p'\"\"\",\n        \"\"\"'q'\"\"\",\n        \"\"\"'r'\"\"\",\n        \"\"\"'s'\"\"\",\n        \"\"\"'t'\"\"\",\n        \"\"\"'u'\"\"\",\n        \"\"\"'v'\"\"\",\n        \"\"\"'w'\"\"\",\n        \"\"\"'x'\"\"\",\n        \"\"\"'y'\"\"\",\n        \"\"\"'z'\"\"\",\n       ",
        "type": "code",
        "location": "/conscious_struct.py:46-100"
    },
    "75": {
        "file_id": 9,
        "content": "This code defines two type aliases, \"mouse_buttons\" and \"keys\", which represent specific button presses and key presses respectively. The mouse_buttons alias includes the literal strings for left, middle, and right mouse buttons, while the keys alias contains a list of literal strings representing commonly used keys on a keyboard.",
        "type": "comment"
    },
    "76": {
        "file_id": 9,
        "content": " \"Key.alt\",  # check pynput.keyboard.Key\n        \"Key.alt_r\",\n        \"Key.backspace\",\n        \"Key.caps_lock\",\n        \"Key.cmd\",\n        \"Key.cmd_r\",\n        \"Key.ctrl\",\n        \"Key.ctrl_r\",\n        \"Key.delete\",\n        \"Key.down\",\n        \"Key.end\",\n        \"Key.enter\",\n        \"Key.esc\",\n        \"Key.f1\",\n        \"Key.f2\",\n        \"Key.f3\",\n        \"Key.f4\",\n        \"Key.f5\",\n        \"Key.f6\",\n        \"Key.f7\",\n        \"Key.f8\",\n        \"Key.f9\",\n        \"Key.f10\",\n        \"Key.f11\",\n        \"Key.f12\",\n        \"Key.f13\",\n        \"Key.f14\",\n        \"Key.f15\",\n        \"Key.f16\",\n        \"Key.f17\",\n        \"Key.f18\",\n        \"Key.f19\",\n        \"Key.f20\",\n        \"Key.home\",\n        \"Key.left\",\n        \"Key.page_down\",\n        \"Key.page_up\",\n        \"Key.right\",\n        \"Key.shift\",\n        \"Key.shift_r\",\n        \"Key.space\",\n        \"Key.tab\",\n        \"Key.up\",\n    ]\nclass HIDActionBase:\n    mouse_resolution: int = 1000\n    keyboard_action_types = list(HIDActionTypes.keyboard_action_types.__args__)\n    mouse_action_types = list(",
        "type": "code",
        "location": "/conscious_struct.py:100-149"
    },
    "77": {
        "file_id": 9,
        "content": "This code defines a list of key names for the pynput.keyboard.Key module and creates two lists for keyboard action types. The first class, HIDActionBase, has two attributes: mouse_resolution (1000) and keyboard_action_types (list of keyboard action types).",
        "type": "comment"
    },
    "78": {
        "file_id": 9,
        "content": "HIDActionTypes.mouse_action_types.__args__)\n    action_types = list(HIDActionTypes.action_types.__args__)\n    mouse_buttons = list(HIDActionTypes.mouse_buttons.__args__)\n    keys = list(HIDActionTypes.keys.__args__)\n    length = (\n        len(action_types)\n        + len(keys)\n        + len(mouse_buttons)\n        + 1  # mouse pressed\n        + 4 * mouse_resolution\n    )  # ,\n    #             1)\n    @staticmethod\n    def unshift_keycode(keycode: str) -> Union[str, None]:\n        unshift_keycodes = {\n            \"!\": \"1\",\n            \"@\": \"2\",\n            \"#\": \"3\",\n            \"$\": \"4\",\n            \"%\": \"5\",\n            \"^\": \"6\",\n            \"&\": \"7\",\n            \"*\": \"8\",\n            \"(\": \"9\",\n            \")\": \"0\",\n            \"_\": \"-\",\n            \"+\": \"=\",\n            \"{\": \"[\",\n            \"}\": \"]\",\n            \"|\": \"\\\\\",\n            \":\": \";\",\n            '\"': \"'\",\n            \"<\": \",\",\n            \">\": \".\",\n            \"?\": \"/\",\n            \"~\": \"`\",\n        }\n        ctrl_keycodes = {\n            \"\\x01\": \"a\",\n           ",
        "type": "code",
        "location": "/conscious_struct.py:149-190"
    },
    "79": {
        "file_id": 9,
        "content": "Extracting argument types for action types, mouse buttons, and keys.\nCalculating total length including base type and additional mappings for a given keycode.\nStatic method to unshift keycodes based on specific conditions.",
        "type": "comment"
    },
    "80": {
        "file_id": 9,
        "content": " \"\\x02\": \"b\",\n            \"\\x03\": \"c\",\n            \"\\x04\": \"d\",\n            \"\\x05\": \"e\",\n            \"\\x06\": \"f\",\n            \"\\x07\": \"g\",\n            \"\\x08\": \"h\",\n            \"\\t\": \"i\",\n            \"\\n\": \"j\",\n            \"\\x0b\": \"k\",\n            \"\\x0c\": \"l\",\n            \"\\r\": \"m\",\n            \"\\x0e\": \"n\",\n            \"\\x0f\": \"o\",\n            \"\\x10\": \"p\",\n            \"\\x11\": \"q\",\n            \"\\x12\": \"r\",\n            \"\\x13\": \"s\",\n            \"\\x14\": \"t\",\n            \"\\x15\": \"u\",\n            \"\\x16\": \"v\",\n            \"\\x17\": \"w\",\n            \"\\x18\": \"x\",\n            \"\\x19\": \"y\",\n            \"\\x1a\": \"z\",\n            \"<219>\": \"[\",\n            \"<221>\": \"]\",\n            \"<189>\": \"-\",\n            \"<187>\": \"=\",\n            \"<192>\": \"`\",\n            \"<48>\": \"0\",\n            \"<49>\": \"1\",\n            \"<50>\": \"2\",\n            \"<51>\": \"3\",\n            \"<52>\": \"4\",\n            \"<53>\": \"5\",\n            \"<54>\": \"6\",\n            \"<55>\": \"7\",\n            \"<56>\": \"8\",\n            \"<57>\": \"9\",\n            \"<220>\": \"\\\\\",\n            \"<186>\": \";\",\n ",
        "type": "code",
        "location": "/conscious_struct.py:190-232"
    },
    "81": {
        "file_id": 9,
        "content": "This code is mapping various characters and special keys to their corresponding alphabets and numbers.",
        "type": "comment"
    },
    "82": {
        "file_id": 9,
        "content": "           \"<222>\": \"'\",\n            \"<188>\": \",\",\n            \"<190>\": \".\",\n            \"<191>\": \"/\",\n        }\n        keycode = unshift_keycodes.get(keycode, ctrl_keycodes.get(keycode, keycode))\n        # still, this is something out of concern.\n        if keycode.startswith(\"<\") and keycode.endswith(\">\"):\n            logger.warning(\"Discarding unconvertable keycode: %s\" % keycode)\n            # keycode = pynput.keyboard.KeyCode(int(keycode[1:-1]))\n            return\n        return keycode\n    @staticmethod\n    def uncover_keycode(keycode: str) -> Union[str, None]:\n        if not keycode.startswith(\"Key.\"):\n            keycode_converted = HIDActionBase.unshift_keycode(\n                keycode\n                if keycode.startswith(\"<\") and keycode.endswith(\">\")\n                else ast.literal_eval(keycode)\n            )\n            return keycode_converted\n            # this could be None.\n            # when this is None, simply skip this code. do not end the conversion. skip it.\n        else:\n            ",
        "type": "code",
        "location": "/conscious_struct.py:232-257"
    },
    "83": {
        "file_id": 9,
        "content": "This code is handling keycode conversion for a keyboard input processing system. It maps certain special characters to their corresponding codes, and handles unconvertable keycodes by discarding them. If the keycode starts with \"<\" and ends with \">\", it's treated as a special keycode. The function then attempts to convert the keycode, and if successful, returns the converted keycode; otherwise, it returns None.",
        "type": "comment"
    },
    "84": {
        "file_id": 9,
        "content": "return keycode\nclass HIDAction(BaseModel, HIDActionBase):\n    # static method: from_action\n    # static method: from_ndarray\n    # instance method: to_ndarray\n    # instance method: to_action\n    max_x: int\n    max_y: int\n    action_type: Literal[\n        \"key_press\",  # [\"key_press\", \"'w'\"]\n        \"key_release\",  # [\"key_release\", \"'r'\"]\n        \"mouse_move\",  # [\"mouse_move\", [176.7734375, 580.40625]], \"timeStamp\": 1680247557.125498}\n        \"mouse_click\",  # [\"mouse_click\", [176.7734375, 580.40625, \"Button.left\", true]]\n        \"mouse_scroll\",  # [\"mouse_scroll\", [938.76171875, 318.75, 0, 0]]\n        #         None,  # end_of_action\n    ]  # you need to specify this.\n    key: Union[\n        HIDActionTypes.keys,\n        None,\n    ] = None\n    mouse_button: Union[HIDActionTypes.mouse_buttons, None] = None\n    mouse_pressed: Union[bool, None] = None\n    x: Union[float, None] = None\n    y: Union[float, None] = None\n    dx: Union[float, None] = None\n    dy: Union[float, None] = None\n    @validator(\"max_x\", \"max_",
        "type": "code",
        "location": "/conscious_struct.py:257-286"
    },
    "85": {
        "file_id": 9,
        "content": "This class represents a HID action that can be a key press, key release, mouse move, mouse click, or mouse scroll. It has properties for the action type, key, mouse button, and coordinates. The to_ndarray method converts the instance to a numpy array, while the to_action method returns an action string in the format [\"<type>\", \"<key or coordinates>\"].",
        "type": "comment"
    },
    "86": {
        "file_id": 9,
        "content": "y\")\n    def greater_than_zero(cls, v):\n        assert type(v) == int\n        assert v > 0\n        return v\n    @validator(\"action_type\")\n    def action_type_within_action_types(cls, v):\n        if v:\n            assert v in HIDActionBase.action_types\n        return v\n    @validator(\"key\")\n    def key_within_keys(cls, v):\n        if v:\n            assert v in HIDActionBase.keys\n        return v\n    @validator(\"mouse_button\")\n    def mouse_button_within_mouse_buttons(cls, v):\n        if v:\n            assert v in HIDActionBase.mouse_buttons\n        return v\n    @validator(\"mouse_pressed\")\n    def mouse_pressed_type_check(cls, v):\n        if v:\n            assert type(v) == bool\n        return v\n    @staticmethod\n    def from_action_json(action_json: list, max_x: int, max_y: int):\n        action_type = action_json[0]\n        action_args = action_json[1]\n        construct_args = dict(max_x=max_x, max_y=max_y, action_type=action_type)\n        # BUG: convert single char keys to quoted format.\n        # TODO: make sure ' '",
        "type": "code",
        "location": "/conscious_struct.py:286-324"
    },
    "87": {
        "file_id": 9,
        "content": "This code defines several validator functions and a static method for a class. The validator functions check if the input values are of the correct type and within specific ranges or sets, returning the input if it passes the checks, otherwise raising an exception. The static method, \"from_action_json,\" constructs objects from a JSON list representation, handling some potential issues with single char keys.",
        "type": "comment"
    },
    "88": {
        "file_id": 9,
        "content": " is converted into Key.Space\n        if action_type.startswith(\"key\"):\n            if len(action_args) == 1:\n                if action_args != \"'\":\n                    action_args = f\"'{action_args}'\"\n                else:\n                    action_args = f'\"{action_args}\"'\n            if action_args == repr(\" \"):\n                action_args = \"Key.space\"\n        if action_type == \"key_press\":\n            assert action_args in HIDActionBase.keys\n            construct_args.update(dict(key=action_args))\n        elif action_type == \"key_release\":\n            assert action_args in HIDActionBase.keys\n            construct_args.update(dict(key=action_args))\n        elif action_type == \"mouse_move\":\n            assert action_args[0] >= 0 and action_args[0] <= max_x\n            assert action_args[1] >= 0 and action_args[1] <= max_y\n            construct_args.update(dict(x=action_args[0], y=action_args[1]))\n        elif action_type == \"mouse_click\":\n            assert action_args[0] >= 0 and action_args[0] <= max",
        "type": "code",
        "location": "/conscious_struct.py:324-348"
    },
    "89": {
        "file_id": 9,
        "content": "Checks if the action type is a key press or release, mouse move, or mouse click. If it's a key action, converts it into Key.Space format. For mouse actions, checks that x and y coordinates are within valid ranges. Updates construct_args accordingly.",
        "type": "comment"
    },
    "90": {
        "file_id": 9,
        "content": "_x\n            assert action_args[1] >= 0 and action_args[1] <= max_y\n            assert action_args[2] in HIDActionBase.mouse_buttons\n            assert type(action_args[3]) == bool\n            construct_args.update(\n                dict(\n                    x=action_args[0],\n                    y=action_args[1],\n                    mouse_button=action_args[2],\n                    mouse_pressed=action_args[3],\n                )\n            )\n        elif action_type == \"mouse_scroll\":\n            assert action_args[0] >= 0 and action_args[0] <= max_x\n            assert action_args[1] >= 0 and action_args[1] <= max_y\n            assert action_args[2] >= -max_x and action_args[2] <= max_x\n            assert action_args[3] >= -max_y and action_args[3] <= max_y\n            construct_args.update(\n                dict(\n                    x=action_args[0],\n                    y=action_args[1],\n                    dx=action_args[2],\n                    dy=action_args[3],\n                )\n            )\n        else:\n",
        "type": "code",
        "location": "/conscious_struct.py:348-376"
    },
    "91": {
        "file_id": 9,
        "content": "The code checks the type and validity of input arguments for constructing different types of actions. It updates a dictionary with the action's parameters based on the action type (mouse click or scroll).",
        "type": "comment"
    },
    "92": {
        "file_id": 9,
        "content": "            raise Exception(\n                \"Unknown action type: %s\\naction args: %s\" % (action_type, action_args)\n            )\n        mHIDAction = HIDAction(**construct_args)\n        return mHIDAction\n    @staticmethod\n    def from_ndarray(ndarray: np.ndarray, max_x: int, max_y: int):\n        assert ndarray.shape == (HIDActionBase.length,)\n        cursor = 0\n        action_type_ndarray = ndarray[cursor : cursor + len(HIDActionBase.action_types)]\n        cursor += len(HIDActionBase.action_types)\n        action_type_index = np.argmax(action_type_ndarray)\n        action_type = HIDActionBase.action_types[action_type_index]\n        del action_type_ndarray\n        del action_type_index\n        construct_args = dict(max_x=max_x, max_y=max_y, action_type=action_type)\n        if action_type:\n            key_ndarray = ndarray[cursor : cursor + len(HIDActionBase.keys)]\n            cursor += len(HIDActionBase.keys)\n            key_index = np.argmax(key_ndarray)\n            key = HIDActionBase.keys[key_index]\n    ",
        "type": "code",
        "location": "/conscious_struct.py:376-402"
    },
    "93": {
        "file_id": 9,
        "content": "Raises an exception if the action type is unknown.\nCreates a HIDAction object using given arguments.\nStatic method to create HIDAction from numpy array.\nAsserts that the shape of ndarray is (HIDActionBase.length,).\nGets the action type index from ndarray.\nConstructs construct_args with max_x, max_y, and action_type.\nIf action type exists, gets key index from ndarray and retrieves key.",
        "type": "comment"
    },
    "94": {
        "file_id": 9,
        "content": "        del key_ndarray\n            del key_index\n            mouse_button_ndarray = ndarray[\n                cursor : cursor + len(HIDActionBase.mouse_buttons)\n            ]\n            cursor += len(HIDActionBase.mouse_buttons)\n            mouse_button_index = np.argmax(mouse_button_ndarray)\n            mouse_button = HIDActionBase.mouse_buttons[mouse_button_index]\n            del mouse_button_ndarray\n            del mouse_button_index\n            mouse_pressed_ndarray = ndarray[cursor : cursor + 1]\n            cursor += 1\n            mouse_pressed = bool(mouse_pressed_ndarray[0][0])\n            del mouse_pressed_ndarray\n            x_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            cursor += HIDActionBase.mouse_resolution\n            x_index = np.argmax(x_ndarray)\n            x = (x_index / HIDActionBase.mouse_resolution) * max_x\n            del x_ndarray\n            del x_index\n            y_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            c",
        "type": "code",
        "location": "/conscious_struct.py:402-427"
    },
    "95": {
        "file_id": 9,
        "content": "Deleting variables, processing mouse button and position data, updating cursor position.",
        "type": "comment"
    },
    "96": {
        "file_id": 9,
        "content": "ursor += HIDActionBase.mouse_resolution\n            y_index = np.argmax(y_ndarray)\n            y = (y_index / HIDActionBase.mouse_resolution) * max_y\n            del y_ndarray\n            del y_index\n            dx_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            cursor += HIDActionBase.mouse_resolution\n            dx_index = np.argmax(dx_ndarray)\n            dx = (dx_index / HIDActionBase.mouse_resolution) * 2 * max_x - max_x\n            del dx_ndarray\n            del dx_index\n            dy_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            cursor += HIDActionBase.mouse_resolution\n            dy_index = np.argmax(dy_ndarray)\n            dy = (dy_index / HIDActionBase.mouse_resolution) * 2 * max_y - max_y\n            del dy_ndarray\n            del dy_index\n            if action_type == \"key_press\":\n                construct_args.update(dict(key=key))\n            elif action_type == \"key_release\":\n                construct_args.update(dict(key=ke",
        "type": "code",
        "location": "/conscious_struct.py:427-450"
    },
    "97": {
        "file_id": 9,
        "content": "Calculating mouse and key inputs based on ndarray max index, updating construct_args accordingly.",
        "type": "comment"
    },
    "98": {
        "file_id": 9,
        "content": "y))\n            elif action_type == \"mouse_move\":\n                construct_args.update(dict(x=x, y=y))\n            elif action_type == \"mouse_click\":\n                construct_args.update(\n                    dict(\n                        x=x, y=y, mouse_button=mouse_button, mouse_pressed=mouse_pressed\n                    )\n                )\n            elif action_type == \"mouse_scroll\":\n                construct_args.update(dict(x=x, y=y, dx=dx, dy=dy))\n        else:\n            pass\n        del cursor\n        mHIDAction = HIDAction(**construct_args)\n        return mHIDAction\n    def round_within(self, number: Union[int, float], number_name: str) -> int:\n        result = round(number)\n        if result > self.mouse_resolution:\n            logger.warning(f\"Warning: {number_name} overflow\")\n            logger.warning(f\"Value {result} greater than {self.mouse_resolution}\")\n            return self.mouse_resolution\n        elif result < 0:\n            logger.warning(f\"Warning: {number_name} overflow\")\n         ",
        "type": "code",
        "location": "/conscious_struct.py:450-477"
    },
    "99": {
        "file_id": 9,
        "content": "This code is handling different types of mouse actions. It updates construct_args based on the action type and creates a HIDAction object using those arguments. If the number is greater than mouse_resolution, it logs a warning.",
        "type": "comment"
    }
}