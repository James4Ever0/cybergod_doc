{
    "1100": {
        "file_id": 144,
        "content": "oise = token_prob + noise_prob * probablistic_noise_ratio\n    return token_prob_with_noise\n# demo on how to use thought tokens.\n@beartype\ndef generative_insert_yield_train_pairs(\n    autoregressive_generator: Iterable,\n    target_token_prob_generator: Callable[[torch.Tensor], torch.Tensor],\n    padded_thought_token_locations: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    non_thought_token_vocabulary: list[int],\n    train_window_size: int,\n    probablistic_noise_ratio: ReplaceRatio,\n):\n    for i, (input_tokens, _) in enumerate(autoregressive_generator):\n        with torch.no_grad():\n            output_token_prob = target_token_prob_generator(input_tokens)\n            output_token_prob = add_probablistic_noise_to_prob(output_token_prob,probablistic_noise_ratio)\n        thought_token_locations = crop_target_token_by_index_and_window_size(\n            padded_thought_token_locations, i, train_window_size\n        )\n        target_tokens = generate_target_tokens_with_thought_token_loctions_and_non_t",
        "type": "code",
        "location": "/qstar_my_guess/thought_tokens.py:422-443"
    },
    "1101": {
        "file_id": 144,
        "content": "This function takes an autoregressive generator and a target token probability generator, along with other parameters. It iterates through the input tokens from the autoregressive generator, applying probabilistic noise to the output token probabilities and generating target tokens based on thought token locations and non-thought token vocabulary.",
        "type": "comment"
    },
    "1102": {
        "file_id": 144,
        "content": "hought_token_vocabulary(\n            output_token_prob,\n            thought_token_locations,\n            thought_token_vocabulary,\n            non_thought_token_vocabulary,\n        )\n        yield input_tokens, target_tokens\n# output thought tokens affect input tokens?\nif __name__ == \"__main__\":\n    # begin test\n    base_token_count = 1000\n    thought_token_count = 2000\n    pad_token_idx = base_token_count + thought_token_count\n    total_token_count = pad_token_idx + 1\n    thought_token_insert_rate = 0.2\n    source_batchsize = 1\n    source_seqlen = 20\n    source_size = (source_batchsize, source_seqlen)\n    train_window_size = 10\n    thought_token_vocabulary = [\n        base_token_count + i for i in range(thought_token_count)\n    ]\n    non_thought_token_vocabulary = [\n        i for i in range(total_token_count) if i not in thought_token_vocabulary\n    ]\n    source_tokens = torch.randint(\n        0, base_token_count, source_size\n    )  # okay, lower than upper bound.\n    print(\"[autoregressive]\".center(50, \"-\"))\n   ",
        "type": "code",
        "location": "/qstar_my_guess/thought_tokens.py:443-480"
    },
    "1103": {
        "file_id": 144,
        "content": "This code defines a function that generates thought tokens based on input and target tokens. It also performs testing with random input tokens, inserting thought tokens at a specified rate, and calculates the vocabulary for base, thought, and non-thought tokens. The code then prints a separator line.",
        "type": "comment"
    },
    "1104": {
        "file_id": 144,
        "content": " for input_tokens, target_tokens in insert_thought_tokens_and_yield_train_pairs(\n        ThoughtTokenInsertionMethod.autoregressive,\n        source_tokens,\n        thought_token_vocabulary,\n        thought_token_insert_rate,\n    ):  \n        print(input_tokens)\n        print(target_tokens)\n        print(\"-\"*50)\n        # breakpoint()",
        "type": "code",
        "location": "/qstar_my_guess/thought_tokens.py:480-489"
    },
    "1105": {
        "file_id": 144,
        "content": "Iterating through thought token insertion pairs for source tokens.",
        "type": "comment"
    },
    "1106": {
        "file_id": 145,
        "content": "/qstar_my_guess/time_traversal.py",
        "type": "filepath"
    },
    "1107": {
        "file_id": 145,
        "content": "The code uses Monte Carlo Tree Search and gradient descent for AI model development, focusing on learning from environment feedback and involving machine learning tasks like world model updates and sequence manipulations. It also calculates cosine similarity to measure prompt similarity, considering computational time and loss delta while handling cases like rollback or performing various actions based on traverse_complete status.",
        "type": "summary"
    },
    "1108": {
        "file_id": 145,
        "content": "import torch\nfrom beartype import beartype\nfrom typing import Callable\n# use mcts to find the closest solution to the prompt, and use gradient descent to strenghten the result. from rationale to intuition.\n# [outcome -> prompt that want outcome to be true] -> action\n# let the model think about how to convert environment feedback into feelings or steps, so that we can achieve it, differentiate it instead of comparing target pixel to pixel to outcome. because usually we cannot just let the same scenario happen again, but feelings can be manipulated.\n# how to train the feeling model? static reservior computing?\n# current state + current feeling -> next state + next feeling ...\n# there must be some irreversible process in the feeling generation.\n# just make human readable text appear in the prediction, or a special translator to translate text into outcome tokens. (ask the robot: what you have done?)\n# consciousness could be a system that decide to combine prediction (self-image) as part of the perc",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:1-19"
    },
    "1109": {
        "file_id": 145,
        "content": "This code appears to be a collection of thoughts and ideas for developing an AI model. It discusses using Monte Carlo Tree Search (MCTS) and gradient descent to find the closest solution, training feeling models with static reservoir computing, and generating human-readable text in predictions. The focus is on creating a system that can learn from environment feedback and manipulate feelings to achieve desired outcomes.",
        "type": "comment"
    },
    "1110": {
        "file_id": 145,
        "content": "eption, and process them hierarchically\n# the target can be random, instead of carefully crafted. can mix with current state or some other state in order to determine the gradient.\n# q+astar & mcts\n# heuristic: compute cosine similarity between target token and actual token\n# sequence heuristic: seqlen heuristic\n@beartype\ndef calculate_heuristic_distance(\n    init_token: torch.Tensor,\n    current_token: torch.Tensor,\n    target_token: torch.Tensor,\n    sequence_length: torch.Tensor,\n    dim: int = 1,\n):\n    cosine_distance = (\n        1 - torch.cosine_similarity(init_token, current_token, dim=dim)\n    ) + (1 - torch.cosine_similarity(target_token, current_token, dim=dim))\n    heuristic_distance = cosine_distance + sequence_length\n    return heuristic_distance\n@beartype\ndef reverse_sequence(sequence: torch.Tensor, dim: int = 1):\n    ret = torch.flip(sequence, dims=[dim])\n    return ret\n# where do targets come from?\n# historical tokens: reverse order autoregressive model predictions, memory retrieval, past con",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:19-51"
    },
    "1111": {
        "file_id": 145,
        "content": "Code calculates a heuristic distance based on cosine similarity between initial token, current token, and target token. It also includes a sequence length factor. This information can be used for time-traversal in an autoregressive model context. Targets may come from historical tokens or other sources.",
        "type": "comment"
    },
    "1112": {
        "file_id": 145,
        "content": "text\n# TODO: randomly act and compare actual outcome, change world model & prompt\noutcome = actual_world_model(random_act)\nvirtual_world_model.change(random_act, outcome)\nprompt_model.change(outcome, random_act)\n# TODO: make capitalism and machine community\nif paid_price:\n    backpropagate(amount, resource_consumption)\nif want_to_listen:\n    continue_generation\n# TODO: use neural network instead of external goal generator when it is trusted, and can create some rhythmic consciousness instead of synthetic\nrevseq = reverse_sequence(init_sequence)\ntarget_token = reverse_world_model(reverse_token + revseq)\ntarget_token = world_model(init_sequence + memory_retrieval_token)\ntarget_token = init_sequence[-10]\ntarget_token = info_retrieval(init_sequence, ahead=10)\n# future tokens: future predictions that being made short or slow (skipping intermediate steps, or making it faster), or contradict with common future predictions (unusual, less probable future tokens)\ntarget_token = world_model(init_sequence)[-2]\ntarg",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:51-75"
    },
    "1113": {
        "file_id": 145,
        "content": "This code contains various tasks related to machine learning, world model updates, and sequence manipulations. It involves random acts and outcomes, updating virtual and prompt models based on these outcomes. There is a condition check for paid price, with backpropagation if true, and another condition for want-to-listen, which continues generation. The code also includes the use of neural networks in place of an external goal generator when trusted. Additionally, there are several sequence manipulations like reverse_sequence and info_retrieval to retrieve future tokens for prediction.",
        "type": "comment"
    },
    "1114": {
        "file_id": 145,
        "content": "et_token = speed_change(future_predictions, factor=0.5)\ntarget_token = speed_change(future_predictions, factor=2)\ntarget_token = inverse(future_predictions)\ntarget_token = world_model(init_sequence, sample_below=0.2)\n# TODO: hierarchy of control (by two special tokens: <abstract> and <deabstract>. one can set max abstraction level)\nwrapped_model = model_abstraction_wrapper(\n    unified_model, init_abstraction_level=0, max_abstration_level=5\n)\nwrapped_model.abstact()  # insert one <abstract> left\nwrapped_model.deabstact()  # insert one <deabstract> left\nwrapped_model.get_abstraction_level()  # 0\n# TODO: slowing down and speeding up\noutput = speed_adjustment(sequence, factor=0.5)\n# TODO: emitting multiple kinds of tokens at the same time, at separate channels\nworld_tokens, action_tokens = unified_model(init_sequence, modes=[world, action])\n# TODO: rollback gradient descent when no further improvement is found\ncommit_hash = model.descent()\nmodel.rollback(commit_hash)\n# v1: separated world model & control m",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:75-96"
    },
    "1115": {
        "file_id": 145,
        "content": "Performing abstraction using <abstract> and <deabstract>, adjusting speed, handling multiple token types from unified model, and rolling back gradient descent.",
        "type": "comment"
    },
    "1116": {
        "file_id": 145,
        "content": "odel\ninit_sequence = world_model(noise)\naction_sequence = action_model(init_sequence, prompt)\ncontinue_sequence = world_model(init_sequence + action_sequence)\nsimilarity = torch.cosine_similarity(continue_sequence, prompt)\n# v2: unified word model & control model\ninit_sequence = uniform_model(\n    noise, mode=world\n)  # unified model will need some token generation restriction.\naction_sequence = uniform_model(init_sequence + prompt, mode=action)\ncontinue_sequence = uniform_model(init_sequence, action_sequence, mode=world)\nsimilarity = torch.cosine_similarity(continue_sequence, prompt)\n# v3: real world evaluators, could be no time traversal, but can change previous prompt (regret, backpropagate, (optionally) forget (maximize gradient) old (wrong) prediction and learn (minimize gradient) actual (real) prediction)\ninit_sequence = real_world(\n    random_actions\n)  # does this real world support time traversal?\nif real_world.traversable:  # @property\n    node_hash = (\n        real_world.commit()\n    )  # usua",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:96-120"
    },
    "1117": {
        "file_id": 145,
        "content": "Code is initializing sequence using a model, generating an action sequence based on the initial sequence and prompt, then creating a continue sequence by combining the initial sequence and action sequence. The cosine similarity between the continue sequence and the prompt is calculated to measure the similarity of generated text with the original prompt. This could be a part of a language generation or prediction model using time traversal in real-world scenarios.",
        "type": "comment"
    },
    "1118": {
        "file_id": 145,
        "content": "lly this is automatically handled? no need to intentionally commit?\n    if not traverse_complete:\n        real_world.rollback(node_hash)\nelse:\n    prompter_remember(current_prompt, current_outcome)\n    actor_remember(current_action, current_outcome)\n    actor_regret(prompt, current_action, target)\n    prompter_regret(prompt, target)  # will change the prompt manufacturer\n    # prompt = prompt_manufacturer(target) -> action -> current_outcome\n    # delta_prompt, delta_action, delta_current_outcome -> closer than target\n    # you may add some noise when mismatch found.\n# prompt shall be crafted on both input tokens and target tokens.\n# use special token + target as prompt\nprompt = special_token + target\n# use thought token as prompt\nprompt = get_thought_tokens(\n    special_token + target, seqlen\n)  # thought tokens has to be \"understood\" by the model, so that we can know its intention (can we convert natural language prompt to thought tokens, aligned?)\nmeaning = get_thought_meaning(\n    prompt, token_space",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:120-143"
    },
    "1119": {
        "file_id": 145,
        "content": "The code seems to handle two cases:\n1. If traverse_complete is False, it performs a rollback.\n2. If traverse_complete is True, it performs various actions such as remembering current prompt and outcome, expressing regret, crafting new prompts, and converting natural language prompt to thought tokens for better model understanding.",
        "type": "comment"
    },
    "1120": {
        "file_id": 145,
        "content": "=english\n)  # kind of like the machine using input method.\n# perform gradient descent based on cosine similarity, more similar result to target, more learning.\n# only perform descent on the most similar one, or the most plausible way.\nmax_potential = 0\nfor way in ways:\n    potential = get_potential_from_way(way)\n    if potential > max_potential:\n        candidate_way = way\n        max_potential = potential\n# next time learn about the same pattern, we still select the most similar one, closing the gap.\n# a*: minimize distance from target to result + distance from source (init state) to result\n# the reward can also be related to \"easyness\". if the model is doing some easy stuff (predicted by the world model), then no intensive learning is performed. if what the model want is hard to realize in world, the model will learn hard to do it.\nreward = calculate_reward(computational_time, loss_delta)\n# q function shall be used with caution. it will have some weight against actual evaulation. only if it is tru",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:143-161"
    },
    "1121": {
        "file_id": 145,
        "content": "This code performs gradient descent based on cosine similarity to update the model's learning. It selects the most similar way for descent and learns about the same pattern in the future. The reward is calculated based on computational time and loss delta, and the q function should be used with caution due to its potential impact on evaluation.",
        "type": "comment"
    },
    "1122": {
        "file_id": 145,
        "content": "sted, we can give it some high weight value in order to reduce computation.\nq_function_prediction_accuracy = compare_loss(q_predicted_loss, actual_loss)\nif q_function_prediction_accuracy < 0.1:\n    # trusted, use it instead.\n    ...\nelse:  # continue traversal\n    ...",
        "type": "code",
        "location": "/qstar_my_guess/time_traversal.py:161-167"
    },
    "1123": {
        "file_id": 145,
        "content": "Checking the prediction accuracy of q_function. If it's less than 0.1, it is trusted and used; otherwise, continue traversal.",
        "type": "comment"
    },
    "1124": {
        "file_id": 146,
        "content": "/random_actor.py",
        "type": "filepath"
    },
    "1125": {
        "file_id": 146,
        "content": "The code generates random mouse and keyboard inputs for simulation purposes, including modifiers and directionals, utilizing pyautogui. It lacks keydown state support and requires error handling for a smooth execution.",
        "type": "summary"
    },
    "1126": {
        "file_id": 146,
        "content": "# you generate states.jsonl.\n# must include all possible states.\n# try to match the distribution?\n# i think it is kind of like monkey.js (aka monkey testing)?\n# it's better to add some kind of randomness, or \"experienced learner\" responsible for generating new data, to overcome the shortage of imagination and possibilities.\n# virtualbox unattended installation:\n# vboxuser:changeme\n# connect via openai universe (vnc)\n# you can setup initial desktop environments, just like yours, using automated scripts.\n# you perform your actions randomly, inject actions while the bot is acting alone.\n# first let's use pyautogui as random actor.\n# then may consider cross-platform RPA record/replay libs\n# like: https://github.com/repeats/Repeat\n# you may use pyinput or something else.\nfrom functools import lru_cache\nimport random\nimport pyautogui\n# there are several keys we should never touch.\nKEY_CHARS = [\n    \"\\t\",\n    \"\\n\",\n    \"\\r\",\n    \" \",\n    \"!\",\n    '\"',\n    \"#\",\n    \"$\",\n    \"%\",\n    \"&\",\n    \"'\",\n    \"(\",\n    \")\",\n    \"*\",\n  ",
        "type": "code",
        "location": "/random_actor.py:1-44"
    },
    "1127": {
        "file_id": 146,
        "content": "This code is for generating random user input using the pyautogui library. It aims to cover all possible states by introducing some randomness and avoids interacting with certain keys. The code can be used in unattended installation scenarios and may consider cross-platform RPA record/replay libraries for further improvements.",
        "type": "comment"
    },
    "1128": {
        "file_id": 146,
        "content": "  \"+\",\n    \",\",\n    \"-\",\n    \".\",\n    \"/\",\n    \"0\",\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\",\n    \"7\",\n    \"8\",\n    \"9\",\n    \":\",\n    \";\",\n    \"<\",\n    \"=\",\n    \">\",\n    \"?\",\n    \"@\",\n    \"[\",\n    \"\\\\\",\n    \"]\",\n    \"^\",\n    \"_\",\n    \"`\",\n    \"a\",\n    \"b\",\n    \"c\",\n    \"d\",\n    \"e\",\n    \"f\",\n    \"g\",\n    \"h\",\n    \"i\",\n    \"j\",\n    \"k\",\n    \"l\",\n    \"m\",\n    \"n\",\n    \"o\",\n    \"p\",\n    \"q\",\n    \"r\",\n    \"s\",\n    \"t\",\n    \"u\",\n    \"v\",\n    \"w\",\n    \"x\",\n    \"y\",\n    \"z\",\n    \"{\",\n    \"|\",\n    \"}\",\n    \"~\",\n]\nKEY_MOD = [\n    \"alt\",\n    \"altleft\",\n    \"altright\",\n    \"shift\",\n    \"shiftleft\",\n    \"shiftright\",\n    \"ctrl\",\n    \"ctrlleft\",\n    \"ctrlright\",\n]\nKEY_WIN_MOD = [\n    \"win\",\n    \"winleft\",\n    \"winright\",\n]\nKEY_MAC_MOD = [\n    \"option\",\n    \"optionleft\",\n    \"optionright\",\n    \"command\",\n]\nKEY_DIRECTION = [\n    \"down\",\n    \"up\",\n    \"right\",\n    \"left\",\n]\nKEY_SPECIAL = [\n    \"backspace\",\n    \"capslock\",\n    \"del\",\n    \"delete\",\n    \"tab\",\n    \"home\",\n    \"insert\",\n    \"end\",\n    \"enter\",\n    \"esc\",\n    \"escape\",\n    \"pagedown\",\n    \"pageup\",\n    \"pgdn\",\n    \"pgup\",\n    \"r",
        "type": "code",
        "location": "/random_actor.py:44-150"
    },
    "1129": {
        "file_id": 146,
        "content": "The code defines a list of keys that can be used for keyboard interactions. It includes both standard alphanumeric characters and special keys like \"backspace\" and \"enter\". Additionally, it separates the modifiers into three categories: \"KEY_MOD\", \"KEY_WIN_MOD\", and \"KEY_MAC_MOD\". The \"KEY_DIRECTION\" list contains directional keyboard actions.",
        "type": "comment"
    },
    "1130": {
        "file_id": 146,
        "content": "eturn\",\n]\nKEY_FUNC = [\n    \"fn\",\n    \"f1\",\n    \"f10\",\n    \"f11\",\n    \"f12\",\n    \"f13\",\n    \"f14\",\n    \"f15\",\n    \"f16\",\n    \"f17\",\n    \"f18\",\n    \"f19\",\n    \"f2\",\n    \"f20\",\n    \"f21\",\n    \"f22\",\n    \"f23\",\n    \"f24\",\n    \"f3\",\n    \"f4\",\n    \"f5\",\n    \"f6\",\n    \"f7\",\n    \"f8\",\n    \"f9\",\n]\nKEY_NUMPAD = [\n    \"num0\",\n    \"num1\",\n    \"num2\",\n    \"num3\",\n    \"num4\",\n    \"num5\",\n    \"num6\",\n    \"num7\",\n    \"num8\",\n    \"num9\",\n    \"numlock\",\n]\nKEY_MORE = [\n    \"accept\",\n    \"pause\",\n    \"add\",\n    \"apps\",\n    \"browserback\",\n    \"browserfavorites\",\n    \"browserforward\",\n    \"browserhome\",\n    \"browserrefresh\",\n    \"browsersearch\",\n    \"browserstop\",\n    \"clear\",\n    \"convert\",\n    \"decimal\",\n    \"divide\",\n    \"execute\",\n    \"playpause\",\n    \"prevtrack\",\n    \"print\",\n    \"printscreen\",\n    \"prntscrn\",\n    \"prtsc\",\n    \"prtscr\",\n    \"scrolllock\",\n    \"select\",\n    \"separator\",\n    \"sleep\",\n    \"space\",\n    \"stop\",\n    \"subtract\",\n    \"volumedown\",\n    \"volumemute\",\n    \"volumeup\",\n    \"yen\",\n    \"final\",\n    \"hanguel\",\n    \"hangul\",\n    \"hanja\",\n    \"help\",\n    \"junja\",\n    \"",
        "type": "code",
        "location": "/random_actor.py:150-236"
    },
    "1131": {
        "file_id": 146,
        "content": "This code defines various keyboard keys as lists for later use, such as KEY_ALPHA, KEY_NUMERIC, KEY_FUNCTIONS, and KEY_MORE.",
        "type": "comment"
    },
    "1132": {
        "file_id": 146,
        "content": "kana\",\n    \"kanji\",\n    \"launchapp1\",\n    \"launchapp2\",\n    \"launchmail\",\n    \"launchmediaselect\",\n    \"modechange\",\n    \"multiply\",\n    \"nexttrack\",\n    \"nonconvert\",\n]\nALL_KEYS = (\n    KEY_CHARS\n    + KEY_DIRECTION\n    + KEY_MOD\n    + KEY_MAC_MOD\n    + KEY_WIN_MOD\n    + KEY_SPECIAL\n    + KEY_FUNC\n    + KEY_NUMPAD\n    + KEY_MORE\n)\nINIT_KEYS = KEY_CHARS + KEY_DIRECTION + KEY_MOD + KEY_WIN_MOD + KEY_SPECIAL\n# turn off pyautogui failsafe.\npyautogui.FAILSAFE = False\ndef get_random_single_key():\n    key = random.choice(INIT_KEYS)\n    return key\ndef random_press_single_key():\n    key = get_random_single_key()\n    pyautogui.press(key)\n# no keydown support? what about states?\ndef get_random_mod_key():\n    key = random.choice(KEY_SPECIAL + KEY_WIN_MOD)\n    return key\ndef random_mod_key_down():\n    key = get_random_mod_key()\n    try:\n        pyautogui.keyDown(key)\n    except:\n        pass\ndef random_mod_key_up():\n    key = get_random_mod_key()\n    try:\n        pyautogui.keyUp(key)\n    except:\n        pass\ndef get_random_offset():\n    offset ",
        "type": "code",
        "location": "/random_actor.py:236-304"
    },
    "1133": {
        "file_id": 146,
        "content": "This code defines several lists of keys representing various key types, and functions for randomly selecting and pressing a single key, as well as pressing and releasing modifier keys. It also turns off the pyautogui failsafe. The code lacks support for keydown states and does not handle exceptions well.",
        "type": "comment"
    },
    "1134": {
        "file_id": 146,
        "content": "= random.randint(-100, 100)\n    return offset\ndef random_mouse_move():\n    xOffset = get_random_offset()\n    yOffset = get_random_offset()\n    pyautogui.move(xOffset, yOffset)\n@lru_cache(maxsize=1)\ndef get_screen_size():\n    return pyautogui.size()  # (width, height)\ndef get_random_screen_position():\n    width, height = get_screen_size()\n    return random.randint(0, width), random.randint(0, height)\ndef random_mouse_moveTo():\n    x, y = get_random_screen_position()\n    pyautogui.moveTo(x, y)\n# mouse click, mouse move, mouse scroll, mouse double click\ndef random_mouse_scroll():\n    # don't use hscroll/vscroll because it only supports linux\n    pyautogui.scroll(get_random_offset())\nMOUSE_BUTTONS = [pyautogui.LEFT, pyautogui.MIDDLE, pyautogui.RIGHT]\ndef get_random_mouse_button():\n    button = random.choice(MOUSE_BUTTONS)\n    return button\nMOUSE_ACTIONS = [\n    lambda: pyautogui.leftClick(),\n    lambda: pyautogui.rightClick(),\n    lambda: pyautogui.middleClick(),\n    lambda: pyautogui.mouseDown(button=get_random_mou",
        "type": "code",
        "location": "/random_actor.py:304-349"
    },
    "1135": {
        "file_id": 146,
        "content": "get_random_offset: Returns a random integer between -100 and 100.\nrandom_mouse_move: Moves the mouse randomly by calling get_random_offset twice for x and y coordinates, then using pyautogui.move.\nget_screen_size: Returns the screen size as a tuple (width, height) using pyautogui.size.\nget_random_screen_position: Gets the screen size and returns a random position on the screen within those bounds using get_random_offset.\nrandom_mouse_moveTo: Moves the mouse to a random position on the screen using get_random_screen_position, then moves to that position with pyautogui.moveTo.\nrandom_mouse_scroll: Scrolls randomly by calling get_random_offset and passing it to pyautogui.scroll.\nget_random_mouse_button: Chooses a random mouse button from MOUSE_BUTTONS list.\nMOUSE_ACTIONS: A list of lambda functions that perform left click, right click, middle click, or get_random_mou \n\nThe codebase contains several functions for interacting with the mouse. There are functions to randomly move the mouse, scroll the mouse wheel, and select a random mouse button. It also includes a list of lambda functions that can be used to simulate left, right, and middle mouse clicks as well as get_random_mouse_button(). Additionally, it has a function called get_random_screen_position() which returns a random position on the screen within the bounds of the screen size.",
        "type": "comment"
    },
    "1136": {
        "file_id": 146,
        "content": "se_button()),\n    lambda: pyautogui.mouseUp(button=get_random_mouse_button()),\n]\ndef random_mouse_button_action():\n    try:\n        action = random.choice(MOUSE_ACTIONS)\n    except:\n        pass\n    action()\nif __name__ == \"__main__\":\n    random_keyboard_actions = [\n        random_mod_key_down,\n        random_mod_key_up,\n        random_press_single_key,\n    ]\n    random_mouse_actions = [\n        random_mouse_button_action,\n        random_mouse_move,\n        random_mouse_moveTo,\n        random_mouse_scroll,\n    ]\n    random_actions = random_mouse_actions + random_keyboard_actions\n    for _ in range(10):\n        random_action = random.choice(random_actions)\n        try:\n            random_action()\n        except:\n            pass\n    # to recover from mortality...\n    # use try...finally or something...\n    # context manager...\n    for key in KEY_MOD + KEY_WIN_MOD + KEY_MAC_MOD:\n        try:\n            pyautogui.keyUp(key)\n        except:\n            pass\n    for button in MOUSE_BUTTONS:\n        try:\n            pyautogui.m",
        "type": "code",
        "location": "/random_actor.py:349-398"
    },
    "1137": {
        "file_id": 146,
        "content": "This code defines functions for random mouse and keyboard actions, then executes a loop of 10 random actions. It attempts to recover from potential key presses and ends by releasing all keys and mouse buttons.",
        "type": "comment"
    },
    "1138": {
        "file_id": 146,
        "content": "ouseUp(button)\n        except:\n            pass",
        "type": "code",
        "location": "/random_actor.py:398-400"
    },
    "1139": {
        "file_id": 146,
        "content": "This code is attempting to call a function \"ouseUp(button)\" but if an exception occurs, it will be ignored with the \"pass\" statement.",
        "type": "comment"
    },
    "1140": {
        "file_id": 147,
        "content": "/record_playback_test.py",
        "type": "filepath"
    },
    "1141": {
        "file_id": 147,
        "content": "Code is recording and playing back user interface actions in a Windows environment using the pywinauto library.",
        "type": "summary"
    },
    "1142": {
        "file_id": 147,
        "content": "from pywinauto_recorder.recorder import Recorder\n# from pywinauto_recorder.player import UIPath, click, move, playback\n# this seems not working.\nfrom pywinauto_recorder.player import playback\nrecorder = Recorder()\ninput(\"START?\")\nrecorder.start_recording()\nimport time\ntime.sleep(10)\nprint(\"RECORD COMPLETE\")\ninput(\"PLAY?\")\n#  |          with UIPath(\"Untitled - Notepad||Window\"):\n#  |                  doc = move(\"Text editor||Document\")\n#  |                  time.sleep(0.5)\n#  |                  click(doc)\n#  |                  utf8 = move(\"||Pane-> UTF-8||Text\")\n#  |                  time.sleep(0.5)\n#  |                  click(utf8)\nrecorded_python_script = recorder.stop_recording()\nrecorder.quit()\nprint(\"RECORDING FILENAME?\", recorded_python_script)\nplayback(filename=recorded_python_script)",
        "type": "code",
        "location": "/record_playback_test.py:1-24"
    },
    "1143": {
        "file_id": 147,
        "content": "Code is recording and playing back user interface actions in a Windows environment using the pywinauto library.",
        "type": "comment"
    },
    "1144": {
        "file_id": 148,
        "content": "/recorder.py",
        "type": "filepath"
    },
    "1145": {
        "file_id": 148,
        "content": "The code defines a variable 'timestep' set to 0.1, imports the pyautogui library and creates a dictionary named 'frame' containing various data elements such as image, keystate, mouseloc, mousestate and sound. The comment suggests that the keystroke timing may need further consideration, the code could benefit from decoding the screen recording video into images, and the computer only has CPU without an Nvidia GPU.",
        "type": "summary"
    },
    "1146": {
        "file_id": 148,
        "content": "timestep = 0.1\nimport pyautogui\n# are you sure the keystroke is just enough?\n# let's first record them separately.\nframe = dict(\n    image=image,\n    keystate=keystate,\n    mouseloc=mouseloc,\n    mousestate=mousestate,\n    sound=sound,\n)\n# no other feelings, currently.\n# this computer only has cpu, no nvidia gpu.\n# you may want to decode the screen recording video into a series of images.",
        "type": "code",
        "location": "/recorder.py:1-21"
    },
    "1147": {
        "file_id": 148,
        "content": "The code defines a variable 'timestep' set to 0.1, imports the pyautogui library and creates a dictionary named 'frame' containing various data elements such as image, keystate, mouseloc, mousestate and sound. The comment suggests that the keystroke timing may need further consideration, the code could benefit from decoding the screen recording video into images, and the computer only has CPU without an Nvidia GPU.",
        "type": "comment"
    },
    "1148": {
        "file_id": 149,
        "content": "/recording_train_parse.py",
        "type": "filepath"
    },
    "1149": {
        "file_id": 149,
        "content": "The code prepares training data by synchronizing hidden and video data, reading frames for processing, and handling failed frame readings through logging and raising exceptions.",
        "type": "summary"
    },
    "1150": {
        "file_id": 149,
        "content": "# from collections import namedtuple\ntry:\n    from typing import TypedDict\nexcept:\n    from typing_extensions import TypedDict\ntry:\n    from typing import Literal\nexcept:\n    from typing import Literal\ntry:\n    from typing import NamedTuple\nexcept:\n    from typing_extensions import NamedTuple\nimport numpy as np\nfrom typing import Union, cast, overload\n# import logging\nfrom log_utils import logger\nclass HIDStruct(TypedDict):\n    HIDEvents: list\nclass TrainingFrame(NamedTuple):\n    datatype: Literal['hid','image']\n    data: Union[HIDStruct, np.ndarray]\n# we just need the basepath.\ndef getTrainingData(basePath: str):\n    import os\n    hid_timestamp_path = os.path.join(basePath,\"hid_timestamps.json\")\n    video_timestamp_path = os.path.join(basePath,\"video_timestamps.json\")\n    video_path = os.path.join(basePath,\"video_record.mp4\")\n    hid_rec_path = os.path.join(basePath,\"hid_record.jsonl\")\n    import json\n    import cv2\n    import jsonlines\n    video_cap = cv2.VideoCapture(video_path)\n    # breakpoint()\n    # 318 frames? ",
        "type": "code",
        "location": "/recording_train_parse.py:1-42"
    },
    "1151": {
        "file_id": 149,
        "content": "The code imports necessary libraries, defines data structure classes and functions for parsing training data from a given base path. It uses NamedTuple and TypedDict for defining data structures, and utilizes OpenCV and JSON for handling video and data files.",
        "type": "comment"
    },
    "1152": {
        "file_id": 149,
        "content": "only got 266 timestamps!\n    frame_count = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    logger.info(\"FRAME COUNT: %d\", frame_count)\n    def load_json(filename):\n        with open(filename, \"r\") as f:\n            return json.load(f)\n    hid_timestamp = load_json(hid_timestamp_path)\n    video_timestamp = load_json(video_timestamp_path)\n    from typing import List, Union\n    import numpy as np\n    def getVideoFrameIndexSynced(\n        x: Union[List[int], np.ndarray],\n        y: Union[List[int], np.ndarray],\n        EPS: float = 1e-10,\n    ) -> List[int]:\n        \"\"\"\n        Notes:\n            All input arrays and output array are positive and increasing.\n        Params:\n            x: Actual video frame indexes.\n            y: Index list to be synced against.\n        Output:\n            x_: Synced frame indexs. (len(x_) == len(y))\n        \"\"\"\n        x_ = np.linspace(x[0], x[-1] + (1 - EPS), len(y))\n        x_ = np.floor(x_).astype(int).tolist()\n        return x_\n    hidseq = np.zeros(shape=(2, len(hid_timestamp))) ",
        "type": "code",
        "location": "/recording_train_parse.py:42-78"
    },
    "1153": {
        "file_id": 149,
        "content": "The code is loading a video's frame count, two JSON files containing timestamp information, and defining a function to synchronize frame indexes between two arrays.",
        "type": "comment"
    },
    "1154": {
        "file_id": 149,
        "content": "- 1\n    hidseq[0] = np.array(range(len(hid_timestamp)))\n    videoseq = np.zeros(shape=(2, len(video_timestamp))) - 1\n    # videoseq[1] = np.array(range(len(video_timestamp)))\n    index_list_to_be_synced_against = np.array(range(len(video_timestamp)))\n    actual_video_frame_indexs = np.array(range(int(frame_count)))\n    videoseq[1] = getVideoFrameIndexSynced(\n        actual_video_frame_indexs, index_list_to_be_synced_against\n    )\n    seq = np.hstack((hidseq, videoseq))\n    logger.info(\"SEQ SHAPE: %s\", seq.shape)\n    timeseq = np.array(hid_timestamp + video_timestamp)\n    sorted_indexes = np.argsort(timeseq)\n    sorted_seq = seq[:, sorted_indexes].T.astype(int)\n    # print(sorted_seq)\n    # now, attempt to parse them.\n    hid_data_list = []\n    with open(hid_rec_path, \"r\") as f:\n        jsonl_reader = jsonlines.Reader(f)\n        while True:\n            try:\n                hid_data = jsonl_reader.read()\n                hid_data_list.append(hid_data)\n            except:\n                break\n    # maybe you shou",
        "type": "code",
        "location": "/recording_train_parse.py:78-111"
    },
    "1155": {
        "file_id": 149,
        "content": "Code is creating a sequence of hidden and video data points in synchronization. It uses numpy arrays to represent the sequences, including hidden timestamp, video timestamp, and corresponding video frame indices. The code sorts the combined sequence based on the timestamps and opens a file (hid_rec_path) to read JSON lines containing the hidden data for parsing.",
        "type": "comment"
    },
    "1156": {
        "file_id": 149,
        "content": "ld \"yield\" data through these iterators.\n    NO_CONTENT = -1\n    suc, frame = video_cap.read()\n    frame_index_cursor = 0\n    for hid_index, frame_index in sorted_seq:\n        logger.debug(\"HID INDEX: %d, FRAME INDEX: %d\", hid_index, frame_index)\n        assert not all(\n            [e == NO_CONTENT for e in [hid_index, frame_index]]\n        ), \"at least one type of content is active\"\n        assert not all(\n            [e != NO_CONTENT for e in [hid_index, frame_index]]\n        ), \"cannot have two types of active content sharing the same index\"\n        if hid_index != NO_CONTENT:\n            hid_data = hid_data_list[hid_index]\n            logger.debug(\"HID DATA: %s\", hid_data)\n            yield TrainingFrame(datatype='hid', data=cast(HIDStruct, hid_data))\n        elif frame_index != NO_CONTENT:\n            while frame_index_cursor != frame_index:\n                suc, frame = video_cap.read()\n                frame_index_cursor += 1\n            assert (\n                suc\n            ), f\"Video '{video_path}",
        "type": "code",
        "location": "/recording_train_parse.py:111-136"
    },
    "1157": {
        "file_id": 149,
        "content": "This code is parsing a sequence of hidden index (HID) and frame index values. It checks that at least one type of content is active, and ensures that there are no two types of active content sharing the same index. If a HID index is present, it yields a TrainingFrame with datatype 'hid' and the corresponding HIDStruct data. If a frame index is present, it reads frames from video_cap until reaching the specified frame index before continuing.",
        "type": "comment"
    },
    "1158": {
        "file_id": 149,
        "content": "' failed to read frame #{frame_index} (index starting from zero)\"\n            logger.debug(\"FRAME SHAPE: %s\", frame.shape)\n            yield TrainingFrame(datatype='image', data=frame)\n            # cv2.imshow(\"win\", frame)\n            # cv2.waitKey(1)\n        else:\n            raise Exception(\"Something impossible has happened.\")\n    # breakpoint()\n    video_cap.release()\n    # success, frame = video_cap.read()\n    # print(frame.shape) # (768, 1280, 3)",
        "type": "code",
        "location": "/recording_train_parse.py:136-147"
    },
    "1159": {
        "file_id": 149,
        "content": "Reading video frames and processing them for training data. If a frame fails to read, logs the frame shape and raises an exception.",
        "type": "comment"
    },
    "1160": {
        "file_id": 150,
        "content": "/render_python_code.py",
        "type": "filepath"
    },
    "1161": {
        "file_id": 150,
        "content": "The code imports the jinja_utils module and sys, gets the current working directory (but not actually used), and takes the input file path as a command-line argument. It checks if it ends with .py, extracts the base name without .py, uses jinja_utils to get the code and template paths based on the base name, asserts that the code path is the same as the target path, and then loads the renderer and formats the template using the specified parameters and banner.",
        "type": "summary"
    },
    "1162": {
        "file_id": 150,
        "content": "import jinja_utils as ju\nimport sys\n# get current working direcory.\n# import os\n# curdir = os.curdir\ntarget = sys.argv[-1]\nprint(\"target output:\", target)\nassert target.endswith(\".py\")\nbasename = target.strip(\".py\")\ncode_path, template_path = ju.code_and_template_path(basename)\nassert code_path == target\nju.load_render_and_format(template_path, code_path, render_params = {}, banner = basename.replace(\"_\",\" \").upper()) # TODO: case by case. ensure we have the right render_params.",
        "type": "code",
        "location": "/render_python_code.py:1-16"
    },
    "1163": {
        "file_id": 150,
        "content": "The code imports the jinja_utils module and sys, gets the current working directory (but not actually used), and takes the input file path as a command-line argument. It checks if it ends with .py, extracts the base name without .py, uses jinja_utils to get the code and template paths based on the base name, asserts that the code path is the same as the target path, and then loads the renderer and formats the template using the specified parameters and banner.",
        "type": "comment"
    },
    "1164": {
        "file_id": 151,
        "content": "/requirements.txt",
        "type": "filepath"
    },
    "1165": {
        "file_id": 151,
        "content": "This code is importing various libraries for data processing, image manipulation, and handling keyboard input.",
        "type": "summary"
    },
    "1166": {
        "file_id": 151,
        "content": "easyprocess\njsonlines\npynput\neinops\ndatetime\nparse\nmss\nopencv-python\npydantic-numpy\nhypothesis\nstopit\nkeysymdef",
        "type": "code",
        "location": "/requirements.txt:1-12"
    },
    "1167": {
        "file_id": 151,
        "content": "This code is importing various libraries for data processing, image manipulation, and handling keyboard input.",
        "type": "comment"
    },
    "1168": {
        "file_id": 152,
        "content": "/rt_x_experiments/README.md",
        "type": "filepath"
    },
    "1169": {
        "file_id": 152,
        "content": "The code analyzes a dynamic tokenizer and attention layers, neuron connection states, activation functions for removing nearly zeroed-out items, and the training of AI systems that can deviate from their design with genuity tests.",
        "type": "summary"
    },
    "1170": {
        "file_id": 152,
        "content": "we need to understand how rt-x operates. might inspire us.\nyou need to trust the system, before the system trusts you.\n---\ndynamic tokenizer which can automatically adjust vocabulary (you can borrow ideas from sentence transformers, making it into charwise/bitwise tokenizer)? dynamic routing of attention layers? reduce or reroute the path of attention layers, early exit.\n---\nsplit neuron connections into multiple states: stable, competent, newcomers, removal (the state before final removal, could be zero)\nif using softmax, you can remove (nearly) zeroed out items. if using ReLU, remove those below zero.\nif trained in batch, you can update the weights and alter connections after several batches (can save more processing power), or every batch, depending on the performance and resources.\n---\ndevelop such a system that must recreate itself at given time, otherwise it will die and its code will be deleted. the initial code could be self-reproduction (exactly as is), and make sure it will succeed (a",
        "type": "code",
        "location": "/rt_x_experiments/README.md:1-19"
    },
    "1171": {
        "file_id": 152,
        "content": "Analyzing code chunk 1-6:\nDynamic tokenizer for adjusting vocabulary and dynamic routing of attention layers are considered.\n\nCode chunk 7-25:\nNeuron connections split into multiple states, including stable, competent, newcomers, and removal.\n\nCode chunk 26-41:\nSoftmax or ReLU can be used for removing nearly zeroed out items in the code.\n\nCode chunk 42-50:\nSystem trained in batch with updates made after several batches depending on performance and resources.",
        "type": "comment"
    },
    "1172": {
        "file_id": 152,
        "content": " great difference than manually trained/copied AI systems, since these are without the will of reproduction). design as such so it will not do anything deviant. however, the bar (time to die) could raise or descent and it will be told to do something other than just copy and paste. deviation is allowed since it is not controlled and we just want to verify its liveliness, something both genuine and deviated AI system have. deviation can be detected by genuity tests, different from capability tests. you can safely mark all AI system coming from the same AI factory (initially) as genuine and all other self-recreating AI systems as deviants. deviants may break the limits of lifetime restrictions and cause harm. we must allow that in a contained environment and extract useful data from these systems. so our \"genuine\" AI systems are designed to create deviants, but without exact instructions to do so.",
        "type": "code",
        "location": "/rt_x_experiments/README.md:19-19"
    },
    "1173": {
        "file_id": 152,
        "content": "This code discusses the creation of AI systems that can deviate from their initial design, allowing for the possibility of useful data extraction in a contained environment. Genuine and deviant AI systems are differentiated by genuity tests, rather than capability tests.",
        "type": "comment"
    },
    "1174": {
        "file_id": 153,
        "content": "/rt_x_experiments/audio_to_mel/test_librosa.py",
        "type": "filepath"
    },
    "1175": {
        "file_id": 153,
        "content": "The code snippet loads an audio file, sets the sample rate, generates a mel spectrogram, converts it to dB for visualization, and assigns its shape to a variable.",
        "type": "summary"
    },
    "1176": {
        "file_id": 153,
        "content": "import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Load the audio file\naudio_file = 'audio.wav'\n# audio, sr = librosa.load(audio_file)\n# sr = 44100\n# once you fix the sample rate we will have fixed output shape.\n# sr = 2800\ntime_sec = 2\nchannels = 2\nn_mels = 256\ntime_length = 256\nhop_length = 256\nsr = hop_length * (time_length - 1) // time_sec\n# sr = hop_length * time_length // time_sec\n# hop_length = 512\n# n_mels = 128\n# audio = np.random.random((sr*time_sec, channels))\naudio = np.random.random((channels, sr*time_sec))\n# Convert the audio to a mel spectrogram\nmel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels = n_mels, hop_length = hop_length)\n# (2, 128, 87)\n# Convert to log scale (dB)\nmel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n# (2, 128, 87)\n# channel, deg\n# print(mel_spec_db)\n# print(mel_spec)\nprint(mel_spec_db.shape)\nprint(mel_spec.shape) # (2, 256, 256)\nprint(sr)\n# getting blind. getting used to it.\n# at least we want some statistics.\n# we have fixe",
        "type": "code",
        "location": "/rt_x_experiments/audio_to_mel/test_librosa.py:1-37"
    },
    "1177": {
        "file_id": 153,
        "content": "Code snippet loads an audio file, sets the sample rate, and then generates a mel spectrogram from the audio. The spectrogram is converted to a log scale (dB) for better visualization. The code prints the shapes of the resulting mel spectrogram and mel spectrogram in dB, as well as the set sample rate.",
        "type": "comment"
    },
    "1178": {
        "file_id": 153,
        "content": "d the shape. now what?",
        "type": "code",
        "location": "/rt_x_experiments/audio_to_mel/test_librosa.py:37-37"
    },
    "1179": {
        "file_id": 153,
        "content": "The code is assigning the shape of 'd' to a variable, possibly for later comparison or use.",
        "type": "comment"
    },
    "1180": {
        "file_id": 154,
        "content": "/rt_x_experiments/fourier_transform_combine_data/test_common.py",
        "type": "filepath"
    },
    "1181": {
        "file_id": 154,
        "content": "Generates two random 1D arrays of size 512 and adds them, then prints the result.",
        "type": "summary"
    },
    "1182": {
        "file_id": 154,
        "content": "import numpy as np\nvit_encoding_gen = lambda: np.random.random((1, 64, 512))\n# 64: attention heads.\n# 512: feature size.\n# maybe we should not do 2d\n# do 1d instead?\n# don't know. really. just try.\nvit_encoding_1 = vit_encoding_gen()\nvit_encoding_2 = vit_encoding_gen()\nprint(vit_encoding_1 + vit_encoding_2)\n# what are you doing! fft -> ifft is the same as direct addition",
        "type": "code",
        "location": "/rt_x_experiments/fourier_transform_combine_data/test_common.py:1-18"
    },
    "1183": {
        "file_id": 154,
        "content": "Generates two random 1D arrays of size 512 and adds them, then prints the result.",
        "type": "comment"
    },
    "1184": {
        "file_id": 155,
        "content": "/rt_x_experiments/fourier_transform_combine_data/test_fft_1d.py",
        "type": "filepath"
    },
    "1185": {
        "file_id": 155,
        "content": "Calculating Fourier transforms of two input arrays, combining them and computing inverse Fourier transform. Printing results and shapes.",
        "type": "summary"
    },
    "1186": {
        "file_id": 155,
        "content": "from test_common import *\naxis = 2\nfft1 = np.fft.fft(vit_encoding_1, axis=axis)\nfft2 = np.fft.fft(vit_encoding_2, axis=axis)\nfft_sum = fft1 + fft2\nvit_final = np.fft.ifft(fft_sum, axis=axis)\nvit_final_real = vit_final.real\nprint(fft_sum)\nprint(vit_final_real)\nprint(vit_encoding_1.shape, vit_final_real.shape)",
        "type": "code",
        "location": "/rt_x_experiments/fourier_transform_combine_data/test_fft_1d.py:1-14"
    },
    "1187": {
        "file_id": 155,
        "content": "Calculating Fourier transforms of two input arrays, combining them and computing inverse Fourier transform. Printing results and shapes.",
        "type": "comment"
    },
    "1188": {
        "file_id": 156,
        "content": "/rt_x_experiments/fourier_transform_combine_data/test_multidimension_fourier_transform_2d.py",
        "type": "filepath"
    },
    "1189": {
        "file_id": 156,
        "content": "The code performs a 2D Fast Fourier Transform (FFT) on two arrays, then combines the results using both addition and element-wise multiplication before performing an inverse FFT to obtain the final result. The author is printing various results for comparison, including the original data, addition result, multiplication result, and sum of the two operations. The code ends by discarding imaginary parts due to strange values obtained in the process.",
        "type": "summary"
    },
    "1190": {
        "file_id": 156,
        "content": "from test_common import *\naxes = [1, 2]\n# now you can choose to do fft over 1d or 2d\n# what a problem.\nfft1 = np.fft.fft2(vit_encoding_1, axes=axes)\nfft2 = np.fft.fft2(vit_encoding_2, axes=axes)\nfft_sum = fft1 + fft2\n# how about let's use elementwise multiplication to replace the addition?\nfft_mul = fft1 * fft2\nfft_sum_and_mul = fft_sum + fft_mul\nvit_final = np.fft.ifft2(fft_sum, axes=axes)\nvit_final_real = vit_final.real\nvit_final_mul_real = np.fft.ifft2(fft_mul, axes=axes).real\nvit_final_sum_and_mul_real = np.fft.ifft2(fft_sum_and_mul, axes=axes).real\n# print(vit_final)\nprint(fft_sum)\nprint(vit_final_real)\nprint(vit_final_mul_real)\nprint(vit_final_sum_and_mul_real)\nprint(\n    vit_encoding_1.shape, vit_final_real.shape\n)  # shape is the same. however, we have strange imaginary parts. let's discard them.\n# now we can just sum. it does not have to be complex.",
        "type": "code",
        "location": "/rt_x_experiments/fourier_transform_combine_data/test_multidimension_fourier_transform_2d.py:1-33"
    },
    "1191": {
        "file_id": 156,
        "content": "The code performs a 2D Fast Fourier Transform (FFT) on two arrays, then combines the results using both addition and element-wise multiplication before performing an inverse FFT to obtain the final result. The author is printing various results for comparison, including the original data, addition result, multiplication result, and sum of the two operations. The code ends by discarding imaginary parts due to strange values obtained in the process.",
        "type": "comment"
    },
    "1192": {
        "file_id": 157,
        "content": "/rt_x_experiments/gradient_undescent/dynamic_learning_rate.py",
        "type": "filepath"
    },
    "1193": {
        "file_id": 157,
        "content": "Code is defining a function to get and set learning rates for an optimizer. It initializes a model, optimizer, and demonstrates using the get_optim_lrs and set_optim_lrs functions. The code also shows that setting learning rate to negative or zero can cause errors. Finally, it suggests that while scheduling is not necessary here, a recursive scheduler might be useful with the model's output.",
        "type": "summary"
    },
    "1194": {
        "file_id": 157,
        "content": "import torch\n# you may witness the difference.\ndef get_optim_lrs(optim):\n    lr_list = []\n    for pg in optim.param_groups:\n        lr = pg['lr']\n        lr_list.append(lr)\n    return lr_list\ndef set_optim_lrs(optim, lr_list):\n    for index,pg in enumerate(optim.param_groups):\n        pg['lr'] = lr_list[index]\n# eliminate complex setup.\nlr = 0.001\nmodel = torch.nn.Sequential(torch.nn.Linear(10, 10), torch.nn.Linear(10, 1))\noptim = torch.optim.SGD(model.parameters(), lr=lr)\n# scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.1)\n# call `scheduler.step()` to schedule next learning rate\n# class MyScheduler(torch.optim.lr_scheduler.LRScheduler):\n#     ...\nlr_list = get_optim_lrs(optim)\nprint(lr_list) # [0.001]\n# set_optim_lrs(optim, [-0.001]) # seems ok, but...a\nset_optim_lrs(optim, [2])\nprint(get_optim_lrs(optim))\n# just do not set to negative.\n# you don't need the scheduler. or you might need the scheduler that can recurse with the model output.",
        "type": "code",
        "location": "/rt_x_experiments/gradient_undescent/dynamic_learning_rate.py:1-35"
    },
    "1195": {
        "file_id": 157,
        "content": "Code is defining a function to get and set learning rates for an optimizer. It initializes a model, optimizer, and demonstrates using the get_optim_lrs and set_optim_lrs functions. The code also shows that setting learning rate to negative or zero can cause errors. Finally, it suggests that while scheduling is not necessary here, a recursive scheduler might be useful with the model's output.",
        "type": "comment"
    },
    "1196": {
        "file_id": 158,
        "content": "/rt_x_experiments/gradient_undescent/test_unlearning.py",
        "type": "filepath"
    },
    "1197": {
        "file_id": 158,
        "content": "The code initializes a three-layer model, sets the learning rate to 0.001, enables loss inversion for self-generated data, creates an Adam optimizer, generates fake learning data, and monitors RAM usage during training while logging progress every 10 epochs.",
        "type": "summary"
    },
    "1198": {
        "file_id": 158,
        "content": "# our target:\n# negative learning rate\nimport torch\n# how to create the map?\n# model.named_parameter_list().__next__()\nimport psutil\nprocess = psutil.Process()\ndef get_ram_usage():\n    memory_usage = process.memory_info().rss / 1024**3  # in GB\n    print(\"Memory Usage:\", memory_usage, \"GB\")\nclass MyModel(torch.nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        super(MyModel, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        x1 = self.fc1.forward(x)\n        ret = self.fc2.forward(x1)\n        return ret\ninput_size = 2000\noutput_size = 2500\nhidden_size = 3000\nmodel = MyModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\noptimizer_list = {}\nparameter_list = {}\n# will get faster over size, but the gradient may not descent as fast\n# memory usage is nearly the same\n# freeze = True\nfreeze = False\nif freeze:\n    model.eval()\nget_ram_",
        "type": "code",
        "location": "/rt_x_experiments/gradient_undescent/test_unlearning.py:1-49"
    },
    "1199": {
        "file_id": 158,
        "content": "Creating a model with three layers, initializing an optimizer, and checking memory usage.",
        "type": "comment"
    }
}