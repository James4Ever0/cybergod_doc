{
    "1200": {
        "file_id": 158,
        "content": "usage() # 0.22\n# lr = -0.001 # invalid learning rate?\nlr = 0.001\nloss_inversion = True\n# loss_inversion = False\n# usually the model prediction from itself has lower loss than the loss coming from environment. we need to increase the loss at this time, if trained using data generated by itself.\ncriterion = torch.nn.MSELoss()\n# param_limit = 1\n# param_limit = 2\nparam_limit = None\nif not freeze:\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\nelse:\n    for index, it in enumerate(model.named_parameters()):\n        if param_limit is not None:\n            if index >= param_limit: break\n        name, param = it\n        # print(name)\n        parameter_list[name] = param\n        optimizer_list[name] = torch.optim.Adam([param], lr=lr)\nget_ram_usage() # 0.22\n# fc1.weight\n# fc1.bias\n# fc2.weight\n# fc2.bias\n# create fake learning data\nbatch_size = 10\nx = torch.randn(batch_size, input_size)\ntarget = torch.randn(batch_size, output_size)\nparameter_names = list(parameter_list.keys())\nparameter_index_count = len(paramete",
        "type": "code",
        "location": "/rt_x_experiments/gradient_undescent/test_unlearning.py:49-89"
    },
    "1201": {
        "file_id": 158,
        "content": "The code sets the learning rate to 0.001 and enables loss inversion if the model is trained using data generated by itself. It then creates an optimizer using Adam algorithm with the specified learning rate for parameters that are not frozen. The code also retrieves RAM usage information at some point, and it creates fake learning data to be used during training.",
        "type": "comment"
    },
    "1202": {
        "file_id": 158,
        "content": "r_names)\n# this is good\nrandomize_param_selection = True\n# this is bad\n# randomize_param_selection = False\nimport random\nfor epoch in range(100):\n    if freeze:\n        if randomize_param_selection:\n            selected_parameter_index = random.randint(0, parameter_index_count - 1)\n        else:\n            selected_parameter_index = epoch % parameter_index_count\n        selected_parameter_name = parameter_names[selected_parameter_index]\n        # this part is not necessary. i doubt that.\n        # does not save memory\n        for pname, param in parameter_list.items():\n            if pname != selected_parameter_name:\n                param.requires_grad = False\n                param.grad = None\n            else:\n                param.requires_grad = True\n        # breakpoint()\n        optimizer = optimizer_list[selected_parameter_name]\n    # Forward pass\n    output = model(x)\n    # Compute the loss\n    loss = (-1 if loss_inversion else 1) * criterion(output, target) # so this is great. loss is getting bigger.\n  ",
        "type": "code",
        "location": "/rt_x_experiments/gradient_undescent/test_unlearning.py:89-124"
    },
    "1203": {
        "file_id": 158,
        "content": "Code comments:\n- Randomizes parameter selection if `randomize_param_selection` is True.\n- Unnecessary code block for setting parameters' gradients and requires_grad flags.\n- Loss inversion to increase or decrease the loss based on a flag.",
        "type": "comment"
    },
    "1204": {
        "file_id": 158,
        "content": "  # loss = criterion(output, target)\n    # Zero the gradients\n    # optimizer.zero_grad(set_to_none=True)\n    optimizer.zero_grad()\n    # Backward pass\n    loss.backward()\n    # Update the weights\n    optimizer.step()\n    # Print the loss every 10 epochs\n    if epoch % 10 == 9:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")",
        "type": "code",
        "location": "/rt_x_experiments/gradient_undescent/test_unlearning.py:124-138"
    },
    "1205": {
        "file_id": 158,
        "content": "Calculating the loss, zeroing gradients, performing backward pass, updating weights, and printing loss every 10 epochs.",
        "type": "comment"
    },
    "1206": {
        "file_id": 159,
        "content": "/rt_x_experiments/partial_training_network/test_freeze_one_and_train_another.py",
        "type": "filepath"
    },
    "1207": {
        "file_id": 159,
        "content": "The code creates a neural network model, manages RAM usage by freezing or training layers based on a flag, trains with Adam optimizer, and controls GPU memory usage to prevent overfitting.",
        "type": "summary"
    },
    "1208": {
        "file_id": 159,
        "content": "import torch\n# how to create the map?\n# model.named_parameter_list().__next__()\nimport psutil\nprocess = psutil.Process()\ndef get_ram_usage():\n    memory_usage = process.memory_info().rss / 1024**3  # in GB\n    print(\"Memory Usage:\", memory_usage, \"GB\")\nclass MyModel(torch.nn.Module):\n    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n        super(MyModel, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n    def forward(self, x):\n        x1 = self.fc1.forward(x)\n        ret = self.fc2.forward(x1)\n        return ret\ninput_size = 2000\noutput_size = 2500\nhidden_size = 3000\nmodel = MyModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\noptimizer_list = {}\nparameter_list = {}\n# will get faster over size, but the gradient may not descent as fast\n# memory usage is nearly the same\nfreeze = True\n# freeze = False\nif freeze:\n    model.eval()\nget_ram_usage() # 0.22\nlr = 0.001\ncriterion = t",
        "type": "code",
        "location": "/rt_x_experiments/partial_training_network/test_freeze_one_and_train_another.py:1-50"
    },
    "1209": {
        "file_id": 159,
        "content": "This code defines a simple neural network model, creates an instance of it, and initializes an optimizer. It also checks the current RAM usage and allows freezing or training the model depending on a boolean flag. The model architecture consists of two linear layers.",
        "type": "comment"
    },
    "1210": {
        "file_id": 159,
        "content": "orch.nn.MSELoss()\n# param_limit = 1\n# param_limit = 2\nparam_limit = None\nif not freeze:\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\nelse:\n    for index, it in enumerate(model.named_parameters()):\n        if param_limit is not None:\n            if index >= param_limit: break\n        name, param = it\n        # print(name)\n        parameter_list[name] = param\n        optimizer_list[name] = torch.optim.Adam([param], lr=lr)\nget_ram_usage() # 0.22\n# fc1.weight\n# fc1.bias\n# fc2.weight\n# fc2.bias\n# create fake learning data\nbatch_size = 10\nx = torch.randn(batch_size, input_size)\ntarget = torch.randn(batch_size, output_size)\nparameter_names = list(parameter_list.keys())\nparameter_index_count = len(parameter_names)\n# this is good\nrandomize_param_selection = True\n# this is bad\n# randomize_param_selection = False\nimport random\nfor epoch in range(100):\n    if freeze:\n        if randomize_param_selection:\n            selected_parameter_index = random.randint(0, parameter_index_count - 1)\n        else:\n            sele",
        "type": "code",
        "location": "/rt_x_experiments/partial_training_network/test_freeze_one_and_train_another.py:50-93"
    },
    "1211": {
        "file_id": 159,
        "content": "Defining model loss function\nSetting param_limit to either 1 or 2, freezing parameters\nCreating Adam optimizer for all parameters if not freezing, otherwise creating separate Adam optimizers for selected parameters\nChecking and logging RAM usage\nIterating through fake learning data for 100 epochs\nRandomly selecting a parameter index for optimization if randomize_param_selection is True",
        "type": "comment"
    },
    "1212": {
        "file_id": 159,
        "content": "cted_parameter_index = epoch % parameter_index_count\n        selected_parameter_name = parameter_names[selected_parameter_index]\n        # this part is not necessary. i doubt that.\n        # does not save memory\n        for pname, param in parameter_list.items():\n            if pname != selected_parameter_name:\n                param.requires_grad = False\n                param.grad = None\n            else:\n                param.requires_grad = True\n        # breakpoint()\n        optimizer = optimizer_list[selected_parameter_name]\n    # Forward pass\n    output = model(x)\n    # Compute the loss\n    loss = criterion(output, target)\n    # Zero the gradients\n    # optimizer.zero_grad(set_to_none=True)\n    optimizer.zero_grad()\n    # Backward pass\n    loss.backward()\n    # Update the weights\n    optimizer.step()\n    # Print the loss every 10 epochs\n    if epoch % 10 == 9:\n        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n# how to get memory usage?\n# Get the memory usage\n# no nothing shown of cpu\n# memory_usage = tor",
        "type": "code",
        "location": "/rt_x_experiments/partial_training_network/test_freeze_one_and_train_another.py:93-134"
    },
    "1213": {
        "file_id": 159,
        "content": "Updating the weights of selected parameters by freezing others.",
        "type": "comment"
    },
    "1214": {
        "file_id": 159,
        "content": "ch.cuda.memory_allocated(device='cpu')  # in Bytes\n# # memory_usage = torch.cuda.memory_allocated(device=device) / 1024**3  # in GB\n# print(\"Memory Usage:\", memory_usage, \"Bytes\")\n############ CPU Usage ############\nprint(\"Freeze?\", freeze)\nget_ram_usage()\n# I think maybe this is intended to be used in online training. cause it significantly reduces overfitting.\n# Freeze? False Memory Usage: 0.17539215087890625 GB\n# Epoch 100, Loss: 1.3618760931422003e-05\n# Freeze? True Memory Usage: 0.17582321166992188 GB\n# Epoch 100, Loss: 0.021172840148210526\n################################################################\n# Epoch 100, Loss: 5.7390594482421875\n# Freeze? True Memory Usage: 0.3774299621582031 GB\n# Epoch 100, Loss: 0.0014482313999906182\n# Freeze? False Memory Usage: 0.37708282470703125 GB\n# Epoch 100, Loss: 0.00897219032049179\n# Freeze? True Memory Usage: 0.32117462158203125 GB\n# Epoch 100, Loss: 0.13553307950496674\n# Freeze? True Memory Usage: 0.3498115539550781 GB\n# less memory used?",
        "type": "code",
        "location": "/rt_x_experiments/partial_training_network/test_freeze_one_and_train_another.py:134-165"
    },
    "1215": {
        "file_id": 159,
        "content": "This code snippet is controlling the GPU memory usage by freezing and unfreezing certain layers of a neural network. It prints the current memory usage and CPU usage for each epoch during training. Freezing a layer reduces overfitting, while unfreezing it allows the model to learn from new data more effectively.",
        "type": "comment"
    },
    "1216": {
        "file_id": 160,
        "content": "/rt_x_experiments/real_attention/2d_convolve.py",
        "type": "filepath"
    },
    "1217": {
        "file_id": 160,
        "content": "This code defines an input image and kernel, performs 2D convolution on the image using scipy.signal's convolve2d function, and prints the shapes of the input images and resulting convolved image. It also provides a comment about potentially applying cv2's filter2D function for additional image processing.",
        "type": "summary"
    },
    "1218": {
        "file_id": 160,
        "content": "import numpy as np\nfrom scipy.signal import convolve2d\n# Define the input image and the kernel\nimage = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\nkernel = np.array([[1, 0, -1],\n                   [2, 0, -2],\n                   [1, 0, -1]])\n# Perform 2D convolution\nconvolved_image = convolve2d(image, kernel, mode='valid')\n# Print the resulting convolved image\nprint(image.shape, kernel.shape, convolved_image.shape)\n# so this might works.\n# (3, 3) (3, 3) (1, 1)\n################ EXTRA COMPUTATION MIGHT INVOLVED ##################\n# import cv2\n# # Read the input image\n# image = cv2.imread('image.jpg')\n# # Define the kernel\n# kernel = np.array([[0, -1, 0],\n#                    [-1, 5, -1],\n#                    [0, -1, 0]])\n# # Apply filter2D\n# filtered_image = cv2.filter2D(image, -1, kernel)",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/2d_convolve.py:1-34"
    },
    "1219": {
        "file_id": 160,
        "content": "This code defines an input image and kernel, performs 2D convolution on the image using scipy.signal's convolve2d function, and prints the shapes of the input images and resulting convolved image. It also provides a comment about potentially applying cv2's filter2D function for additional image processing.",
        "type": "comment"
    },
    "1220": {
        "file_id": 161,
        "content": "/rt_x_experiments/real_attention/low_rank_positional_encoding.py",
        "type": "filepath"
    },
    "1221": {
        "file_id": 161,
        "content": "This code is implementing low-rank positional encoding using matrix multiplication and Fast Fourier Transform (FFT) in PyTorch. The resulting positional encoding is then used for attention mechanisms in a model, potentially improving its ability to learn by aligning actions with perceptions.",
        "type": "summary"
    },
    "1222": {
        "file_id": 161,
        "content": "import torch\n# you can also use fft to further enhance the performance, if it supports autograd\noriginal_height_or_width = 1024\nrank = 2\n# ifft?\nmat1 = torch.zeros((original_height_or_width, rank))\nmat2 = torch.zeros((rank, original_height_or_width))\nposenc_real= mat1 @ mat2 # instead of elementwise multiplication\nmat3 = torch.zeros((original_height_or_width, rank))\nmat4 = torch.zeros((rank, original_height_or_width))\nposenc_imag = mat3 @ mat4\n# posenc_fft = torch.fft.fft2(posenc)\nposenc = posenc_real + 1j * posenc_imag\nposenc_final = torch.fft.ifft2(posenc) # complex number\n# print(posenc.shape)\nprint(posenc.real)\n# print(posenc_final)\n# torch.Size([1024, 1024])\n# picture + (mat1 * mat2 = positional_encoding)\n# if i insert something different into the model output, like 'read forward' or 'write forward', line up actions with perceptions, maybe the model will learn more.",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/low_rank_positional_encoding.py:1-27"
    },
    "1223": {
        "file_id": 161,
        "content": "This code is implementing low-rank positional encoding using matrix multiplication and Fast Fourier Transform (FFT) in PyTorch. The resulting positional encoding is then used for attention mechanisms in a model, potentially improving its ability to learn by aligning actions with perceptions.",
        "type": "comment"
    },
    "1224": {
        "file_id": 162,
        "content": "/rt_x_experiments/real_attention/recursive_positional_encoding.py",
        "type": "filepath"
    },
    "1225": {
        "file_id": 162,
        "content": "Applying recursive positional encoding to zoomed patches, considering exponential weight or average weight options, and potentially using a uniform Gaussian distribution.",
        "type": "summary"
    },
    "1226": {
        "file_id": 162,
        "content": "# apply recursive positional encoding to zoomed patches\n# use exponential weight or average weight?\n# or you could use some uniform gausian distribution",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/recursive_positional_encoding.py:1-4"
    },
    "1227": {
        "file_id": 162,
        "content": "Applying recursive positional encoding to zoomed patches, considering exponential weight or average weight options, and potentially using a uniform Gaussian distribution.",
        "type": "comment"
    },
    "1228": {
        "file_id": 163,
        "content": "/rt_x_experiments/real_attention/sin_2d_positional_encoding.py",
        "type": "filepath"
    },
    "1229": {
        "file_id": 163,
        "content": "The code imports a package, creates a 3D tensor with zeros, initializes a PositionalEncoding2D object for rescaling, and then performs bilinear interpolation and normalization to display the resulting image using matplotlib without axis ticks or labels.",
        "type": "summary"
    },
    "1230": {
        "file_id": 163,
        "content": "# do we need a new dimension?\n# pip install positional-encodings[pytorch]\nimport torch\nimport einops\nfrom positional_encodings.torch_encodings import PositionalEncoding2D\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n# according to the formula, shall we recalculate or interpolate the encodings?\n# or we just downscale the picture?\n# we will view the thing.\n# it is not gaussian.\nchannel_count = 3\nbatch_size = 1\noriginal_image_width_or_height = 256\n# original_image_width_or_height = 1024\nscale_factor = 0.5\nscaled_image_width_or_height = int(original_image_width_or_height * scale_factor)\ninput_tensor_shape = (\n    batch_size,\n    original_image_width_or_height,\n    original_image_width_or_height,\n    channel_count,\n)\ninput_tensor = torch.zeros(input_tensor_shape)\n# how to rescale the thing?\nposenc_2d = PositionalEncoding2D(channel_count)\noutput_tensor = posenc_2d(input_tensor)\noutput_tensor_rearranged = einops.rearrange(output_tensor, \"b h w c -> b c h w\")\nnew_size = (scaled_image_width_or_height, s",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/sin_2d_positional_encoding.py:1-39"
    },
    "1231": {
        "file_id": 163,
        "content": "Importing positional encodings package.\nCreating a 3D tensor with zeros for input tensor shape.\nInitializing PositionalEncoding2D object for rescaling.\nRescaling input tensor using PositionalEncoding2D.\nRearranging the output tensor.",
        "type": "comment"
    },
    "1232": {
        "file_id": 163,
        "content": "caled_image_width_or_height)\n# first check the result of the rescaled tensor.\ninterpolated_tensor = F.interpolate(\n    output_tensor_rearranged, size=new_size, mode=\"bilinear\", align_corners=False\n)\nprint(f\"Original tensor shape: {input_tensor.shape}\")\nprint(f\"Output tensor shape: {output_tensor.shape}\")\nprint(f\"Output tensor (rearranged) shape: {output_tensor_rearranged.shape}\")\nprint(f\"Interpolated tensor shape: {interpolated_tensor.shape}\")\nimage = output_tensor_rearranged.numpy()\nimage = einops.rearrange(image, \"b c h w -> b h w c\")\n# now, view the tensor.\n# Normalize the image between 0 and 1\nimage = (image - image.min()) / (image.max() - image.min())\n# Display the image using matplotlib\nplt.imshow(image[0, :, :, :])\n# plt.imshow(image)\nplt.axis(\"off\")  # Remove axis ticks and labels\nplt.show()",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/sin_2d_positional_encoding.py:39-62"
    },
    "1233": {
        "file_id": 163,
        "content": "The code rescales the input tensor, interpolates it using bilinear interpolation, and then normalizes the resulting image between 0 and 1. It displays the normalized image using matplotlib without axis ticks or labels.",
        "type": "comment"
    },
    "1234": {
        "file_id": 164,
        "content": "/rt_x_experiments/real_attention/test_model_level_real_attention.py",
        "type": "filepath"
    },
    "1235": {
        "file_id": 164,
        "content": "This code introduces two image cropping functions, utilizing libraries like cv2 and numpy. It determines crop centers through grayscale image analysis and explores the need for positional embedding in magnified areas or its skipping, considering fractal functions.",
        "type": "summary"
    },
    "1236": {
        "file_id": 164,
        "content": "# contrary to external/environmental attention mechanism such as adjusting the zoom level, we use internal bisect multihead mechanism instead.\n# first let's define the patch size, 256x256 pixels, and anything larger than that will be downscaled.\n# we will extract the attended area and bisect. we feed it again into the network and recurse.\n# while extracting the attended area, we will mask out the padding area for sure, to avoid crop being misplaced.\nimage_path = ...\nmax_zoom_level = 3  # should this be adjustable.\n# if the attention center array is like: [(0, 0), (0, 0), (0, 0)]\n# we will do cropping right at the center, for three times\n# every number in attention center array shall be ranged from -1 to 1.\n# so how do you combine these recursive embeddings? fft?\ndef crop_at_interested_area_recursive(\n    image, attention_center_array: list[tuple[float, float]]\n):\n    ret = image.copy()\n    for center in attention_center_array:\n        ret = crop_at_interested_area(ret, center)\n    return ret\ndef check_",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/test_model_level_real_attention.py:1-28"
    },
    "1237": {
        "file_id": 164,
        "content": "This code defines a function `crop_at_interested_area_recursive` that takes an image and an attention center array, then applies cropping recursively at each attention center point in the array. The cropped image is returned after all iterations are completed.",
        "type": "comment"
    },
    "1238": {
        "file_id": 164,
        "content": "if_numer_in_range(number: float, _min: float, _max: float):\n    assert _min < _max\n    assert number >= _min\n    assert number <= _max\ndef crop_at_interested_area(image, attention_center: tuple[float, float]):\n    x_c, y_c = attention_center\n    assert check_if_numer_in_range(x_c, -1, 1)\n    assert check_if_numer_in_range(y_c, -1, 1)\n    _, x, y = image.shape()\n    half_x, half_y = x // 2, y // 2\n    quad_x, quad_y = half_x // 2, half_y // 2\n    new_x = half_x + x_c * half_x\n    new_y = half_y + y_c * half_y\n    ret = image[:, new_x - quad_x : new_x + quad_x, new_y - quad_y : new_y + quad_y]\n    return ret\n# use integral or convolution and select the max index, to reduce computation cost.\n# if you want to use multihead or something like that, you would:\n# 1 -> 1 -> 1 ...\n# 1 -> 2 -> 4 ...\n# 1 -> 3 -> 9 ...\nimport cv2\nimport numpy as np\nfrom scipy.signal import convolve2d\ndef analyze_grayscale_image_and_get_crop_center(_grayscale_image):\n    xs, ys = _grayscale_image.shape()\n    x_size, y_size = xs // 2, ys // ",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/test_model_level_real_attention.py:28-59"
    },
    "1239": {
        "file_id": 164,
        "content": "This code defines a function `crop_at_interested_area` that takes an image and an attention center (x, y) as input, and crops the image to a new area centered at the attention center. The code also imports necessary libraries such as cv2, numpy, and scipy.signal for image processing operations. Additionally, it defines a function `analyze_grayscale_image_and_get_crop_center` that analyzes a grayscale image to determine the crop center based on some image analysis techniques.",
        "type": "comment"
    },
    "1240": {
        "file_id": 164,
        "content": "2\n    kernel = np.ones((x_size, y_size))\n    grayscale_image = _grayscale_image.copy()\n    convoluted_image = convolve2d(grayscale_image, kernel, mode=\"valid\")\n    min_val, max_val, min_indx, max_indx = cv2.minMaxLoc(convoluted_image)\n    left_corner = max_indx\n    # now patch the attended area\n    x_start = left_corner[0]\n    y_start = left_corner[1]\n    x_end = x_start + x_size\n    y_end = y_start + y_size\n    grayscale_image[x_start:x_end, y_start:y_end] = 0\n    c_x = ((x_end - x_start) // 2 - x_size) / x_size\n    c_y = ((y_end - y_start) // 2 - y_size) / y_size\n    center = (c_x, c_y)\n    return grayscale_image, center\n# do it again.\ndef analyze_image_and_get_crop_center_list(image, center_count: int = 1):\n    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    center_list = []\n    for _ in range(center_count):\n        grayscale_image, center = analyze_grayscale_image_and_get_crop_center(\n            grayscale_image\n        )\n        center_list.append(center)\n    return center_list\n# now, do you ",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/test_model_level_real_attention.py:59-89"
    },
    "1241": {
        "file_id": 164,
        "content": "This code calculates the crop center by analyzing grayscale images. It convolves the image with a kernel, determines the maximum location in the convoluted image, and patches that area to zero. The function can be called multiple times to get a list of crop centers.",
        "type": "comment"
    },
    "1242": {
        "file_id": 164,
        "content": "have to do positional embedding over magnified areas? you only get embeddings! maybe we don't have to! otherwise we need some fractal function to do this job.",
        "type": "code",
        "location": "/rt_x_experiments/real_attention/test_model_level_real_attention.py:89-89"
    },
    "1243": {
        "file_id": 164,
        "content": "This code suggests that the author is considering whether positional embedding needs to be done over magnified areas or if it can be skipped. If it needs to be done, a fractal function might be required for this task.",
        "type": "comment"
    },
    "1244": {
        "file_id": 165,
        "content": "/rt_x_experiments/requirements.txt",
        "type": "filepath"
    },
    "1245": {
        "file_id": 165,
        "content": "This file contains package dependencies for the codebase.\n\nExplanation: The \"requirements.txt\" file lists the required packages, namely 'rtx-torch', 'classifier-free-guidance-pytorch', and 'efficientnet-pytorch' to be installed for the codebase to function properly.",
        "type": "summary"
    },
    "1246": {
        "file_id": 165,
        "content": "rtx-torch\nclassifier-free-guidance-pytorch\nefficientnet-pytorch",
        "type": "code",
        "location": "/rt_x_experiments/requirements.txt:1-3"
    },
    "1247": {
        "file_id": 165,
        "content": "This file contains package dependencies for the codebase.\n\nExplanation: The \"requirements.txt\" file lists the required packages, namely 'rtx-torch', 'classifier-free-guidance-pytorch', and 'efficientnet-pytorch' to be installed for the codebase to function properly.",
        "type": "comment"
    },
    "1248": {
        "file_id": 166,
        "content": "/rt_x_experiments/rt_x_test_code/requirements.txt",
        "type": "filepath"
    },
    "1249": {
        "file_id": 166,
        "content": "This code is listing two PyTorch libraries: classifier-free-guidance-pytorch and efficientnet-pytorch.",
        "type": "summary"
    },
    "1250": {
        "file_id": 166,
        "content": "classifier-free-guidance-pytorch\nefficientnet-pytorch",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/requirements.txt:1-2"
    },
    "1251": {
        "file_id": 166,
        "content": "This code is listing two PyTorch libraries: classifier-free-guidance-pytorch and efficientnet-pytorch.",
        "type": "comment"
    },
    "1252": {
        "file_id": 167,
        "content": "/rt_x_experiments/rt_x_test_code/rtx1_example.py",
        "type": "filepath"
    },
    "1253": {
        "file_id": 167,
        "content": "Creates an RTX1 model, generates random video data and instructions, computes train logits, sets the model to evaluation mode, and computes eval logits with a conditional scale of 3.0.",
        "type": "summary"
    },
    "1254": {
        "file_id": 167,
        "content": "import torch\nfrom rtx.rtx1 import RTX1\nmodel = RTX1()\nvideo = torch.randn(2, 3, 6, 224, 224)\ninstructions = [\"bring me that apple sitting on the table\", \"please pass the butter\"]\n# compute the train logits\ntrain_logits = model.train(video, instructions)\nprint('train logits:', train_logits.shape)\n# set the model to evaluation mode\nmodel.model.eval()\n# compute the eval logits with a conditional scale of 3\neval_logits = model.run(video, instructions, cond_scale=3.0)\nprint('eval_logits:', eval_logits.shape)",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx1_example.py:1-19"
    },
    "1255": {
        "file_id": 167,
        "content": "Creates an RTX1 model, generates random video data and instructions, computes train logits, sets the model to evaluation mode, and computes eval logits with a conditional scale of 3.0.",
        "type": "comment"
    },
    "1256": {
        "file_id": 168,
        "content": "/rt_x_experiments/rt_x_test_code/rtx2_example.py",
        "type": "filepath"
    },
    "1257": {
        "file_id": 168,
        "content": "The code introduces a function for enhancing model training efficiency, addressing robot action interpretation and DirectML incompatibility on RTX2, while also measuring GPU memory usage.",
        "type": "summary"
    },
    "1258": {
        "file_id": 168,
        "content": "import torch\nimport typing\nimport functools\n# import zeta.nn.attention\n# no you must change the code.\ntry:\n    from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n        apply_activation_checkpointing,\n    )\nexcept:\n    # let's patch the error.\n    import torch.distributed.algorithms._checkpoint.checkpoint_wrapper\n    def lambda_auto_wrap_policy(\n        module: torch.nn.Module,\n        recurse: bool,\n        unwrapped_params: int,\n        lambda_fn: typing.Callable,\n    ) -> bool:\n        \"\"\"\n        A convenient auto wrap policy to wrap submodules based on an arbitrary user\n        function. If `lambda_fn(submodule) == True``, the submodule will be wrapped as\n        a `wrapper_cls` unit.\n        Return if a module should be wrapped during auto wrapping.\n        The first three parameters are required by :func:`_recursive_wrap`.\n        Args:\n        module (nn.Module):\n            The module to be considered in this decision.\n        recurse (bool):\n            Indicate if this is called",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:1-35"
    },
    "1259": {
        "file_id": 168,
        "content": "This code is importing necessary modules, attempting to handle errors, and defining a lambda_auto_wrap_policy function. It appears to be related to module wrapping based on an arbitrary user function in the context of torch distributed algorithms.",
        "type": "comment"
    },
    "1260": {
        "file_id": 168,
        "content": " to make a decision on whether we\n            should recurse down a subgraph of the module structure.\n            If False, it means this function is called to make a decision\n            on whether we should wrap the said module.\n        unwrapped_params (int):\n            The number of parameters yet to be wrapped in this module.\n        lambda_fn (Callable[nn.Module] -> bool):\n            If this returns ``True``, this module will be wrapped by\n            wrapper_cls individually.\n        \"\"\"\n        if recurse:\n            # always recurse\n            return True\n        else:\n            # if not recursing, decide whether we should wrap for the leaf node or reminder\n            return lambda_fn(module)\n    def apply_activation_checkpointing_wrapper(\n        model,\n        checkpoint_wrapper_fn=torch.distributed.algorithms._checkpoint.checkpoint_wrapper.checkpoint_wrapper,\n        check_fn=lambda _: True,\n    ):\n        \"\"\"\n        Applies :func:`checkpoint_wrapper` to modules within `model` based on a",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:35-59"
    },
    "1261": {
        "file_id": 168,
        "content": "Function makes a decision to recurse down the module structure (lines 34-58)\n\nUnwrapped parameters determine if there are any left to wrap in this module.\n\nLambda function is used to decide whether to wrap a module individually or not.\n\nIf recursion is enabled, always recurse (line 40).\n\nElse, check lambda function for each module (line 43-45).",
        "type": "comment"
    },
    "1262": {
        "file_id": 168,
        "content": " user-defined\n        configuration. For each module within `model`, the `check_fn` is used to decide\n        whether `module` should be wrapped with :func:`checkpoint_wrapper` or not.\n        Note::\n            This function modifies `model` in place and replaces appropriate layers with\n            their checkpoint-wrapped modules.\n        Note::\n            This function will not wrap the overall root module. If this is needed, please directly use\n            :class:`CheckpointWrapper`.\n        Usage::\n            model = nn.Sequential(\n                nn.Linear(10, 10), nn.Linear(10, 10), nn.Linear(10, 10)\n            )\n            check_fn = lambda l: isinstance(l, nn.Linear)\n            apply_activation_checkpointing(model, checkpoint_wrapper_fn=checkpoint_wrapper, check_fn=check_fn)\n        Args:\n            module (nn.Module):\n                The model who's submodules (or self) should be wrapped with activation checkpointing.\n            checkpoint_wrapper_fn (Optional[Callable[nn.Module]])\n     ",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:59-79"
    },
    "1263": {
        "file_id": 168,
        "content": "This function applies activation checkpointing to a given model by wrapping appropriate layers with the specified wrapper function. It does not wrap the overall root module and requires the user to define a check function to determine which modules to wrap.",
        "type": "comment"
    },
    "1264": {
        "file_id": 168,
        "content": "           A `Callable` which will wrap modules\n            check_fn (Optional[Callable[nn.Module, nn.Module]])\n                A lambda function which will be passed current layer and returns\n                ``True`` or ``False`` depending on whether input layer should be wrapped.\n        Returns: None (`model` is modified inplace)\n        \"\"\"\n        # TODO: Importing inside function to avoid circular import issue between FSDP and\n        # checkpoint_wrapper. This can be resolved once wrap() APIs are decoupled from FSDP code.\n        from torch.distributed.fsdp.wrap import _recursive_wrap\n        return _recursive_wrap(\n            module=model,\n            auto_wrap_policy=functools.partial(\n                lambda_auto_wrap_policy, lambda_fn=check_fn\n            ),\n            wrapper_cls=checkpoint_wrapper_fn,\n            ignored_modules=set(),\n            ignored_params=set(),\n            only_wrap_children=True,\n        )\n    setattr(\n        torch.distributed.algorithms._checkpoint.checkpoint_wrap",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:79-101"
    },
    "1265": {
        "file_id": 168,
        "content": "The code defines a function that wraps modules in a model based on a given check function. The wrapped modules can be used for checkpointing during training to improve efficiency. It uses the _recursive_wrap function from fsdp.wrap module, and sets various parameters like auto_wrap_policy, wrapper_cls, etc. for the wrapping process.",
        "type": "comment"
    },
    "1266": {
        "file_id": 168,
        "content": "per,\n        \"apply_activation_checkpointing\",\n        apply_activation_checkpointing_wrapper,\n    )\nfrom rtx import RTX2\nmajor_torch_version = int(torch.__version__.split(\".\")[0])\nflash_attention = False\nif major_torch_version >= 2:  # use flash attention\n    print(\"Using flash attention\")\n    flash_attention = True\n# when posting ad via email. be sure there is an unsubscribe button to avoid legal issues.\ndev = None\ndevice_name = \"CPU\"\nif torch.cuda.is_available():\n    print(\"Trying to use first CUDA device.\")\n    # dev = torch.cuda.device(0)  # not working on torch 1.11\n    dev = \"cuda\"\n    device_name = \"CUDA\"\nelse:\n    print(\"`torch.cuda` is not available.\")\n    try:\n        print(\"Trying DirectML.\")\n        import torch_directml\n        dev = torch_directml.device()\n        device_name = \"DirectML\"\n    except:\n        print(\"Could not find DirectML device.\")\nprint(f\"Using {device_name}.\")\ndef forward_new(self, img: torch.Tensor, text: torch.Tensor):\n    \"\"\"Forward pass of the model.\"\"\"\n    try:\n        _encoded",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:101-137"
    },
    "1267": {
        "file_id": 168,
        "content": "The code is detecting the available device for execution and setting up the corresponding device name (CPU, CUDA or DirectML). It also initializes a function called \"forward_new\" that performs the forward pass of the model.",
        "type": "comment"
    },
    "1268": {
        "file_id": 168,
        "content": " = self.encoder(img, return_embeddings=True)\n        print(\"encoded context shape: {}\".format(_encoded.shape))\n        # torch.Size([2, 64, 512])\n        # b, wtf, 2*dim\n        encoded = _encoded\n        # the shape is fixed. damn.\n        # now we either need addition or some thin nn\n        # encoded = torch.zeros((2,128,512)).to(dev)\n        # encoded = torch.zeros((2,64,1024)).to(dev)\n        # can we use arbitrary input? can we?\n        return self.decoder(text, context=encoded)\n    except Exception as error:\n        print(f\"Failed in forward method: {error}\")\n        raise\nRTX2.forward = forward_new\n# uninstalling and reinstalling 'timm' 'zetascale' and 'beartype' helps.\n# is it data corruption?\n# windows is not supported\n# 'NoneType' object has no attribute 'cadam32bit_grad_fp32'\nbatch_size = 1\n# batch_size = 2\ninput_length = 1024  # output_length = input_length - 1\n# usage\n# it is trying to expand the observation space to infinity, till the model says it is end.\nimg = torch.randn(\n    batch_size, 3, 2",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:137-167"
    },
    "1269": {
        "file_id": 168,
        "content": "Code is defining a forward method for an RTX2 class. It encodes images with an encoder, prints the shape of the encoded context, and then tries to decode text using the encoded context as input to the decoder. If any exception occurs, it prints the error message and raises the error. The batch size and input length are defined for testing purposes.",
        "type": "comment"
    },
    "1270": {
        "file_id": 168,
        "content": "56, 256\n)  # the size of the image is not the same as vit.\ntext = torch.randint(\n    0, 20000, (batch_size, input_length)\n)  # one of 20000 logits, 1024 as count\n# want classification? you can either use more tokens or use special classifiers.\n# for me, just use more tokens. we will train this model in multiple \"continuous\" scenarios anyway.\n# also benefit from llms\n# adjust the resolution according to the environment\n# what is similar transformer in audio files like the ViT?\n# how do we put audio in?\n# approach 1: audio -> mel graph -> ViT -> embedding (ref: https://github.com/YuanGongND/ast)\n# approach 2: native audio transformers (ref: https://github.com/lucidrains/audiolm-pytorch)\n# how to merge?\n# approach 1: linear addition\n# approach 2: concatenation\n# approach 3: concatenation with thin linear nns\n# visual: add global & local area for the robot to inspect\n# hierarchical cascade structure? like small -> medium -> large\n# THE MOST IMPORTANT THING IS TO FIGURE OUT HOW TO CONNECT THE LM WITH VIT\nmodel",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:167-194"
    },
    "1271": {
        "file_id": 168,
        "content": "The code is creating a random image dataset for training a transformer model. It uses torch.randint() to generate logits, adjusting the resolution based on the environment and considering different approaches to handle audio input with ViTs. The most important task is finding a way to connect the language model (LM) with the ViT.",
        "type": "comment"
    },
    "1272": {
        "file_id": 168,
        "content": " = RTX2(attn_flash=flash_attention)\n# let's use the gpu.\n# breakpoint()\nif dev is not None:\n    model.to(dev)\n    output1, output2 = model(img.to(dev), text.to(dev))\nelse:\n    output1, output2 = model(img, text)\n# output1: torch.Size([1, 1023, 20000]) (logits)\n# output2 is a single number (loss? how come?)\n# this is the same as text input. is it?\n# or just trying to reduce the loss against input text?\nprint(\"output logits:\", output1.shape, output2.shape)  # with gradient!\nprint(\"device:\", output1.device)\n# breakpoint()\n# output logits: torch.Size([2, 1023, 20000]) torch.Size([])\n# device: privateuseone:0\n# possible implementations:\n# i decide to go with the latter.\n# 1. interpret the robot action sequentially in a loop, if separated by any other token the interpretation ends, and it will start over from the beginning\n# 2. use indicator/classifier to determine what type of action the robot is taking for every token (token classification)\n# not working for DirectML\n# memory_usage = torch.cuda.memory_allocated",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:194-221"
    },
    "1273": {
        "file_id": 168,
        "content": "The code is initializing a RTX2 model and moving it to the GPU if one is specified. The model takes in an image and text input, and returns logits (output1) for the image and a single loss value (output2) as well as printing the shapes of these outputs along with the device they are on. The code then explains possible implementations for interpreting the robot action sequentially or using token classification. It also mentions that this implementation does not work for DirectML and measures GPU memory usage.",
        "type": "comment"
    },
    "1274": {
        "file_id": 168,
        "content": "(device=dev) / 1024**3  # in GB\n# print(\"Memory Usage:\", memory_usage, \"GB\")\n############ CPU Usage ############\nimport psutil\nprocess = psutil.Process()\nmemory_usage = process.memory_info().rss / 1024**3  # in GB\nprint(\"Memory Usage:\", memory_usage, \"GB\")\n# ref: https://github.com/microsoft/DirectML/issues/444",
        "type": "code",
        "location": "/rt_x_experiments/rt_x_test_code/rtx2_example.py:221-233"
    },
    "1275": {
        "file_id": 168,
        "content": "Computes and prints the system's memory usage in GB.",
        "type": "comment"
    },
    "1276": {
        "file_id": 169,
        "content": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py",
        "type": "filepath"
    },
    "1277": {
        "file_id": 169,
        "content": "The code introduces a hierarchical Transformer language model called \"Hourglass,\" utilizing relative attention, resampling, research-based layers, and customizable decoder blocks for various NLP tasks and autoregressive language modeling. The Transformer decoder network function allows for layer and architecture customization, creating an hourglass-shaped language model for efficient processing.",
        "type": "summary"
    },
    "1278": {
        "file_id": 169,
        "content": "# coding=utf-8\n# Copyright 2023 The Trax Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Hourglass - a hierarchical Transformer language model.\"\"\"\nimport trax.layers as tl\nfrom trax.layers.research.rel_attention import get_rel_att_inputs\nfrom trax.layers.research.rel_attention import RelativeAttentionWrapper\nfrom trax.layers.research.resampling import AttentionResampling\nfrom trax.layers.research.resampling import AveragePooling\nfrom trax.layers.research.resampling import FeedForwardBlock\nfrom trax.layers.",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:1-24"
    },
    "1279": {
        "file_id": 169,
        "content": "This code is for a hierarchical Transformer language model called \"Hourglass\" which utilizes relative attention, resampling, and research-based layers.",
        "type": "comment"
    },
    "1280": {
        "file_id": 169,
        "content": "research.resampling import LinearUpsampling\nfrom trax.models.research.configurable_transformer import ApplyAttentionLayer\ndef _RelativeDecoderBlock(attention_type, d_model, d_ff, n_heads, dropout,\n                          dropout_shared_axes, mode, ff_activation,\n                          context_bias_layer, location_bias_layer,\n                          total_pooling):\n  \"\"\"Returns a list of layers.\n    The layers implement a Transformer decoder block with relative attention\n  parametrization.\n  The input to the block is a pair, (activations, mask), where the mask was\n  created from the original source tokens to prevent attending to the padding\n  part of the input.\n  Args:\n    attention_type: attention type.\n    d_model: Final dimension of tensors at most points in the model, including\n      the initial embedding output.\n    d_ff: Size of special dense layer in the feed-forward part of each block.\n    n_heads: Number of attention heads.\n    dropout: Stochastic rate (probability) for dropping an activa",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:24-47"
    },
    "1281": {
        "file_id": 169,
        "content": "This code defines a function that creates a Transformer decoder block with relative attention parameterization. The block takes in parameters such as attention type, d_model (final dimension of tensors), d_ff (size of dense layer in feed-forward part), n_heads (number of attention heads), dropout rate, etc.",
        "type": "comment"
    },
    "1282": {
        "file_id": 169,
        "content": "tion value when\n      applying dropout within a block.\n    dropout_shared_axes: Tensor axes on which to share a dropout mask. Sharing\n      along batch and sequence axes (`dropout_shared_axes=(0,1)`) is a useful\n      way to save memory and apply consistent masks to activation vectors at\n      different sequence positions.\n    mode: If `'train'`, each block will include dropout; else, it will pass all\n      values through unaltered.\n    ff_activation: Type of activation function at the end of each block; must be\n      an activation-type subclass of `Layer`.\n    context_bias_layer: context bias layer.\n    location_bias_layer: location bias layer.\n    total_pooling: The combined pool size of previously used funnel blocks.\n  Returns:\n    A list of layers that maps (activations, att_vecs, mask) to\n                               (activations, att_vecs, mask).\n  \"\"\"\n  if attention_type == RelativeAttentionWrapper:\n    attention = RelativeAttentionWrapper(\n        d_model,\n        n_heads,\n        dropout,\n       ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:47-70"
    },
    "1283": {
        "file_id": 169,
        "content": "Applies dropout to block activations based on shared axes and mode, returns a list of layers for attention calculation.",
        "type": "comment"
    },
    "1284": {
        "file_id": 169,
        "content": " mode=mode,\n        context_bias_layer=context_bias_layer,\n        location_bias_layer=location_bias_layer,\n        total_pooling=total_pooling)\n  else:\n    attention = ApplyAttentionLayer(\n        attention_type,\n        d_model,\n        n_heads,\n        d_model // n_heads,\n        d_model // n_heads,\n        causal=True,\n        masked=False,\n        attention_dropout=dropout,\n        output_dropout=dropout,\n        attention_chunk_size=0,  # Disables tl.Chunk in ApplyAttentionLayer.\n        mode=mode,\n    )\n  feed_forward = FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes,\n                                  mode, ff_activation)\n  def _Dropout():\n    return tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)\n  return [\n      tl.Residual(  # vecs\n          tl.LayerNorm(),\n          attention,\n          _Dropout(),\n      ),  # vecs\n      tl.Residual(\n          tl.LayerNorm(),\n          feed_forward,\n          _Dropout(),\n      ),  # vecs\n  ]\ndef _parse_hierarchy(hierarchy_str):  # pylint: di",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:70-109"
    },
    "1285": {
        "file_id": 169,
        "content": "This code defines a function for creating a hierarchical language model with attention and feed-forward layers, along with layer normalization and dropout regularization. It can be used for various natural language processing tasks.",
        "type": "comment"
    },
    "1286": {
        "file_id": 169,
        "content": "sable = invalid-name\n  \"\"\"Parse hierarchy for Hourglass definition.\"\"\"\n  levels = hierarchy_str.split(' ')\n  if levels != levels[::-1]:\n    raise ValueError('Hierarchy is not a palindrome')\n  layer_level_pairs = [(x.split('@')) for x in levels[:1 + (len(levels) // 2)]]\n  hierarchy_n_layers = [int(x[0]) for x in layer_level_pairs]\n  total_sf_per_level = [int(x[1]) for x in layer_level_pairs]\n  hierarchy_shorten_factors = []\n  for current_sf, prev_sf in zip(total_sf_per_level,\n                                 [1] + total_sf_per_level[:-1]):\n    if current_sf % prev_sf != 0:\n      raise ValueError(\n          f'Hierarchy not divisible by previous level: {current_sf}, {prev_sf}')\n    hierarchy_shorten_factors.append(current_sf // prev_sf)\n  return hierarchy_n_layers, hierarchy_shorten_factors\ndef HourglassLM(vocab_size,\n                d_model=512,\n                d_ff=2048,\n                vanilla_layers=(1, 1),\n                hierarchy='6@3',\n                n_heads=8,\n                dropout=0.1,\n            ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:109-136"
    },
    "1287": {
        "file_id": 169,
        "content": "This code defines a function HourglassLM that takes in parameters such as vocabulary size, model dimensions, and hierarchy structure. It returns the number of layers and shorten factors for each layer based on the given hierarchy. If the hierarchy is not a palindrome or not divisible by previous levels, it raises a ValueError.",
        "type": "comment"
    },
    "1288": {
        "file_id": 169,
        "content": "    dropout_shared_axes=None,\n                mode='train',\n                ff_activation=tl.FastGelu,\n                vanilla_attn_type=RelativeAttentionWrapper,\n                middle_attn_type=RelativeAttentionWrapper,\n                downsampling_fn=AttentionResampling,\n                upsampling_fn=AttentionResampling,\n                attention_downsampling_fn=AveragePooling,\n                attention_upsampling_fn=LinearUpsampling):\n  \"\"\"Returns a hierarchical Transformer language model.\n  This model performs autoregressive language modeling:\n    - input: rank 2 tensor representing a batch of text strings via token IDs\n      plus padding markers; shape is (batch_size, sequence_length). The tensor\n      elements are integers in `range(vocab_size)`, and `0` values mark padding\n      positions.\n    - output: rank 3 tensor representing a batch of log-probability\n      distributions for each sequence position over possible token IDs;\n      shape is (batch_size, sequence_length, `vocab_size`).\n  This mo",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:136-158"
    },
    "1289": {
        "file_id": 169,
        "content": "This code defines a hierarchical Transformer language model for autoregressive language modeling, taking input as batch of text strings via token IDs and producing output as rank 3 tensor representing log-probability distributions over possible token IDs.",
        "type": "comment"
    },
    "1290": {
        "file_id": 169,
        "content": "del uses only the decoder part of the overall Transformer.\n  Args:\n    vocab_size: Input vocabulary size -- each element of the input tensor should\n      be an integer in `range(vocab_size)`. These integers typically represent\n      token IDs from a vocabulary-based tokenizer.\n    d_model: Final dimension of tensors at most points in the model, including\n      the initial embedding output.\n    d_ff: Size of special dense layer in the feed-forward part of each encoder\n      block.\n    vanilla_layers: (pre_layers, post_layers) tuple - number of full token-level\n      Transformer decoder layers before and after shortening.\n    hierarchy: string - shortening hierarchy, as described in the paper.\n      Hierarchy levels must form a palindrome, e.g. '1@2 2@6 1@2'.\n    n_heads: Number of attention heads.\n    dropout: Stochastic rate (probability) for dropping an activation value when\n      applying dropout within an encoder block.\n    dropout_shared_axes: Tensor axes on which to share a dropout mask. Sharing\n ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:158-176"
    },
    "1291": {
        "file_id": 169,
        "content": "This code defines a Transformer decoder with optional shortening. The input parameters include vocabulary size, d_model, d_ff, vanilla_layers, hierarchy, n_heads, and dropout.",
        "type": "comment"
    },
    "1292": {
        "file_id": 169,
        "content": "     along batch and sequence axes (`dropout_shared_axes=(0,1)`) is a useful\n      way to save memory and apply consistent masks to activation vectors at\n      different sequence positions.\n    mode: str: 'train' or 'eval'.\n    ff_activation: Type of activation function at the end of each encoder block;\n      must be an activation-type subclass of `Layer`.\n    vanilla_attn_type: class: attention class such as SelfAttention to use in\n      the layers before and after shortening (vanilla layers).\n    middle_attn_type: class: attention class to use in the middle layers (these\n      operating on the shortened sequence).\n    downsampling_fn: function that takes full token-level vectors of length `l`\n      and transforms them into `l` / `k` vectors, where `k` denotes\n      `shorten_factor` parameter.\n    upsampling_fn: function that takes shortened representations of a sequence,\n      consisting of `l` / `k` vectors and transforms them into full token-level\n      representations of length `l`.\n    attention",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:176-192"
    },
    "1293": {
        "file_id": 169,
        "content": "The code defines a function for building an hourglass LM model. It takes arguments for dropout_shared_axes, mode, ff_activation, vanilla_attn_type, middle_attn_type, downsampling_fn, and upsampling_fn to construct the model.",
        "type": "comment"
    },
    "1294": {
        "file_id": 169,
        "content": "_downsampling_fn: Downsampling function that transforms token-level\n      vectors into query vectors with reduced length. Necessary only when\n      AttentionResampling is used as `downsampling_fn`.\n    attention_upsampling_fn: Upsampling function for AttentionResampling. Valid\n      only when AttentionResampling is used as a `upsampling_fn`.\n  Returns:\n    A Transformer language model as a layer that maps from a tensor of tokens\n    to activations over a vocab set.\n  \"\"\"\n  assert mode != 'predict'  # For now, 'predict' mode is unsupported.\n  hierarchy_n_layers, hierarchy_shorten_factors = _parse_hierarchy(hierarchy)\n  token_encoder = [\n      tl.Embedding(vocab_size, d_model),\n      tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode)\n  ]\n  context_bias_layer, location_bias_layer = get_rel_att_inputs(d_model, n_heads)\n  n_pre_decoder_blocks, n_post_decoder_blocks = vanilla_layers\n  def create_decoder_blocks(n_layers, total_pooling,  # pylint: disable = invalid-name\n                       ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:192-215"
    },
    "1295": {
        "file_id": 169,
        "content": "This code is defining a function that creates a Transformer language model. It includes an embedding layer, dropout layer, relative attention inputs, and decoder blocks. The function also takes into account the hierarchical structure of the model, with shortening factors for each level. However, it does not support 'predict' mode at this time.",
        "type": "comment"
    },
    "1296": {
        "file_id": 169,
        "content": "     attention_type):\n    decoder_blocks = [\n        # pylint: disable=g-complex-comprehension\n        _RelativeDecoderBlock(attention_type, d_model, d_ff, n_heads, dropout,\n                              dropout_shared_axes, mode, ff_activation,\n                              context_bias_layer, location_bias_layer,\n                              total_pooling) for _ in range(n_layers)\n    ]\n    return decoder_blocks + [tl.LayerNorm()]\n  def create_hourglass_valley(rest_shorten_factors, rest_n_funnel_blocks,  # pylint: disable = invalid-name\n                              current_total_pooling):\n    assert rest_shorten_factors\n    assert len(rest_shorten_factors) == len(rest_n_funnel_blocks)\n    current_sf = rest_shorten_factors[0]\n    current_n_layers = rest_n_funnel_blocks[0]\n    shortening_layer = downsampling_fn(\n        current_sf,\n        d_model,\n        is_upsampling=False,\n        d_ff=d_ff,\n        n_heads=n_heads,\n        dropout=dropout,\n        dropout_shared_axes=dropout_shared_axes,\n        mode=",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:215-241"
    },
    "1297": {
        "file_id": 169,
        "content": "The code is creating a decoder block and a function for the hourglass valley. The decoder block consists of multiple layers with different parameters, and the function creates an hourglass valley layer with downsampling based on given factors.",
        "type": "comment"
    },
    "1298": {
        "file_id": 169,
        "content": "mode,\n        ff_activation=ff_activation,\n        context_bias_layer=context_bias_layer,\n        location_bias_layer=location_bias_layer,\n        total_pooling=current_total_pooling,\n        resampling_fn=attention_downsampling_fn)\n    upsampling_layer = upsampling_fn(\n        current_sf,\n        d_model=d_model,\n        is_upsampling=True,\n        d_ff=d_ff,\n        n_heads=n_heads,\n        dropout=dropout,\n        dropout_shared_axes=dropout_shared_axes,\n        mode=mode,\n        ff_activation=ff_activation,\n        context_bias_layer=context_bias_layer,\n        location_bias_layer=location_bias_layer,\n        total_pooling=current_total_pooling,\n        resampling_fn=attention_upsampling_fn)\n    if len(rest_shorten_factors) > 1:  # we need to go deeper again\n      pre_stage_blocks = create_decoder_blocks(\n          current_n_layers, current_total_pooling * current_sf,\n          middle_attn_type)\n      post_stage_blocks = create_decoder_blocks(\n          current_n_layers, current_total_pooling * current_sf",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:241-269"
    },
    "1299": {
        "file_id": 169,
        "content": "Creates downsampling and upsampling layers for decoder blocks with specified parameters.",
        "type": "comment"
    }
}