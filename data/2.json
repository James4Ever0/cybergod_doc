{
    "200": {
        "file_id": 22,
        "content": "        char = char.encode()\n                if char == b\"\":\n                    break\n                read_byte_len += 1\n                self.read_entropy_calc.count(char)\n                if len(head_content) < self.read_head_bytes:\n                    head_content += char\n                else:\n                    tail_content.append(char)\n            except Exception as e:\n                if type(e) not in READ_KNOWN_EXCEPTIONS:\n                    traceback.print_exc()\n                break\n        tail_content = b\"\".join(list(tail_content))\n        if read_byte_len <= self.read_head_bytes + self.read_tail_bytes:\n            sep = b\"\"\n        else:\n            sep = b\"\\n...\\n\"\n        content = sep.join([head_content, tail_content])\n        print(\"read:\", get_repr(content), sep=\"\\t\")\n        self.read_bytes += read_byte_len\n        return content\n    def __del__(self):\n        # TODO: separate calculation logic from here, to be used in metaheuristics\n        stats = self.stats\n        print(\"summary\".center",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:391-418"
    },
    "201": {
        "file_id": 22,
        "content": "Iteratively reads input, splits content into head and tail based on length, handles exceptions, and returns joined content with optional separator.",
        "type": "comment"
    },
    "202": {
        "file_id": 22,
        "content": "(50, \"=\"))\n        print(\"start time:\", formatTimeAtShanghai(stats.start_time), sep=\"\\t\")\n        print(\"end time:\", formatTimeAtShanghai(stats.end_time), sep=\"\\t\")\n        print(\"up time:\", stats.up_time, sep=\"\\t\")\n        print(\"loop count:\", stats.loop_count, sep=\"\\t\")\n        print(\"total bytes read:\", stats.read_bytes, sep=\"\\t\")\n        print(\"total bytes write:\", stats.write_bytes, sep=\"\\t\")\n        print(\"r/w ratio:\", stats.rw_ratio)\n        print(\"w/r ratio:\", stats.wr_ratio)\n        print(\"read bytes entropy:\", stats.read_ent)\n        print(\"write bytes entropy:\", stats.write_ent)\n        print(\"r/w entropy ratio:\", stats.rw_ent_ratio)\n        print(\"w/r entropy ratio:\", stats.wr_ent_ratio)\n    def getStatsDict(self):\n        start_time = self.start_time\n        end_time = time.time()\n        up_time = end_time - self.start_time\n        read_ent = self.read_entropy_calc.entropy\n        write_ent = self.write_entropy_calc.entropy\n        loop_count = self.loop_count\n        rw_ratio, wr_ratio = le",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:418-439"
    },
    "203": {
        "file_id": 22,
        "content": "Displays statistics in a tabular format, including start and end times, uptime, loop count, bytes read/written, and entropy ratios.\nCreates a dictionary containing up_time, read_ent, write_ent, and loop_count.",
        "type": "comment"
    },
    "204": {
        "file_id": 22,
        "content": "ftAndRightSafeDiv(self.read_bytes, self.write_bytes)\n        rw_ent_ratio, wr_ent_ratio = leftAndRightSafeDiv(read_ent, write_ent)\n        statsDict = dict(\n            start_time=start_time,\n            end_time=end_time,\n            up_time=up_time,\n            loop_count=loop_count,\n            read_ent=read_ent,\n            read_bytes=self.read_bytes,\n            write_bytes=self.write_bytes,\n            write_ent=write_ent,\n            rw_ratio=rw_ratio,\n            wr_ratio=wr_ratio,\n            rw_ent_ratio=rw_ent_ratio,\n            wr_ent_ratio=wr_ent_ratio,\n        )\n        return statsDict\n    @property\n    def stats(self):\n        # TODO: calculate recent statistics, not just full statistics\n        # somehow cached.\n        if not (\n            isinstance(self._stats, self.actorStatsClass)\n            and self._stats.loop_count == self.loop_count\n        ):\n            statsDict = self.getStatsDict()\n            self._stats = self.actorStatsClass(**statsDict)\n        return self._stats\n    def loop(s",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:439-469"
    },
    "205": {
        "file_id": 22,
        "content": "This code calculates and stores statistics for an actor's performance. It employs safe division functions to calculate ratios and creates a dictionary of statistics (e.g., start/end times, up time, loop count) before returning or storing them in the `_stats` attribute for later use.",
        "type": "comment"
    },
    "206": {
        "file_id": 22,
        "content": "elf):\n        self.read()\n        self.write(NaiveVocab.generate())\n        return True\n    def init_check(self):\n        \"\"\"\n        Check or wait until the interactive program emits expected output.\n        \"\"\"\n        ret = func_timeout.func_timeout(self.max_init_time, self._init_check)\n        print(\"init check passed\")\n        return ret\n    def _init_check(self):\n        \"\"\"\n        Implementation of init checks.\n        \"\"\"\n        ...\n    def heartbeat(self):\n        # to prove the program as if still running.\n        # do not override this method, unless you know what you are doing.\n        access_time = heartbeat_base(uuid = actor_uuid, action = 'heartbeat', pid=current_pid, role='client')\n        print('beat at:', access_time)\n        return True\n    def run(self):\n        loop = True\n        try:\n            self.init_check()\n        except:\n            print(\"init check failed\")\n            log_and_print_unknown_exception()\n            raise InteractiveChallengeFailed(\n                f\"Failed to pass ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:469-504"
    },
    "207": {
        "file_id": 22,
        "content": "468-503: This code segment defines methods for interacting with an actor, including reading and writing data, initializing the check, performing init checks, emitting heartbeat, and running the program. The code also handles exceptions and failure cases, such as failed init checks, by logging and raising InteractiveChallengeFailed.",
        "type": "comment"
    },
    "208": {
        "file_id": 22,
        "content": "init challenge of: {self.__class__.__name__}\"\n            )\n        while self.heartbeat():\n            loop = self.loop()\n            if loop is True:\n                print(f\"[loop\\t{str(self.loop_count)}]\".center(60, \"-\"))\n                self.loop_count += 1\n            else:\n                break\n        print(\n            f\"{'heartbeat' if loop else 'loop'} failed.\\nexiting at #{self.loop_count}.\"\n        )\ndef run_naive(cls):\n    actor = cls(f\"{sys.executable} naive_interactive.py\")\n    actor.run()\nif __name__ == \"__main__\":\n    run_naive(NaiveActor)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:504-524"
    },
    "209": {
        "file_id": 22,
        "content": "The code defines a class-based actor that runs a loop until a condition is met, then exits. It also includes a function to run an instance of the class as the main program.",
        "type": "comment"
    },
    "210": {
        "file_id": 23,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_interactive.py",
        "type": "filepath"
    },
    "211": {
        "file_id": 23,
        "content": "Imports time module and NaiveVocab, creates a class for interactive loop with random sleep and word generation.",
        "type": "summary"
    },
    "212": {
        "file_id": 23,
        "content": "# import time\nfrom vocabulary import NaiveVocab\n# will sleep for random time and respond.\nclass NaiveInteractive:\n    def __init__(self):\n        self.sleep = 1\n    def loop(self):\n        input()\n        print(NaiveVocab.generate())\n        return True\n    def run(self):\n        while self.loop():\n            ...\nif __name__ == \"__main__\":\n    interactive = NaiveInteractive()\n    interactive.run()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_interactive.py:1-23"
    },
    "213": {
        "file_id": 23,
        "content": "Imports time module and NaiveVocab, creates a class for interactive loop with random sleep and word generation.",
        "type": "comment"
    },
    "214": {
        "file_id": 24,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/predictive_alpine_actor.py",
        "type": "filepath"
    },
    "215": {
        "file_id": 24,
        "content": "Code imports necessary modules, defines a class for a predictive AlpineActor, sets up the predictor wrapper with a specified ksize and naive predictor. The loop method reads content, enqueues it to the predictor wrapper, gets predicted content based on the write_len, writes the content, and returns True.",
        "type": "summary"
    },
    "216": {
        "file_id": 24,
        "content": "import random\nfrom alpine_actor import AlpineActor, run_actor_forever\nfrom sequence_learner import NaivePredictor, PredictorWrapper\n# from vocabulary import AsciiVocab\nfrom vocabulary import BytesVocab\nclass PredictiveAlpineActor(AlpineActor):\n    def __init__(self, ksize = 256):\n        self.predictorWrapper = PredictorWrapper(ksize, NaivePredictor)\n        self.predictorWrapper.seq.extend(list(BytesVocab.generate()))\n        # self.predictorWrapper.seq.extend([ord(c) for c in AsciiVocab.generate()])\n        super().__init__()\n    @property\n    def write_len(self):\n        return random.randint(10, 30)\n    @AlpineActor.timeit\n    def loop(self):\n        read_content = self.read()\n        self.predictorWrapper.enqueue(list(read_content))\n        predicted_content = self.predictorWrapper.predict(self.write_len)\n        write_content = bytes(predicted_content)\n        self.write(write_content)\n        return True\nif __name__ == \"__main__\":\n    run_actor_forever(PredictiveAlpineActor)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/predictive_alpine_actor.py:1-32"
    },
    "217": {
        "file_id": 24,
        "content": "Code imports necessary modules, defines a class for a predictive AlpineActor, sets up the predictor wrapper with a specified ksize and naive predictor. The loop method reads content, enqueues it to the predictor wrapper, gets predicted content based on the write_len, writes the content, and returns True.",
        "type": "comment"
    },
    "218": {
        "file_id": 25,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/predictive_bytes_alpine_actor.py",
        "type": "filepath"
    },
    "219": {
        "file_id": 25,
        "content": "Creates a PredictiveAlpineBytesActor class that inherits from BytesActor and PredictiveAlpineActor, then runs the actor forever if this script is the main one.",
        "type": "summary"
    },
    "220": {
        "file_id": 25,
        "content": "from bytes_actor import BytesActor\nfrom predictive_alpine_actor import PredictiveAlpineActor, run_actor_forever\nclass PredictiveAlpineBytesActor(BytesActor, PredictiveAlpineActor):\n    ...\nif __name__ == \"__main__\":\n    run_actor_forever(PredictiveAlpineBytesActor)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/predictive_bytes_alpine_actor.py:1-10"
    },
    "221": {
        "file_id": 25,
        "content": "Creates a PredictiveAlpineBytesActor class that inherits from BytesActor and PredictiveAlpineActor, then runs the actor forever if this script is the main one.",
        "type": "comment"
    },
    "222": {
        "file_id": 26,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/profile_vis.py",
        "type": "filepath"
    },
    "223": {
        "file_id": 26,
        "content": "Importing pstats module and creating a Stats object for analyzing the profiling data.\n\nStorage location: \"basic_interactive_program_emulation_and_image_with_docker_support/profile_vis.py\":6-10\nCode:\n```\n# from pstats import SortKey\np = pstats.Stats(\"alpine_actor.profile\", SortBy=\"calls\")\n```\nComment for code:\nSorting the stats by number of calls.",
        "type": "summary"
    },
    "224": {
        "file_id": 26,
        "content": "import pstats\n# from pstats import SortKey\np = pstats.Stats(\"alpine_actor.profile\")\nstats = p.strip_dirs().sort_stats(2)\nstats.print_stats()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/profile_vis.py:1-6"
    },
    "225": {
        "file_id": 26,
        "content": "Importing pstats module and creating a Stats object for analyzing the profiling data.\n\nStorage location: \"basic_interactive_program_emulation_and_image_with_docker_support/profile_vis.py\":6-10\nCode:\n```\n# from pstats import SortKey\np = pstats.Stats(\"alpine_actor.profile\", SortBy=\"calls\")\n```\nComment for code:\nSorting the stats by number of calls.",
        "type": "comment"
    },
    "226": {
        "file_id": 27,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/pull_alpine_with_different_arch.py",
        "type": "filepath"
    },
    "227": {
        "file_id": 27,
        "content": "This code downloads and saves Docker images of Alpine Linux with different architectures in separate directories. It ensures the target directory is created if it doesn't exist and handles platform-specific architecture pulls for \"amd64\", \"arm64\", \"arm/v7\", and \"i386\".",
        "type": "summary"
    },
    "228": {
        "file_id": 27,
        "content": "arch_list = [\"amd64\", \"arm64\", \"arm/v7\", \"i386\"]\ntarget_image = \"alpine:3.7\"\ntarget_fname_prefix = target_image.replace(\":\", \"_\")\nimport os\ndirpath = \"docker_images\"\nif os.path.exists(dirpath):\n    raise Exception(f\"target directory '{dirpath}' already exists.\")\nos.mkdir(dirpath)\nfor arch in arch_list:\n    fpath = f\"{target_fname_prefix}_{arch.replace('/','')}.tar\"\n    cmds = [\n        f'docker pull --platform \"linux/{arch}\" {target_image}',\n        f\"docker save {target_image} -o {dirpath}/{fpath}\",\n        f\"docker rmi {target_image}\",\n    ]\n    for cmd in cmds:\n        print(\"executing:\", cmd)\n        os.system(cmd)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/pull_alpine_with_different_arch.py:1-19"
    },
    "229": {
        "file_id": 27,
        "content": "This code downloads and saves Docker images of Alpine Linux with different architectures in separate directories. It ensures the target directory is created if it doesn't exist and handles platform-specific architecture pulls for \"amd64\", \"arm64\", \"arm/v7\", and \"i386\".",
        "type": "comment"
    },
    "230": {
        "file_id": 28,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/pyinstrument_profile.py",
        "type": "filepath"
    },
    "231": {
        "file_id": 28,
        "content": "Profiling long-running program using PyInstrument.",
        "type": "summary"
    },
    "232": {
        "file_id": 28,
        "content": "import pyinstrument\nprof = pyinstrument.Profiler()\nprof.start()\n...  # long running program. remember to timeout\nprof.stop()\nprof.print()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/pyinstrument_profile.py:1-7"
    },
    "233": {
        "file_id": 28,
        "content": "Profiling long-running program using PyInstrument.",
        "type": "comment"
    },
    "234": {
        "file_id": 29,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/requirements.txt",
        "type": "filepath"
    },
    "235": {
        "file_id": 29,
        "content": "This is a list of dependencies required for the project. The code checks the operating system and adds specific packages accordingly, such as pexpect and whexpect for POSIX and Windows systems respectively, py-applescript for MacOS, pygetwindow for both Windows and MacOS, docker, func-timeout, progressbar2, better-exceptions, easyprocess, requests, and frozendict. The # comment indicates that frozendict may be optional.",
        "type": "summary"
    },
    "236": {
        "file_id": 29,
        "content": "pexpect==4.6.0; os_name == 'posix'\nwexpect==4.0.0; os_name == 'nt'\npy-applescript; platform_system == 'Darwin'\npygetwindow; platform_system == 'Windows'\npygetwindow; platform_system == 'Darwin'\ndocker\nfunc-timeout\nprogressbar2\nbetter-exceptions\neasyprocess\nelevate\nrequests\n# frozendict",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/requirements.txt:1-13"
    },
    "237": {
        "file_id": 29,
        "content": "This is a list of dependencies required for the project. The code checks the operating system and adds specific packages accordingly, such as pexpect and whexpect for POSIX and Windows systems respectively, py-applescript for MacOS, pygetwindow for both Windows and MacOS, docker, func-timeout, progressbar2, better-exceptions, easyprocess, requests, and frozendict. The # comment indicates that frozendict may be optional.",
        "type": "comment"
    },
    "238": {
        "file_id": 30,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py",
        "type": "filepath"
    },
    "239": {
        "file_id": 30,
        "content": "This code handles Docker service management across different OS, including stopping, restarting, and killing. It uses platform-specific commands but experiences hanging issues on Win11. The code ensures successful launch through multiple trials with a wait period on Windows or macOS.",
        "type": "summary"
    },
    "240": {
        "file_id": 30,
        "content": "# TODO: eliminate stale containers by restarting docker every 10 sessions.\nMACOS_DOCKER_APP_BINARY = \"/Applications/Docker.app/Contents/MacOS/Docker\"\n# killall Docker && open -j -a Docker\n# ps aux | grep Docker.app | grep -v grep | awk '{print $2}' | xargs -Iabc kill abc\nHIDE_DOCKER_ASCRIPT = \"\"\"\ntell application \"System Events\"\n    set visible of processes where name is \"Docker Desktop\" to false\nend tell\n\"\"\"\nWINDOW_TITLE_KW = \"Docker Desktop\"\n# killall docker\n# MACOS_TERM_DOCKER_APP = \"\"\" bash -c 'ps aux | grep Docker.app | grep -v grep | awk \"{print \\\\$2}\" | xargs -I abc kill abc' \"\"\"\nMACOS_KILL_DOCKER_APP = \"\"\" bash -c 'ps aux | grep Docker.app | grep -v grep | awk \"{print \\\\$2}\" | xargs -I abc kill -s KILL abc' \"\"\"\nimport subprocess\nLINUX_CONTROL_DOCKER_SERVICE_CMDGEN = lambda action: f\"sudo systemctl {action} docker\"\nLINUX_RESTART_DOCKER_COMMAND = LINUX_CONTROL_DOCKER_SERVICE_CMDGEN(\"restart\")\nLINUX_STOP_DOCKER_COMMAND = LINUX_CONTROL_DOCKER_SERVICE_CMDGEN(\"stop\")\nLINUX_START_DOCKER_COMMAND = LINU",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:1-19"
    },
    "241": {
        "file_id": 30,
        "content": "Code snippet is related to controlling and restarting the Docker daemon on different operating systems. It provides different methods for MacOS and Linux systems to stop, start or kill the Docker application or service respectively.",
        "type": "comment"
    },
    "242": {
        "file_id": 30,
        "content": "X_CONTROL_DOCKER_SERVICE_CMDGEN(\"start\")\n# DOES NOT WORK ON WIN11\n# kill com.docker.backend.exe? seems to be hanging\n# WINDOWS_TERM_DOCKER_COMMAND = 'taskkill /FI \"IMAGENAME eq Docker*\"'\nWINDOWS_KILL_DOCKER_COMMAND = 'taskkill /FI \"IMAGENAME eq Docker*\" /F'\n# start program minimized? instead use pygetwindow to hide the window once found.\n# find 'Docker Desktop.exe'\n# which docker -> ../../ -> 'Docker Desktop.exe'\n# WINDOWS_RESTART_DOCKER_COMMAND = 'powershell -Command \"Restart-Service -Name *docker*\"' # need elevated permissions\n# Stop-Service & Start-Service\n# net stop com.docker.service/docker & net start com.docker.service/docker\nimport platform\nimport elevate\nimport shutil\nimport os\nkill_safe_codes = [0]\nstart_safe_codes = [0]\nsysname = platform.system()\nREQUIRED_BINARIES = [\"docker\"]\nelevate_needed = False\nDOCKER_DESKTOP_EXE = \"Docker Desktop.exe\"\nfrom typing import List\ndef execute_os_command_and_assert_safe_exit(cmd: str, safe_codes: List[int] = [0]):\n    ret = os.system(cmd)  # use cmd.exe on windows",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:19-49"
    },
    "243": {
        "file_id": 30,
        "content": "This code aims to stop, restart, or kill the Docker service on Windows systems. It first checks the operating system, then defines a command to kill the Docker process, and lists the required binary \"docker\". The function `execute_os_command_and_assert_safe_exit` is used to execute operating system commands while asserting safe exit codes. However, it mentions that the code does not work on Win11, as the command for killing Docker seems to be hanging.",
        "type": "comment"
    },
    "244": {
        "file_id": 30,
        "content": ".\n    assert (\n        ret in safe_codes\n    ), f\"Abnormal exit code {ret} while executing following command:\\n{cmd}\"\nkill_docker_cmds = []\nstart_docker_cmds = []\nif sysname in [\"Windows\", \"Darwin\"]:\n    import pygetwindow\nelse:\n    def check_if_docker_window_exists():\n        return False\nif sysname == \"Windows\":\n    REQUIRED_BINARIES.append(\"taskkill\")\n    docker_path = shutil.which(\"docker\")\n    docker_bin_path = os.path.dirname(docker_path)\n    docker_desktop_dir_path = os.path.split(os.path.split(docker_bin_path)[0])[0]\n    docker_desktop_exe_path = os.path.join(docker_desktop_dir_path, DOCKER_DESKTOP_EXE)\n    assert os.path.exists(\n        docker_desktop_exe_path\n    ), f'Failed to find docker desktop executable at: \"{docker_desktop_exe_path}\"'\n    # kill_docker_cmds.append(WINDOWS_TERM_DOCKER_COMMAND)\n    kill_docker_cmds.append(WINDOWS_KILL_DOCKER_COMMAND)\n    start_docker_cmds.append(f'start \"\" \"{docker_desktop_exe_path}\"')  # bloody chatgpt.\n    def hide_docker():\n        for win in pygetwindow.getW",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:49-83"
    },
    "245": {
        "file_id": 30,
        "content": "Checking if Docker window exists and handling Windows specific commands for killing and starting Docker.",
        "type": "comment"
    },
    "246": {
        "file_id": 30,
        "content": "indowsWithTitle(WINDOW_TITLE_KW):\n            win: pygetwindow.Win32Window\n            win.hide()\n    def check_if_docker_window_exists():\n        wins = pygetwindow.getWindowsWithTitle(WINDOW_TITLE_KW)\n        exist = len(wins) > 0\n        return exist\nelif sysname == \"Linux\":\n    REQUIRED_BINARIES.append(\"systemctl\")\n    elevate_needed = True\n    kill_docker_cmds.append(LINUX_STOP_DOCKER_COMMAND)\n    start_docker_cmds.append(LINUX_START_DOCKER_COMMAND)\n    def hide_docker():\n        ...\nelif sysname == \"Darwin\":\n    import applescript\n    HIDE_DOCKER_ASCRIPT_OBJ = applescript.AppleScript(HIDE_DOCKER_ASCRIPT)\n    kill_safe_codes.append(256)\n    REQUIRED_BINARIES.extend([\"killall\", \"open\", MACOS_DOCKER_APP_BINARY])\n    kill_docker_cmds.extend(\n        [\n            \"killall Docker\",\n            \"killall docker\",\n            #  MACOS_TERM_DOCKER_APP,\n            MACOS_KILL_DOCKER_APP,\n        ]\n    )\n    # start_docker_cmds.append(MACOS_DOCKER_APP_BINARY)\n    start_docker_cmds.append(\"open -j -a Docker\")\n    start_",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:83-120"
    },
    "247": {
        "file_id": 30,
        "content": "The code is checking if the Docker window exists and hiding it based on the operating system. It also defines commands for starting and stopping Docker, depending on the system (Windows, Linux, or macOS).",
        "type": "comment"
    },
    "248": {
        "file_id": 30,
        "content": "docker_cmds.append(f\"open -j -a {MACOS_DOCKER_APP_BINARY}\")\n    start_docker_cmds.append(\"open -a Docker\")\n    start_docker_cmds.append(f\"open -a {MACOS_DOCKER_APP_BINARY}\")\n    def hide_docker():\n        HIDE_DOCKER_ASCRIPT_OBJ.run()\n    def check_if_docker_window_exists():\n        exist = any([WINDOW_TITLE_KW in t for t in pygetwindow.getAllTitles()])\n        return exist\nelse:\n    raise Exception(f\"Unknown platform: {sysname}\")\ndef kill_docker():\n    for cmd in kill_docker_cmds:\n        execute_os_command_and_assert_safe_exit(cmd, kill_safe_codes)\ndef start_docker():\n    for cmd in start_docker_cmds:\n        execute_os_command_and_assert_safe_exit(cmd, start_safe_codes)\nDOCKER_KILLED_KWS = [\n    \"the docker daemon is not running\",\n    \"Cannot connect to the Docker daemon\",\n    \"error during connect\",\n]\ndef verify_docker_killed(timeout=5, encoding=\"utf-8\", inverse: bool = False):\n    output = (\n        subprocess.Popen(\n            [\"docker\", \"ps\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n   ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:120-157"
    },
    "249": {
        "file_id": 30,
        "content": "This code appears to be for controlling the Docker daemon. It includes functions to start, stop, and check if the Docker window is open. The code also handles different platforms (possibly MacOS and others) by appending specific commands or raising an exception for unknown platforms. There are also lists of keywords to recognize when checking if Docker has been killed or not.",
        "type": "comment"
    },
    "250": {
        "file_id": 30,
        "content": "     .communicate(timeout=timeout)[1]\n        .decode(encoding)\n    )\n    killed = any([kw in output for kw in DOCKER_KILLED_KWS])\n    if not inverse:\n        if not killed:\n            raise Exception(\n                f\"Docker not killed.\\nCaptured output from command `docker ps`:\\n{output}\"\n            )\n    else:\n        if killed:\n            raise Exception(\n                f\"Docker not started.\\nCaptured output from command `docker ps`:\\n{output}\"\n            )\nimport time\ndef verify_docker_launched(retries=10, sleep=3, daemon=False):\n    success = False\n    for i in range(retries):\n        try:\n            if not daemon:\n                exist = check_if_docker_window_exists()\n                print(f\"window exists? {exist}\")\n            else:\n                exist = False\n            if not exist:\n                verify_docker_killed(inverse=True)\n            success = True\n            break\n        except Exception as e:\n            if i < retries - 1:\n                print(f\"Retrying in {sleep} seconds...\")\n",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:157-192"
    },
    "251": {
        "file_id": 30,
        "content": "This code checks if Docker is running and handles the case when it's not running or when it's a daemon. It attempts to verify the Docker launch multiple times, with a specified number of retries and sleep time between each retry. If it fails after all retries, it raises an exception.",
        "type": "comment"
    },
    "252": {
        "file_id": 30,
        "content": "                time.sleep(sleep)\n            else:\n                raise e\n    return success\ndef restart_docker():\n    check_required_binaries()\n    print(\"prerequisites checked\")\n    kill_docker()\n    print(\"docker killed\")\n    verify_docker_killed()\n    print(\"kill has been verified\")\n    start_docker()\n    print(\"docker restarted\")\nimport shutil\ndef check_required_binaries():\n    for name in REQUIRED_BINARIES:\n        resolved_path = shutil.which(name)\n        assert resolved_path, f\"{name} is not available in PATH.\"\n        assert os.path.exists(\n            resolved_path\n        ), f\"{name} does not exist.\\nfilepath: {resolved_path}\"\n        print(f\"'{name}' found\")\n# working!\ndef restart_and_verify():\n    # this could be faulty! still stuck even if docker is killed on macOS\n    restart_docker()\n    if sysname in [\"Windows\", \"Darwin\"]:\n        verify_docker_launched()\n        print(\"docker restart verified\")\n        hide_docker()\n        print(\"docker window minimized\")\n    verify_docker_launched(daemon=True)\n",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:192-232"
    },
    "253": {
        "file_id": 30,
        "content": "The code attempts to check if required binaries are available in the PATH and restart Docker. It first checks for the existence of each binary, raises an error if any is missing, and then restarts and verifies the Docker daemon. If running on Windows or macOS, it also hides the Docker window after verification.",
        "type": "comment"
    },
    "254": {
        "file_id": 30,
        "content": "    print(\"docker daemon restart verified\")\nif elevate_needed:\n    elevate.elevate(graphical=False)\nif __name__ == \"__main__\":\n    # kill & perform checks if you really have killed docker.\n    # restart & check if restart is successful.\n    # do it once more.\n    for i in range(2):\n        print(f\"trial #{i}\")\n        restart_and_verify()\n        time.sleep(3)  # m1 is running too damn fast. or is it?",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/rerun_docker_daemon.py:232-245"
    },
    "255": {
        "file_id": 30,
        "content": "This code is checking the restart and successful operation of the Docker daemon. It restarts the daemon twice, performs checks, and waits for 3 seconds between trials to ensure stable results.",
        "type": "comment"
    },
    "256": {
        "file_id": 31,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/sequence_learner.py",
        "type": "filepath"
    },
    "257": {
        "file_id": 31,
        "content": "The code presents a predictor class for sequences using NaivePredictor and PredictorWrapper. It initializes the PredictorWrapper with a NaivePredictor and enqueues a sequence [0, 1, 2, 3, 4]. The next 100 tokens are predicted based on sequence length of 10.",
        "type": "summary"
    },
    "258": {
        "file_id": 31,
        "content": "from collections import deque\nfrom typing import List\nimport numpy as np\nclass NaivePredictor:\n    def __init__(self, ksize: int):\n        self.ksize = ksize\n        self.kernel = np.random.rand(self.ksize)\n    def predict(self, x: List[int]):\n        x_processed = x[-self.ksize :]\n        if len(x_processed) < self.ksize:\n            x_processed = [0] * (self.ksize - len(x_processed)) + x_processed\n        x_one_hot = self.one_hot(x_processed)\n        next_token = np.matmul(x_one_hot, self.kernel)\n        ret = np.argmax(next_token)\n        return ret\n    def one_hot(self, x):\n        x_one_hot = np.eye(self.ksize)[x]\n        return x_one_hot\nclass PredictorWrapper:\n    def __init__(self, ksize: int, predictor_cls: NaivePredictor):\n        self.predictor: NaivePredictor = predictor_cls(ksize)\n        self.seq = deque([], maxlen=ksize)\n    def enqueue(self, seq: List[int]):\n        for tok in seq:\n            self.seq.append(tok)\n    def predict(self, seqlen: int):\n        ret_seq = []\n        for _ in range(seq",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/sequence_learner.py:1-37"
    },
    "259": {
        "file_id": 31,
        "content": "Code describes a class that implements a predictor for sequences, using a NaivePredictor and PredictorWrapper. The NaivePredictor uses a kernel to predict the next token in the sequence based on the last ksize elements of the input list. The PredictorWrapper maintains a deque with a maximum length of ksize and provides a method to enqueue new sequences, as well as a method to predict the next token for a given sequence length.",
        "type": "comment"
    },
    "260": {
        "file_id": 31,
        "content": "len):\n            tok = self.predictor.predict(list(self.seq))\n            self.seq.append(tok)\n            ret_seq.append(tok)\n        return ret_seq\nif __name__ == \"__main__\":\n    pw = PredictorWrapper(10, NaivePredictor)\n    pw.enqueue([0, 1, 2, 3, 4])\n    total_seq = pw.predict(100)\n    # ksize = 10\n    # predictor = NaivePredictor(ksize=ksize)\n    # seq = deque([0, 1, 2, 3, 4], maxlen=ksize)\n    # total_seq = list(seq)\n    # for _ in range(100):\n    #     tok = predictor.predict(list(seq))\n    #     seq.append(tok)\n    #     total_seq.append(tok)\n    print(\"total seq:\", total_seq)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/sequence_learner.py:37-56"
    },
    "261": {
        "file_id": 31,
        "content": "This code initializes a PredictorWrapper object with a NaivePredictor and enqueues a sequence [0, 1, 2, 3, 4]. It then predicts the next tokens for the next 100 steps using the wrapped predictor and prints the total sequence. The naive predictor seems to be used for prediction based on the input sequence length of 10.",
        "type": "comment"
    },
    "262": {
        "file_id": 32,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/test_beat_server.py",
        "type": "filepath"
    },
    "263": {
        "file_id": 32,
        "content": "Testing the beat server using unittest. Can also test killing server or normal clients. Imports from beat_common module.",
        "type": "summary"
    },
    "264": {
        "file_id": 32,
        "content": "# unittest for our damn beat server. so damn error prone.\n# you can also do unittest for kill server or normal clients.\nfrom beat_common import *",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/test_beat_server.py:1-3"
    },
    "265": {
        "file_id": 32,
        "content": "Testing the beat server using unittest. Can also test killing server or normal clients. Imports from beat_common module.",
        "type": "comment"
    },
    "266": {
        "file_id": 33,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/timeout_utils.py",
        "type": "filepath"
    },
    "267": {
        "file_id": 33,
        "content": "Imports libraries for timeouts and retrying. Defines timeout_func decorator for functions with no arguments and a nested retrying_timeout_func decorator for retrying and timeouts. Includes the \"__all__\" variable to export these functions.",
        "type": "summary"
    },
    "268": {
        "file_id": 33,
        "content": "import functools\nimport func_timeout\nimport retrying\n# let's not use multitasking\n# timeout decorator for func with no arg/kwarg\ntimeout_func = lambda timeout: (\n    lambda func: functools.partial(\n        func_timeout.func_timeout, timeout=timeout, func=func\n    )\n)\nretrying_timeout_func = lambda retry_max_count, timeout: (\n    lambda func:\n    # lambda func: multitasking.task(\n    retrying.retry(stop_max_attempt_number=retry_max_count)(timeout_func(timeout)(func))\n    # )\n)\n__all__ = [\"timeout_func\", \"retrying_timeout_func\"]",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/timeout_utils.py:1-22"
    },
    "269": {
        "file_id": 33,
        "content": "Imports libraries for timeouts and retrying. Defines timeout_func decorator for functions with no arguments and a nested retrying_timeout_func decorator for retrying and timeouts. Includes the \"__all__\" variable to export these functions.",
        "type": "comment"
    },
    "270": {
        "file_id": 34,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/to_be_killed.py",
        "type": "filepath"
    },
    "271": {
        "file_id": 34,
        "content": "This code imports os and time modules, gets the process ID (pid), and continuously prints the pid every second until terminated by os.kill().",
        "type": "summary"
    },
    "272": {
        "file_id": 34,
        "content": "import os\nimport time\npid = os.getpid()\n# killed by os.kill!\nwhile True:\n    print(\"process pid:\", pid)\n    time.sleep(1)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/to_be_killed.py:1-9"
    },
    "273": {
        "file_id": 34,
        "content": "This code imports os and time modules, gets the process ID (pid), and continuously prints the pid every second until terminated by os.kill().",
        "type": "comment"
    },
    "274": {
        "file_id": 35,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/type_utils.py",
        "type": "filepath"
    },
    "275": {
        "file_id": 35,
        "content": "These functions ensure the input is either a bytes or string type, and raise exceptions if it's not.",
        "type": "summary"
    },
    "276": {
        "file_id": 35,
        "content": "def enforce_bytes(s):\n    if isinstance(s, str):\n        s = s.encode()\n    if not isinstance(s, bytes):\n        raise Exception(\"unknown line content type:\", type(s))\n    return s\ndef enforce_str(content):\n    if isinstance(content, bytes):\n        content = content.decode()\n    if not isinstance(content, str):\n        raise Exception(\"Invalid content type: %s\\nShould be: str\" % type(content))\n    return content",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/type_utils.py:1-14"
    },
    "277": {
        "file_id": 35,
        "content": "These functions ensure the input is either a bytes or string type, and raise exceptions if it's not.",
        "type": "comment"
    },
    "278": {
        "file_id": 36,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/vocabulary.py",
        "type": "filepath"
    },
    "279": {
        "file_id": 36,
        "content": "NaiveVocab class generates a random string of characters from a given list and AsciiVocab/BytesVocab inherit from NaiveVocab to handle both ASCII and bytes content respectively.",
        "type": "summary"
    },
    "280": {
        "file_id": 36,
        "content": "import random\nfrom type_utils import *\nclass NaiveVocab:\n    charlist = [\"a\"]\n    startpoint = \"\"\n    content_typeguard = enforce_str\n    @classmethod\n    def generate(cls):\n        content = cls.startpoint\n        for _ in range(random.randint(1, 10)):\n            char = random.choice(cls.charlist)\n            content += char\n        return content\n    @classmethod\n    def filter(cls, content):\n        content = cls.content_typeguard(content)\n        result = cls.startpoint\n        for char in content:\n            if char in cls.charlist:\n                result += char\n        return result\nclass AsciiVocab(NaiveVocab):\n    charlist = [chr(x) for x in range(256)]\nclass BytesVocab(NaiveVocab):\n    startpoint = b\"\"\n    charlist = [bytes([x]) for x in range(256)]\n    content_typeguard = enforce_bytes",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/vocabulary.py:1-36"
    },
    "281": {
        "file_id": 36,
        "content": "NaiveVocab class generates a random string of characters from a given list and AsciiVocab/BytesVocab inherit from NaiveVocab to handle both ASCII and bytes content respectively.",
        "type": "comment"
    },
    "282": {
        "file_id": 37,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/wexpect_example.py",
        "type": "filepath"
    },
    "283": {
        "file_id": 37,
        "content": "Starts a cmd child process, waits for prompt, sends commands, prints output, and exits.",
        "type": "summary"
    },
    "284": {
        "file_id": 37,
        "content": "import wexpect\n# Start cmd as child process\nchild = wexpect.spawn(\"cmd.exe\")\n# Wait for prompt when cmd becomes ready.\nchild.expect(\">\")\n# Prints the cmd's start message\nprint(\"before\", child.before)\nprint(\"after\", child.after)\n# run list directory command\nchild.sendline(\"ls\")\n# Waiting for prompt\nchild.expect(\">\")\n# Prints content of the directory\nprint(child.before, end=\"\")\nprint(child.after, end=\"\")\n# Exit from cmd\nchild.sendline(\"exit\")\n# Waiting for cmd termination.\nchild.wait()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/wexpect_example.py:1-27"
    },
    "285": {
        "file_id": 37,
        "content": "Starts a cmd child process, waits for prompt, sends commands, prints output, and exits.",
        "type": "comment"
    },
    "286": {
        "file_id": 38,
        "content": "/binary_program_synthesis_cpu_assembly_execution/README.md",
        "type": "filepath"
    },
    "287": {
        "file_id": 38,
        "content": "Idea: Programs ingest and evolve via evolutionary process, directly executing assembly code without human interface. Interaction through isolated environments like a computer virus. Consider hierarchical tokenizer or embedding for memory efficiency.",
        "type": "summary"
    },
    "288": {
        "file_id": 38,
        "content": "I had an idea that programs shall ingest programs and learn to evolve in a evolutionary manner.\nThe program shall directly execute assembly code. So obviously, no human interface is needed.\nYou may ask how do we interact with such program? Consider computer virus. We first run it in isolated environments, then we interact.\n---\nyou may make hierarchical tokenizer or hierarchical embedding to reduce memory consumption",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/README.md:1-9"
    },
    "289": {
        "file_id": 38,
        "content": "Idea: Programs ingest and evolve via evolutionary process, directly executing assembly code without human interface. Interaction through isolated environments like a computer virus. Consider hierarchical tokenizer or embedding for memory efficiency.",
        "type": "comment"
    },
    "290": {
        "file_id": 39,
        "content": "/binary_program_synthesis_cpu_assembly_execution/bnn_data_ingest.py",
        "type": "filepath"
    },
    "291": {
        "file_id": 39,
        "content": "The code imports libraries, defines the `makeQuant` function to create a quantizer layer, and initializes variables. It generates a random tensor, applies embedding and quantization layers, performs matrix multiplication and softmax operation for attention, and outputs binary values from the quantized results while printing intermediate outputs.",
        "type": "summary"
    },
    "292": {
        "file_id": 39,
        "content": "import larq  # keras/tensorflow\n# ref: https://github.com/itayhubara/BinaryNet.pytorch\nimport tensorflow as tf\nimport random\n# tf.experimental.numpy.experimental_enable_numpy_behavior()\ndim = 2\n# dim = 10\ndef makeQuant(in_dim, out_dim):\n    layer = larq.layers.QuantDense(  # this will regulate all values into integers\n        units=out_dim,  # anything -> 2\n        # units=dim,\n        input_quantizer=larq.quantizers.SteSign(clip_value=1.0),\n        kernel_quantizer=larq.quantizers.SteSign(clip_value=1.0),\n        kernel_constraint=larq.constraints.WeightClip(clip_value=1.0),\n        # input_shape=(42,), # still working? not in sequential.\n        input_shape=(in_dim,),\n    )\n    return layer\n# larq.layers.QuantDense(units=2)\n# dim = 1024\n# model = tf.keras.Sequential()\nseqlen = 20\n# (AB * BA) * (BA * AB)\n# random_x_in = [[random.randint(0, 1) for _ in range(seqlen)]]\nrandom_x_in = [[random.randint(0, 1) for _ in range(seqlen)]]\nx_in = tf.convert_to_tensor(random_x_in, dtype=tf.float32)  # [1, 20]\n# x_in = tf.r",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/bnn_data_ingest.py:1-37"
    },
    "293": {
        "file_id": 39,
        "content": "The code imports necessary libraries and defines a function `makeQuant` that creates a quantizer layer for regulating input and kernel values to integers. It also initializes variables, including the dimension (`dim`) and sequence length (`seqlen`), and creates a tensor `x_in` with random integer data.",
        "type": "comment"
    },
    "294": {
        "file_id": 39,
        "content": "andom.uniform(shape=(1, dim), minval=0, maxval=2, dtype=tf.float32)\nl_emb = tf.keras.layers.Embedding(2, dim)\nt_emb = l_emb(x_in)\nl_q = makeQuant(dim, dim)\nl_k = makeQuant(dim, dim)\nl_v = makeQuant(dim, dim)\nt_q = l_q(t_emb)\nt_k = l_k(t_emb)\nt_v = l_v(t_emb)\nt_att_pre = tf.matmul(t_k, t_q, transpose_a=True)\nt_att = tf.nn.softmax(t_att_pre, axis=2) / (dim**0.5)\nt_feat = tf.matmul(t_v, t_att)\nl_out = makeQuant(dim, 2)\nt_out = l_out(t_feat)\nprint(x_in)\nprint()\nprint(t_out)  # not within 1 and 0\nbinary = tf.argmax(t_out, axis=2)\nprint()\nprint(binary)\nprint(binary.shape, t_out.shape)\n# breakpoint()",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/bnn_data_ingest.py:37-65"
    },
    "295": {
        "file_id": 39,
        "content": "This code generates a random tensor, applies embedding and quantization layers to it, performs matrix multiplication and softmax operation for attention, and finally outputs binary values from the quantized results. The code also prints these intermediate outputs for debugging purposes.",
        "type": "comment"
    },
    "296": {
        "file_id": 40,
        "content": "/binary_program_synthesis_cpu_assembly_execution/convert_binary_files_as_zero_and_one_streams.py",
        "type": "filepath"
    },
    "297": {
        "file_id": 40,
        "content": "This code reads a binary file, converts it to a stream of zeroes and ones, groups the bits into 8-bit groups (bytes), and writes the result in text format with one byte per line.",
        "type": "summary"
    },
    "298": {
        "file_id": 40,
        "content": "# binaries? this reminds me of binary neural networks. whoa there!\n# import numpy as np\nfilepath = \"README.md\"\noutput_path = \"converted_binary_view.txt\"\n# import binascii\n# this can do the job of hex\nimport itertools\nwith open(filepath, \"rb\") as f:\n    _bytes = f.read()\n    # char = f.read(1)\n    # arr = np.frombuffer(char, dtype=np.bool_) # this will not work.\n    # arr = np.frombuffer(char, dtype=np.uint8)\n    # arr = np.array(list(char)).astype(np.uint8)\n    # print(arr)\n    # print(char)\n    # bin_array = np.unpackbits(arr)\n    # bin_array = np.unpackbits(arr).astype(np.bool_)\n    # print(bin_array)  # [False  True False False  True False False  True]\n    bit_repr = [format(it, '08b') for it in _bytes]\n    line_list = []\n    for index, bit_group in itertools.groupby(enumerate(bit_repr), key=lambda x: x[0]//8):\n        line = \" \".join([it for _, it in bit_group])\n        line_list.append(line)\n    with open(output_path, 'w+') as fw:\n        fw.write(\"\\n\".join(line_list))",
        "type": "code",
        "location": "/binary_program_synthesis_cpu_assembly_execution/convert_binary_files_as_zero_and_one_streams.py:1-27"
    },
    "299": {
        "file_id": 40,
        "content": "This code reads a binary file, converts it to a stream of zeroes and ones, groups the bits into 8-bit groups (bytes), and writes the result in text format with one byte per line.",
        "type": "comment"
    }
}