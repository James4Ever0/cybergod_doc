{
    "100": {
        "file_id": 9,
        "content": "   logger.warning(f\"Value {result} smaller than 0\")\n            return 0\n        return result\n    def to_ndarray(\n        self,\n    ) -> np.ndarray:\n        action_type_ndarray = np.zeros((len(self.action_types), 1))\n        action_type_ndarray[self.action_types.index(self.action_type)] = 1\n        key_ndarray = np.zeros((len(self.keys), 1))\n        if self.key:\n            key_ndarray[self.keys.index(self.key)] = 1\n        mouse_button_ndarray = np.zeros((len(self.mouse_buttons), 1))\n        if self.mouse_button:\n            mouse_button_ndarray[self.mouse_buttons.index(self.mouse_button)] = 1\n        mouse_pressed_array = np.zeros((1, 1))\n        if self.mouse_pressed:\n            mouse_pressed_array[0] = 1\n        x_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.x:\n            x_ndarray[\n                self.round_within(self.mouse_resolution * self.x / self.max_x, \"X\")\n            ] = 1\n        y_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.y:\n            y_ndarray[\n     ",
        "type": "code",
        "location": "/conscious_struct.py:477-508"
    },
    "101": {
        "file_id": 9,
        "content": "Creates a numpy array for each action type, key, mouse button, and coordinates, with 1's at their respective indices if they are not None.",
        "type": "comment"
    },
    "102": {
        "file_id": 9,
        "content": "           self.round_within(self.mouse_resolution * self.y / self.max_y, \"Y\")\n            ] = 1\n        dx_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.dx:\n            dx_ndarray[\n                self.round_within(\n                    self.mouse_resolution * (self.dx + self.max_x) / (2 * self.max_x),\n                    \"DX\",\n                )\n            ] = 1\n        dy_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.dy:\n            dy_ndarray[\n                self.round_within(\n                    self.mouse_resolution * (self.dy + self.max_y) / (2 * self.max_y),\n                    \"DY\",\n                )\n            ] = 1\n        ndarray = np.concatenate(\n            [\n                action_type_ndarray,\n                key_ndarray,\n                mouse_button_ndarray,\n                mouse_pressed_array,\n                x_ndarray,\n                y_ndarray,\n                dx_ndarray,\n                dy_ndarray,\n            ]\n        )\n        return ndarray\n    def to_actio",
        "type": "code",
        "location": "/conscious_struct.py:508-543"
    },
    "103": {
        "file_id": 9,
        "content": "The code initializes and concatenates multiple numpy arrays representing various actions and parameters (action type, key pressed, mouse button, mouse position, etc.). It then returns the concatenated array.",
        "type": "comment"
    },
    "104": {
        "file_id": 9,
        "content": "n_json(\n        self,\n    ) -> Union[list, None]:\n        action_type = self.action_type\n        if action_type:\n            if action_type == \"key_press\":\n                assert self.key in self.keys\n                action_args = self.key\n            elif action_type == \"key_release\":\n                assert self.key in self.keys\n                action_args = self.key\n            elif action_type == \"mouse_move\":\n                assert self.x >= 0 and self.x <= self.max_x\n                assert self.y >= 0 and self.y <= self.max_y\n                action_args = [self.x, self.y]\n            elif action_type == \"mouse_click\":\n                assert self.x >= 0 and self.x <= self.max_x\n                assert self.y >= 0 and self.y <= self.max_y\n                assert self.mouse_button in self.mouse_buttons\n                assert type(self.mouse_pressed) == bool\n                action_args = [self.x, self.y, self.mouse_button, self.mouse_pressed]\n            elif action_type == \"mouse_scroll\":\n                as",
        "type": "code",
        "location": "/conscious_struct.py:543-569"
    },
    "105": {
        "file_id": 9,
        "content": "This function checks the action type and constructs appropriate arguments for each type of action (key press/release, mouse move/click/scroll). It asserts that all required conditions are met before constructing the action_args.",
        "type": "comment"
    },
    "106": {
        "file_id": 9,
        "content": "sert self.x >= 0 and self.x <= self.max_x\n                assert self.y >= 0 and self.y <= self.max_y\n                assert self.dx >= -self.max_x and self.dx <= self.max_x\n                assert self.dy >= -self.max_y and self.dy <= self.max_y\n                action_args = [self.x, self.y, self.dx, self.dy]\n            else:\n                raise Exception(\"Unknown action_type: %s\" % action_type)\n            action_json = [action_type, action_args]\n        else:\n            action_json = None\n        return action_json\n#########################\n#  HID DATA VALIDATION  #\n#########################\nfrom pydantic import confloat\nclass KeyPress(BaseModel):\n    _action_type = \"key_press\"\n    key: HIDActionTypes.keys\n    def to_list(self) -> List:\n        return [self._action_type, self.key]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = [lst[1]]\n        assert len(action_args) == 1\n        assert action_type == cls._action_type\n        assert isinstance(action",
        "type": "code",
        "location": "/conscious_struct.py:569-605"
    },
    "107": {
        "file_id": 9,
        "content": "Code is validating input values and converting them into JSON format for use in the application. It also handles exceptions for unknown action types and ensures proper data structure.",
        "type": "comment"
    },
    "108": {
        "file_id": 9,
        "content": "_args[0], HIDActionTypes.keys)\n        return cls(key=action_args[0])\nclass KeyRelease(BaseModel):\n    _action_type = \"key_release\"\n    key: HIDActionTypes.keys\n    def to_list(self) -> List:\n        return [self._action_type, self.key]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = [lst[1]]\n        assert len(action_args) == 1\n        assert action_type == cls._action_type\n        assert isinstance(action_args[0], HIDActionTypes.keys)\n        return cls(key=action_args[0])\nclass MouseClick(BaseModel):\n    _action_type = \"mouse_click\"\n    x: confloat(ge=0)\n    y: confloat(ge=0)\n    button: HIDActionTypes.mouse_buttons\n    pressed: bool\n    def to_list(self) -> List:\n        return [self._action_type, [self.x, self.y, self.button, self.pressed]]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = lst[1]\n        assert len(action_args) == 4\n        assert action_type == cls._action_type\n        assert isinstanc",
        "type": "code",
        "location": "/conscious_struct.py:605-648"
    },
    "109": {
        "file_id": 9,
        "content": "This code defines two classes, KeyRelease and MouseClick, that inherit from BaseModel. Both classes have methods to convert objects to a list and vice versa. The to_list method returns a list representing the action type and relevant parameters (key for KeyRelease, coordinates, button, and press state for MouseClick). The from_list classmethod parses a list and creates an instance of the respective class with the correct values.",
        "type": "comment"
    },
    "110": {
        "file_id": 9,
        "content": "e(action_args[0], confloat(ge=0))\n        assert isinstance(action_args[1], confloat(ge=0))\n        assert isinstance(action_args[2], HIDActionTypes.mouse_buttons)\n        assert isinstance(action_args[3], bool)\n        return cls(\n            x=action_args[0],\n            y=action_args[1],\n            button=action_args[2],\n            pressed=action_args[3],\n        )\nclass MouseMove(BaseModel):\n    _action_type = \"mouse_move\"\n    x: confloat(ge=0)\n    y: confloat(ge=0)\n    def to_list(self) -> List:\n        return [self._action_type, [self.x, self.y]]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = lst[1]\n        assert len(action_args) == 2\n        assert action_type == cls._action_type\n        assert isinstance(action_args[0], confloat(ge=0))\n        assert isinstance(action_args[1], confloat(ge=0))\n        return cls(x=action_args[0], y=action_args[1])\nclass MouseScroll(BaseModel):\n    _action_type = \"mouse_scroll\"\n    x: confloat(ge=0)\n    y: confloat(",
        "type": "code",
        "location": "/conscious_struct.py:648-687"
    },
    "111": {
        "file_id": 9,
        "content": "This code defines two classes, `MouseMove` and `MouseScroll`, which inherit from the `BaseModel` class. Both classes represent different types of mouse actions. The constructor for each class takes arguments representing the x and y coordinates (both expected to be non-negative floats) and a boolean indicating whether a button is pressed or not. The `to_list()` method converts an instance of these classes into a list, while the `from_list()` classmethod recreates an instance from a list representation.",
        "type": "comment"
    },
    "112": {
        "file_id": 9,
        "content": "ge=0)\n    dx: float\n    dy: float\n    def to_list(self) -> List:\n        return [self._action_type, [self.x, self.y, self.dx, self.dy]]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = lst[1]\n        assert len(action_args) == 4\n        assert action_type == cls._action_type\n        assert isinstance(action_args[0], confloat(ge=0))\n        assert isinstance(action_args[1], confloat(ge=0))\n        assert isinstance(action_args[2], float)\n        assert isinstance(action_args[3], float)\n        return cls(\n            x=action_args[0], y=action_args[1], dx=action_args[2], dy=action_args[3]\n        )\n#################\n# VIDEO CONTEXT #\n#################\nclass VideoCaptureContextManager:\n    def __init__(self, videoPath):\n        self.videoPath = videoPath\n    def __enter__(self):\n        logger.info(\"Entering the context...\")\n        self.cap = cv2.VideoCapture(self.videoPath)\n        return self.cap\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        try",
        "type": "code",
        "location": "/conscious_struct.py:687-726"
    },
    "113": {
        "file_id": 9,
        "content": "This code defines a class for storing x and y coordinates, along with dx and dy values. It has methods to convert the object into a list of its attributes and vice versa. Additionally, it includes assertions to ensure that the input list conforms to certain data types and conditions. The code also includes the definition of a VideoCaptureContextManager class for working with video capture contexts using OpenCV's VideoCapture API.",
        "type": "comment"
    },
    "114": {
        "file_id": 9,
        "content": ":\n            self.cap.release()\n        finally:\n            import gc\n            gc.collect()\n            logger.info(\"Leaving the context...\")\n        #  print(exc_type, exc_value, exc_tb, sep=\"\\n\")\n##################\n# CONSCIOUS BASE #\n##################\nclass ConsciousBase:\n    data_types = [\"image\", \"HIDAction\"]\n    special_tokens = [\"image_newline\", \"image_end\", \"action_end\", None]\n    #     vector_size = 1+2+1000+4110 # visual things are pre-encoded. no raw image here!\n    # vector size is \"length\" now\n    image_dim = 224\n    image_channels = 3\n    data_type_length = len(data_types)\n    special_token_length = len(special_tokens)\n    image_length = image_dim * image_dim * image_channels  # obviously flattened.\n    # FIX 1: plus to colon.\n    split_sizes = [\n        len(data_types),\n        len(special_tokens),\n        image_length,  # FIX 9: change to flattened image bits count\n        HIDActionBase.length,  # 4110?\n    ]\n    length = sum(split_sizes)\n    # you cannot easily revert this compression by arg",
        "type": "code",
        "location": "/conscious_struct.py:726-763"
    },
    "115": {
        "file_id": 9,
        "content": "This code defines a class called ConsciousBase, which has variables related to data types, special tokens, and image dimensions for image processing. It also calculates the length of the various components in the consciousness structure.",
        "type": "comment"
    },
    "116": {
        "file_id": 9,
        "content": "max or something else.\n    # so you need the decoder.\n# can it be consciousnessless?\nclass ConsciousBlock(BaseModel, ConsciousBase):\n    data_type: Literal[\"image\", \"HIDAction\"]  # 2 bits, required\n    special_token: Union[\n        Literal[\n            \"image_newline\",\n            \"image_end\",\n            \"action_end\",  # change some of these bits into -torch.inf, so you won't have paradox like results.\n        ],\n        None,\n    ] = None  # 4 bits\n    image_data: Union[\n        None, NDArray\n    ] = None  # what is the shape of this image data? assume to be [3,224,224] (c h w) flattened\n    action_data: Union[None, NDArray] = None  # assume to be: (4110, )\n    # [1,1000] -> [3,1000,1000] -> [3,224,224]\n    #    einsum.repeat       conv2d\n    # so, maybe you still need some ViT decode layer.\n    @staticmethod\n    def from_json(data: Mapping):\n        mConsciousBlock = ConsciousBlock(**data)\n        return mConsciousBlock\n    def to_json(self) -> Mapping:\n        mJson = self.dict()\n        return mJson\n    @st",
        "type": "code",
        "location": "/conscious_struct.py:763-798"
    },
    "117": {
        "file_id": 9,
        "content": "This code defines a class called \"ConsciousBlock\" which is derived from \"BaseModel\" and \"ConsciousBase\". It has attributes for data_type, special_token, image_data, and action_data. The class also includes methods to convert the object to JSON format and create an instance of the class from a dictionary representation of its fields.",
        "type": "comment"
    },
    "118": {
        "file_id": 9,
        "content": "aticmethod\n    def from_tensor(tensor: torch.Tensor):\n        # check its shape.\n        assert tensor.shape == (ConsciousBlock.length,)\n        split_sizes = ConsciousBase.split_sizes\n        data_bits, special_bits, image_data, action_data = einops.unpack(\n            tensor, [[s] for s in split_sizes], \"*\"\n        )\n        #         data_bits, special_bits, image_data, action_data = size_splits(tensor, split_sizes)\n        data_type = ConsciousBase.data_types[int(torch.argmax(data_bits))]\n        if data_type == \"image\":\n            special_bits[2] = -torch.inf\n            special_token = ConsciousBase.special_tokens[\n                int(torch.argmax(special_bits))\n            ]\n            mConsciousBlock = ConsciousBlock(\n                data_type=data_type, special_token=special_token, image_data=image_data\n            )\n        elif data_type == \"HIDAction\":\n            special_bits[0:2] = -torch.inf\n            special_token = ConsciousBase.special_tokens[\n                int(torch.argmax(special_",
        "type": "code",
        "location": "/conscious_struct.py:798-821"
    },
    "119": {
        "file_id": 9,
        "content": "Converts a tensor into a ConsciousBlock instance by splitting based on provided sizes and assigning appropriate data types.",
        "type": "comment"
    },
    "120": {
        "file_id": 9,
        "content": "bits))\n            ]\n            mConsciousBlock = ConsciousBlock(\n                data_type=data_type,\n                special_token=special_token,\n                action_data=action_data,\n            )\n        else:\n            raise Exception(\"Unknown data_type:\", data_type)\n        return mConsciousBlock\n    def to_tensor(self) -> torch.Tensor:\n        mTensorBase = torch.zeros((ConsciousBase.length))\n        # BUG 1: not enough values to unpack\n        #         print(ConsciousBase.length)\n        #         print(ConsciousBase.split_sizes)\n        data_bits, special_bits, image_data, action_data = einops.unpack(\n            mTensorBase, [[s] for s in ConsciousBase.split_sizes], \"*\"\n        )\n        #         data_bits, special_bits, image_data, action_data = size_splits(mTensorBase, ConsciousBase.split_sizes)\n        data_bits[ConsciousBase.data_types.index(self.data_type)] = 1\n        if self.data_type == \"image\":\n            # BUG 2: comparing ndarray to None\n            # FIX 2: change \"!=\" into \"i",
        "type": "code",
        "location": "/conscious_struct.py:821-845"
    },
    "121": {
        "file_id": 9,
        "content": "The code defines a class called ConsciousBlock which has a constructor and a method to convert the object into a Tensor. The constructor takes in data_type, special_token, and action_data as parameters. If an unknown data_type is provided, it raises an exception. The to_tensor() method converts the ConsciousBlock object into a tensor using Einops library's unpack function, then sets the bit corresponding to the data_type of the ConsciousBlock to 1. There are two bugs in this code: BUG 1 - not enough values to unpack; BUG 2 - comparing ndarray to None.",
        "type": "comment"
    },
    "122": {
        "file_id": 9,
        "content": "s not\"\n            assert self.image_data is not None\n            logger.debug(\"Image data shape: %s\", self.image_data.shape)\n            logger.debug(\n                \"Expected data shape: %s\",\n                expected_data_shape := (ConsciousBase.image_length,),\n            )\n            assert self.image_data.shape == expected_data_shape\n            assert self.special_token != \"action_end\"\n            image_data = torch.Tensor(self.image_data)\n            special_bits[ConsciousBase.special_tokens.index(self.special_token)] = 1\n        elif self.data_type == \"HIDAction\":\n            assert self.action_data is not None\n            if len(self.action_data.shape) > 1:\n                self.action_data = self.action_data.reshape((-1,))\n            logger.debug(\"Action data shape: %s\", self.action_data.shape)\n            # BUG: actual: (4110, 1)\n            # shall we reshape this.\n            logger.debug(\n                \"Expected data shape: %s\",\n                expected_data_shape := (HIDActionBase.lengt",
        "type": "code",
        "location": "/conscious_struct.py:845-867"
    },
    "123": {
        "file_id": 9,
        "content": "Ensures image data and action data are not None, checks shapes, and converts to torch.Tensor.",
        "type": "comment"
    },
    "124": {
        "file_id": 9,
        "content": "h,),\n            )  # TODO: expected shape is (4110, )? how to make this typed?\n            assert self.action_data.shape == expected_data_shape\n            assert self.special_token not in [\"image_newline\", \"image_end\"]\n            action_data = torch.Tensor(self.action_data)\n            special_bits[ConsciousBase.special_tokens.index(self.special_token)] = 1\n        else:\n            # FIX: found by pyright (UndefinedVariable)\n            raise Exception(\"Unknown data_type:\", self.data_type)\n        mTensor, _ = einops.pack(\n            (data_bits, special_bits, image_data, action_data), \"*\"\n        )\n        #         mTensor = torch.concat((data_bits, special_bits, image_data, action_data))\n        del mTensorBase\n        return mTensor\nclass ConsciousFlow(BaseModel, ConsciousBase):\n    consciousBlocks: List[ConsciousBlock]\n    @staticmethod\n    def from_json(data: List[Mapping]):\n        mList = [ConsciousBlock.from_json(j) for j in data]\n        mConsciousFlow = ConsciousFlow(consciousBlocks=mList)\n ",
        "type": "code",
        "location": "/conscious_struct.py:867-893"
    },
    "125": {
        "file_id": 9,
        "content": "Creates a tensor from various data types, checking their shapes and handling special tokens.",
        "type": "comment"
    },
    "126": {
        "file_id": 9,
        "content": "       return mConsciousFlow\n    def to_json(self) -> List[Mapping]:\n        mJson = [c.to_json() for c in self.consciousBlocks]\n        return mJson\n    @staticmethod\n    def from_tensor(tensor: torch.Tensor):\n        consciousBlockCount, vector_length = tensor.shape\n        assert vector_length == ConsciousBase.length\n        mConsciousBlocks = []\n        for i in range(consciousBlockCount):\n            arr = tensor[i, :]  # dimension reduction.\n            mConsciousBlock = ConsciousBlock.from_tensor(arr)\n            mConsciousBlocks.append(mConsciousBlock)\n        mConsciousFlow = ConsciousFlow(consciousBlocks=mConsciousBlocks)\n        return mConsciousFlow\n    def to_tensor(self) -> torch.Tensor:\n        mTensor, _ = einops.pack([c.to_tensor() for c in self.consciousBlocks], \"* d\")\n        return mTensor\nclass ConsciousStream(BaseModel, ConsciousBase):\n    consciousFlows: List[ConsciousFlow]\n    @staticmethod\n    def from_json(data: List[Mapping]):\n        mList = [ConsciousFlow.from_json(j) for j in d",
        "type": "code",
        "location": "/conscious_struct.py:893-921"
    },
    "127": {
        "file_id": 9,
        "content": "This code defines classes for representing and manipulating conscious data in the form of flows and streams. The ConsciousFlow class has methods to convert between JSON and tensors, while the ConsciousStream class represents a list of such flows with methods to convert from JSON as well.",
        "type": "comment"
    },
    "128": {
        "file_id": 9,
        "content": "ata]\n        mConsciousStream = ConsciousStream(consciousFlows=mList)\n        return mConsciousStream\n    def to_json(self) -> List[Mapping]:\n        mJson = [c.to_json() for c in self.consciousFlows]\n        return mJson\n    @staticmethod\n    def from_tensor(tensor: torch.Tensor):\n        consciousFlowCount, _, vector_length = tensor.shape\n        assert vector_length == ConsciousBase.length\n        mConsciousFlows = []\n        for i in range(consciousFlowCount):\n            arr = tensor[i, :, :]  # dimension reduction.\n            mConsciousFlow = ConsciousFlow.from_tensor(arr)\n            mConsciousFlows.append(mConsciousFlow)\n        mConsciousStream = ConsciousStream(consciousFlows=mConsciousFlows)\n        return mConsciousStream\n    def to_tensor(self) -> torch.Tensor:\n        mTensor, _ = einops.pack([c.to_tensor() for c in self.consciousFlows], \"* s d\")\n        return mTensor\n#####################\n# TRAINER & DATASET #\n#####################\nclass Trainer:\n    def __init__(self, model, loss_fn, optimiz",
        "type": "code",
        "location": "/conscious_struct.py:921-952"
    },
    "129": {
        "file_id": 9,
        "content": "This code defines a class called ConsciousStream which represents a stream of conscious flows. It has methods for creating an instance from a list of conscious flows, converting the stream to JSON format, converting it to a tensor, and converting individual conscious flows to tensors. The class also mentions Trainer and Dataset but they are not defined in this code snippet.",
        "type": "comment"
    },
    "130": {
        "file_id": 9,
        "content": "er):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer  # shall be registered to model parameters.\n    def step(self, batched_input, batched_output=None):\n        # BUG 8: model forward keyword error\n        # FIX 8: fixing keyword, adding default keyword argument\n        model_output = self.model.forward(batched_input, target_output=batched_output)\n        loss = self.loss_fn(model_output, batched_output)\n        logger.debug(\"LOSS? %s\", loss)\n        # this loss is incorrect. shall use some argmax stuff.\n        # to ensure that this thing is the thing that we want.\n        loss.backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\nfrom typing import Protocol\nclass Enqueue(Protocol):\n    def enqueue(self, data, /):\n        ...\n    def clear(self):\n        ...\nclass TestEnqueue(Enqueue):\n    def __init__(self):\n        ...\n        # self.queue = []\n    def enqueue(self, data):\n        logger.debug(\"DATA QUEUE: %s\", data)\n        # may you print th",
        "type": "code",
        "location": "/conscious_struct.py:952-989"
    },
    "131": {
        "file_id": 9,
        "content": "This code defines a class that represents a conscious structure. It initializes the model, loss function, and optimizer in its constructor. The step method performs forward pass on the input data using the model's forward method, calculates the loss, and applies backpropagation to update the model's parameters. This conscious structure also includes an Enqueue class that can enqueue data and clear the queue.",
        "type": "comment"
    },
    "132": {
        "file_id": 9,
        "content": "e data shape.\n        if data.action_data is not None:\n            logger.debug(\"ACTION DATA SHAPE: %s\", data.action_data.shape)\n        elif data.image_data is not None:\n            logger.debug(\"IMAGE DATA SHAPE: %s\", data.image_data.shape)\n        logger.debug(\"\")\n    def clear(self):\n        ...\nclass SequentialTrainingQueue:\n    def __init__(self, context_length: int, batch_size: int, trainer: Trainer):\n        self.context_length = context_length\n        self.batch_size = batch_size\n        self.trainer = trainer\n        self.max_critical_length = self.context_length + self.batch_size\n        self.min_critical_length = self.context_length + 1\n        self.consciousVectors = []\n    def enqueue(self, consciousBlock: ConsciousBlock, clear: bool = False):\n        # change that into some tensor first.\n        # BUG 3: consciousBlock has tensor output but not numpy ndarray\n        # FIX 3: find and replace all \"consciousBlock.to_nparray()\" with \"consciousBlock.to_tensor()\"\n        #         print(consciou",
        "type": "code",
        "location": "/conscious_struct.py:989-1017"
    },
    "133": {
        "file_id": 9,
        "content": "Checking data shape for action_data and image_data.\nCreating a SequentialTrainingQueue object with context_length, batch_size, and trainer.\nEnqueuing consciousBlock into the queue, optionally clearing it after enqueueing.",
        "type": "comment"
    },
    "134": {
        "file_id": 9,
        "content": "sBlock)\n        #         print(type(consciousBlock))\n        consciousVector = consciousBlock.to_tensor()\n        logger.debug(\n            \"CONSCIOUS VECTOR [TYPE: %s SHAPE: %s]\",\n            type(consciousVector),\n            consciousVector.shape,\n        )\n        self.consciousVectors.append(consciousVector)\n        if not clear:\n            # BUG 5: no \"max_critical_point\"\n            # FIX 5: replace it with \"max_critical_length\"\n            if len(self.consciousVectors) == self.max_critical_length:\n                self.train()\n        else:\n            self.clear()\n    def train(self, clear: bool = False):\n        # TODO: use `ConsciousFlow` to replace logic here.\n        # train the model and clear the queue.\n        # size of queue before: self.context_length+self.batch_size (should be? at least geq to self.length+1)\n        # size of queue after training: self.context_length\n        if len(self.consciousVectors) >= self.min_critical_length:\n            batch_size = len(self.consciousVectors) - s",
        "type": "code",
        "location": "/conscious_struct.py:1017-1042"
    },
    "135": {
        "file_id": 9,
        "content": "This function is appending conscious vectors to a queue, and if the maximum critical length is reached, it trains the model. If \"clear\" is True, it clears the queue instead of training.",
        "type": "comment"
    },
    "136": {
        "file_id": 9,
        "content": "elf.context_length\n            # BUG 6: missing self.\n            # FIX 6: adding self.\n            # BUG 7: torch.Tensor conversion error\n            # FIX 7: change \"torch.Tensor([])\" into einops.pack\n            #             print(self.consciousVectors)\n            batched_input = ConsciousStream(\n                consciousFlows=[\n                    ConsciousFlow(\n                        consciousBlocks=self.consciousVectors[\n                            i : i + self.context_length\n                        ]\n                    )\n                    for i in range(batch_size)\n                ]\n            ).to_tensor()\n            batched_output = ConsciousFlow(\n                consciousBlocks=[\n                    self.consciousVectors[self.context_length + i]\n                    for i in range(batch_size)\n                ]\n            )\n            self.trainer.step(batched_input, batched_output)\n            if not clear:\n                # now remove some elements.\n                self.consciousVectors = s",
        "type": "code",
        "location": "/conscious_struct.py:1042-1073"
    },
    "137": {
        "file_id": 9,
        "content": "The code segment is part of a class that appears to be working with conscious vectors and flows. It initializes batched_input and batched_output by slicing self.consciousVectors into chunks based on the context length. These chunks are then passed to the trainer's step method for further processing. If clear is not set, it then removes some elements from self.consciousVectors.",
        "type": "comment"
    },
    "138": {
        "file_id": 9,
        "content": "elf.consciousVectors[-self.context_length :]\n            else:\n                self.consciousVectors = []\n    def clear(self):\n        # check if anything left in queue. call at the end of queue.\n        self.train(clear=True)\n###############\n# IMAGE UTILS #\n###############\ndef resizeImage(im, desired_size):\n    # im = cv2.imread(im_pth)\n    old_size = im.shape[:2]  # old_size is in (height, width) format\n    ratio = float(desired_size) / max(old_size)\n    new_size = tuple([int(x * ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = desired_size - new_size[1]\n    delta_h = desired_size - new_size[0]\n    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n    left, right = delta_w // 2, delta_w - (delta_w // 2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(\n        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n    )\n    return new_im\nclass SequentialEvalQueue:\n    # loop: init data (visual+hid ac",
        "type": "code",
        "location": "/conscious_struct.py:1073-1111"
    },
    "139": {
        "file_id": 9,
        "content": "The code defines a class `SequentialEvalQueue` that handles evaluation tasks sequentially, has methods to add data (images and hidden activations), clear the queue, and resize images. It also includes a utility function `resizeImage()` to resize input images to desired size by preserving aspect ratio.",
        "type": "comment"
    },
    "140": {
        "file_id": 9,
        "content": "tions) -> predict till end or limit -> update visual and action data\n    # visual data are inserted in a regular basis.\n    # human action data/bot action data will be inserted in between visual data.\n    # limit max machine predict token count per interval to 5. forcing it to end anyway.\n    # what about the machine trying to spit out some visual prediction?\n    # we just shadow it. (do not act! just dream. compare to current visual ground truth and perform gradient descent. maybe you can use that as continuous training basis? or redesign the model so it can choose to skip (by retrievable positional encoding) some blocks of visual data and perform descent only on selected area?)\n    ...\n################\n# READING DATA #\n################\ndesired_size = 224 * 4\n# get perspective width/height with:\n# pyautogui.size()\n# must be on the same machine.\n# with VideoCaptureContextManager(videoPath) as cap:\nfrom recording_train_parse import getTrainingData\nimport json\nimport re\nimport parse\nimport random\n# this pro",
        "type": "code",
        "location": "/conscious_struct.py:1111-1137"
    },
    "141": {
        "file_id": 9,
        "content": "This code appears to be part of a larger program that involves image processing and model training. It seems to handle visual data, human/bot action data, and machine predictions. The code is designed to limit the maximum number of tokens for machine predictions per interval to 5, forcing it to end if necessary. It also suggests the possibility of using this as a continuous training basis or redesigning the model for skipping visual data blocks and performing descent only on selected areas.",
        "type": "comment"
    },
    "142": {
        "file_id": 9,
        "content": "cess is actually training it.\ndef trainModelWithDataBasePath(\n    basePath: str,\n    sequentialTrainingQueue: Enqueue,\n    shuffle_for_test: bool = False,\n    random_seed: int = 43  # pass hypothesis param here!\n    # sequentialTrainingQueue: SequentialTrainingQueue\n):\n    if shuffle_for_test:\n        random.seed(random_seed)\n    # read perspective width & height from basepath.\n    fpath = os.path.join(basePath, \"video_record_script.sh\")\n    with open(fpath, \"r\") as f:\n        data = f.read()\n        # data = json.load(f)\n        parse_target = re.finditer(r\"\\b\\d+x\\d+\\b\", data).__next__().group()\n        parsed_data = parse.parse(\n            \"{perspective_width:d}x{perspective_height:d}\", parse_target\n        )\n        if parsed_data:\n            perspective_width, perspective_height = (\n                parsed_data[\"perspective_width\"],\n                parsed_data[\"perspective_height\"],\n            )\n        else:\n            raise Exception(f\"Cannot parse perspective size from file: {fpath}\")\n    trainingData",
        "type": "code",
        "location": "/conscious_struct.py:1137-1163"
    },
    "143": {
        "file_id": 9,
        "content": "This function trains a model using data from a database path, with optional shuffling for testing. It reads perspective width and height from the file \"video_record_script.sh\" in the basePath. If shuffle_for_test is true, it sets a random seed for shuffling. It then parses perspective size information from the data read from the file and returns trainingData.",
        "type": "comment"
    },
    "144": {
        "file_id": 9,
        "content": "Generator = getTrainingData(basePath)\n    if shuffle_for_test:\n        myGenerator = list(trainingDataGenerator)\n        random.shuffle(myGenerator)\n    for trainingDataFrame in trainingDataGenerator:\n        if trainingDataFrame.datatype == \"hid\":\n            encoded_actions = []\n            actions = trainingDataFrame.data[\"HIDEvents\"]\n            if shuffle_for_test:\n                random.shuffle(actions)\n            for action in actions:\n                action_type, action_args = action\n                if action_type in HIDActionBase.keyboard_action_types:\n                    action_args = HIDActionBase.uncover_keycode(action_args)\n                    if action_args is None:\n                        logger.warning(\"Skipping: %s\", action)\n                        continue\n                mHIDAction = HIDAction.from_action_json(\n                    [action_type, action_args],\n                    max_x=perspective_width,\n                    max_y=perspective_height,\n                )  # related to mouse c",
        "type": "code",
        "location": "/conscious_struct.py:1163-1186"
    },
    "145": {
        "file_id": 9,
        "content": "Creates a generator for training data and shuffles if necessary. Then, iterates through the generated frames, and if it's a hidden action type, shuffles actions within that frame, decodes keyboard actions if applicable, and creates an instance of HIDAction class.",
        "type": "comment"
    },
    "146": {
        "file_id": 9,
        "content": "oordinates.\n                mHIDActionNDArray = mHIDAction.to_ndarray()\n                logger.debug(\"HID NDArray shape: %s\", mHIDActionNDArray.shape)\n                encoded_actions.append(mHIDActionNDArray)\n            for index, EA in enumerate(encoded_actions):\n                st = None\n                if index + 1 == len(encoded_actions):\n                    st = \"action_end\"\n                consciousBlock = ConsciousBlock(\n                    data_type=\"HIDAction\", special_token=st, action_data=EA\n                )\n                sequentialTrainingQueue.enqueue(consciousBlock)\n        elif trainingDataFrame.datatype == \"image\":\n            image = trainingDataFrame.data\n            image_resized = resizeImage(image, desired_size)\n            image_reshaped = einops.rearrange(image_resized, \"h w c -> c h w\")\n            #             image_reshaped = np.rollaxis(image_resized, 2, 0) # (3, 896, 896)\n            image_sliced = [\n                image_reshaped[:, x * 224 : (x + 1) * 224, y * 224 : (y",
        "type": "code",
        "location": "/conscious_struct.py:1186-1206"
    },
    "147": {
        "file_id": 9,
        "content": "Creating conscious blocks from HID actions and image data.\nThe code is creating 'consciousBlocks' for both HID action and image data, if the datatype of trainingDataFrame is either \"HIDAction\" or \"image\". It converts the HID action to a numpy array, checks if it's the last encoded action in the list, creates the ConsciousBlock with the appropriate special_token, appends the encoded action to the sequentialTrainingQueue. For image data, resizes and rearranges the image, slices the image into multiple blocks of size 224x224 for further processing.",
        "type": "comment"
    },
    "148": {
        "file_id": 9,
        "content": " + 1) * 224]\n                for x in range(4)\n                for y in range(4)\n            ]  # ) # c h w\n            # IMAGE RESHAPED: (896, 3, 896)?\n            # IMAGE RESHAPED: (896, 896, 3)\n            #             print('IMAGE RESHAPED:', image_reshaped.shape)\n            #             print('IMAGE SLICED:', image_sliced.shape)\n            #     (16, 3, 224, 224)\n            # hell?\n            consciousBlocks = []\n            for index, im in enumerate(image_sliced):\n                im = einops.rearrange(im, \"c h w -> (c h w)\")\n                st = None\n                if index == 15:\n                    st = \"image_end\"\n                elif index != 0 and (index + 1) % 4 == 0:\n                    st = \"image_newline\"\n                # BUG 4: tuple\n                # FIX 4: remove .to_tensor() method call\n                consciousBlock = ConsciousBlock(\n                    data_type=\"image\", special_token=st, image_data=im\n                )\n                if shuffle_for_test:\n                    con",
        "type": "code",
        "location": "/conscious_struct.py:1206-1232"
    },
    "149": {
        "file_id": 9,
        "content": "Reshaping image, slicing into 16 chunks, creating ConsciousBlock objects with image data and special tokens.",
        "type": "comment"
    },
    "150": {
        "file_id": 9,
        "content": "sciousBlocks.append(consciousBlock)\n                else:\n                    #                 print(consciousBlock)\n                    sequentialTrainingQueue.enqueue(consciousBlock)\n            #             last_output = torch.zeros(1, output_size)\n            if shuffle_for_test:\n                for consciousBlock in consciousBlocks:\n                    sequentialTrainingQueue.enqueue(consciousBlock)\n            del image\n            del image_reshaped\n            del image_resized\n        else:\n            assert False, f\"wrong datatype: {trainingDataFrame.datatype}\"\n    sequentialTrainingQueue.clear()\n#########################################\n#  CONSISTENCY WITH RECORDER AND ACTOR  #\n#########################################\n# import ctypes\n# PROCESS_PER_MONITOR_DPI_AWARE = 2\n# ctypes.windll.shcore.SetProcessDpiAwareness(PROCESS_PER_MONITOR_DPI_AWARE)\n#########\n# MODEL #\n#########\n# notice: when in online mode only image will be backpropagated.\n# like using some upside down mirror.\nclass CustomModel(to",
        "type": "code",
        "location": "/conscious_struct.py:1232-1266"
    },
    "151": {
        "file_id": 9,
        "content": "Appending consciousBlock to consciousBlocks list if not else enqueueing it to sequentialTrainingQueue.\nChecks if the datatype is correct and clears sequentialTrainingQueue if shuffle_for_test is True.\nEnsures consistency with recorder and actor by setting process DPI awareness.\nDefines a class CustomModel for the model.",
        "type": "comment"
    },
    "152": {
        "file_id": 9,
        "content": "rch.nn.Module):\n    def __init__(self, vit_model, hidden_size_vit=1000, vit_block_size=228):\n        #     def __init__(self, rwkv_model, vit_model, tokenizer, hidden_size_rwkv, hidden_size_vit, output_size, vit_times = 4, vit_block_size=228):\n        super(CustomModel, self).__init__()\n        #         self.rwkv_model = rwkv_model # processing language, generate actions.\n        self.vit_model = vit_model\n        self.vit_block_size = vit_block_size  # this is default.\n        #         self.vit_times = vit_times\n        # seq2seq alike.\n        #         self.hidden_size = hidden_size_rwkv+hidden_size_vit\n        self.hidden_size = hidden_size_vit\n        self.HIDEncoder = torch.nn.Linear(HIDActionBase.length, 1000)\n        self.HIDDecoder = torch.nn.Linear(\n            1000, HIDActionBase.length\n        )  # use torch.where or something\n        # sparse matrix?\n        # some magic going elsewhere.\n        self.ViTDecoder = torch.nn.Conv2d(\n            in_channels=9, out_channels=3, kernel_size=22, ",
        "type": "code",
        "location": "/conscious_struct.py:1266-1287"
    },
    "153": {
        "file_id": 9,
        "content": "The code defines a class called `CustomModel` that inherits from `torch.nn.Module`. It has an initializer method that takes `vit_model`, `hidden_size_vit`, and `vit_block_size` as input parameters. The class also includes `HIDEncoder` and `HIDDecoder` linear layers, and a `ViTDecoder` convolutional layer with specific in_channels, out_channels, and kernel_size values.",
        "type": "comment"
    },
    "154": {
        "file_id": 9,
        "content": "stride=4, dilation=5\n        )  # n c h w\n        # FIX 13: change 1000 to ConsciousBase.data_type_length+ConsciousBase.special_token_length+1000\n        io_h_size = (\n            ConsciousBase.data_type_length + ConsciousBase.special_token_length + 1000\n        )\n        self.rnn = torch.nn.LSTM(\n            input_size=io_h_size, hidden_size=io_h_size, batch_first=True\n        )\n        # use tensor.\n    def forward(\n        self,\n        conscious_stream: torch.Tensor,\n        target_output: Union[None, torch.Tensor] = None,\n    ):\n        # if have target, and our datatype bits are wrong, we only optimize the datatype bits, making image/action bits identical\n        # otherwise just calculate all bits from the model.\n        # conscious_stream: [batch_size, (ConsciousBase.length) data_type+special_tokens+image_bits+action_bits]\n        # you need another pydantic model for it.\n        # BUG 9: input shape error\n        # FIX 9: check input and output shape\n        #         print(conscious_stream.shape,",
        "type": "code",
        "location": "/conscious_struct.py:1287-1313"
    },
    "155": {
        "file_id": 9,
        "content": "This code defines a class that contains an LSTM (Long Short-Term Memory) network. The LSTM network takes in a tensor of data called \"conscious_stream\" which has a specific shape and type defined in the ConsciousBase class. If a target output is provided, the network will only optimize the data type bits, making image/action bits identical. Otherwise, it calculates all bits from the model. There are also fixes for Bug 9 and Fix 13 mentioned in comments.",
        "type": "comment"
    },
    "156": {
        "file_id": 9,
        "content": " ConsciousBase.length, )\n        #         if target_output is not None:\n        #             print(target_output.shape)\n        batch_size, seq_length, dim_block = conscious_stream.shape\n        assert dim_block == ConsciousBase.length\n        conscious_stream_reshaped = einops.rearrange(\n            conscious_stream, \"b s d -> (b s) d\"\n        )\n        datatype_bits, special_bits, image_bits, action_bits = einops.unpack(\n            conscious_stream_reshaped, [[s] for s in ConsciousBase.split_sizes], \"b *\"\n        )\n        #         datatype_bits, special_bits, image_bits, action_bits = size_splits(conscious_stream_reshaped, ConsciousBase.split_sizes,dim=1)\n        #         desired_size = self.vit_block_size*self.vit_times\n        datatypes = torch.argmax(datatype_bits, dim=1)\n        image_indexs = datatypes == 0\n        action_indexs = datatypes == 1\n        special_tokens = torch.argmax(special_bits, dim=1)\n        image_newline_indexs = special_tokens == 0\n        image_end_indexs = special_to",
        "type": "code",
        "location": "/conscious_struct.py:1313-1335"
    },
    "157": {
        "file_id": 9,
        "content": "This code appears to be part of a neural network model that is processing data and extracting information from it. It's reshaping the input data, unpacking it into different components, and then performing actions on these different components based on their indexes. The purpose of this specific section may be to identify specific types of data within the input, such as images, special tokens, and actions.",
        "type": "comment"
    },
    "158": {
        "file_id": 9,
        "content": "kens == 1\n        action_end_indexs = special_tokens == 2\n        nop_indexs = special_tokens == 3\n        # 4+2+1000\n        # you are gonna take actions.\n        # prepare some zeros.\n        # BUG 10: len() on int\n        # FIX 10: find and fix incorrect len() calls\n        batched_rnn_input = torch.zeros(\n            (\n                batch_size * seq_length,\n                ConsciousBase.data_type_length\n                + ConsciousBase.special_token_length\n                + 1000,\n            )\n        )\n        batched_rnn_input[image_indexs, 0] = 1\n        batched_rnn_input[action_indexs, 1] = 1\n        # BUG 11: indexs not defined\n        # FIX 11: indexes -> indexs\n        batched_rnn_input[image_newline_indexs, 2] = 1\n        batched_rnn_input[image_end_indexs, 3] = 1\n        batched_rnn_input[action_end_indexs, 4] = 1\n        batched_rnn_input[nop_indexs, 5] = 1\n        # process this.\n        datatype_and_special_token_length = (\n            ConsciousBase.data_type_length + ConsciousBase.special_toke",
        "type": "code",
        "location": "/conscious_struct.py:1335-1367"
    },
    "159": {
        "file_id": 9,
        "content": "Code creates a tensor with initial values of zeros for batch_size x seq_length x (data_type_length + special_token_length + 1000). It then sets certain elements to 1 based on image_indexs, action_indexs, etc.",
        "type": "comment"
    },
    "160": {
        "file_id": 9,
        "content": "n_length\n        )\n        # BUG 12: unannotated unknown axes in einops.rearrange\n        # FIX 12: annotate these axes\n        selected_image_bits = image_bits[image_indexs, :]\n        transformed_image_bits = einops.rearrange(\n            selected_image_bits,\n            \"b (c h w) -> b c h w\",\n            h=ConsciousBase.image_dim,\n            w=ConsciousBase.image_dim,\n            c=ConsciousBase.image_channels,\n        )\n        processed_image_bits = self.vit_model(transformed_image_bits)\n        batched_rnn_input[\n            image_indexs, datatype_and_special_token_length:\n        ] = processed_image_bits\n        selected_action_bits = action_bits[action_indexs, :]\n        processed_action_bits = self.HIDEncoder(selected_action_bits)\n        batched_rnn_input[\n            action_indexs, datatype_and_special_token_length:\n        ] = processed_action_bits\n        batched_rnn_input_reshaped = einops.rearrange(\n            batched_rnn_input, \"(b s) d -> b s d\", b=batch_size, s=seq_length\n        )\n      ",
        "type": "code",
        "location": "/conscious_struct.py:1367-1395"
    },
    "161": {
        "file_id": 9,
        "content": "This code is preparing the input data for a Recurrent Neural Network (RNN) by selecting image and action bits, transforming them, and then reshaping the batched input. The code annotates unknown axes to fix bug 12, selects image and action bits based on indexes, processes the selected bits using a Vision Transformer model and HID Encoder respectively, and finally reshapes the batched RNN input for further processing.",
        "type": "comment"
    },
    "162": {
        "file_id": 9,
        "content": "  # BUG 13: mismatched shape for rnn\n        _, (h1, c1) = self.rnn(batched_rnn_input_reshaped)\n        # shape of h1: [batch_size, dim_block]\n        if target_output is not None:\n            (\n                target_datatype_bits,\n                target_special_bits,\n                target_image_bits,\n                target_action_bits,\n            ) = einops.unpack(\n                target_output, [[s] for s in ConsciousBase.split_sizes], \"b *\"\n            )\n        output_datatype_bits, output_special_bits, output_data_bits = einops.unpack(\n            einops.rearrange(h1, \"b s d -> (b s) d\"),\n            [\n                [s]\n                for s in [\n                    ConsciousBase.data_type_length,\n                    ConsciousBase.special_token_length,\n                    1000,\n                ]\n            ],\n            \"b *\",\n        )\n        #         output_datatype_bits, output_special_bits, output_data_bits = size_splits(h1, [ConsciousBase.data_type_length, ConsciousBase.special_token_length",
        "type": "code",
        "location": "/conscious_struct.py:1395-1422"
    },
    "163": {
        "file_id": 9,
        "content": "Unpacks RNN output, reshapes and assigns values to specific variables for data types, special tokens, and other data.",
        "type": "comment"
    },
    "164": {
        "file_id": 9,
        "content": ", 1000] ,dim=1)\n        output_datatypes = torch.argmax(output_datatype_bits, dim=1)\n        output_image_indexs = output_datatypes == 0\n        output_action_indexs = output_datatypes == 1\n        if target_output is not None:\n            target_output_datatypes = torch.argmax(target_datatype_bits, dim=1)\n            target_output_image_indexs = target_output_datatypes == 0\n            target_output_action_indexs = target_output_datatypes == 1\n            common_output_image_indexs = torch.logical_and(\n                target_output_image_indexs, output_image_indexs\n            )\n            common_output_action_indexs = torch.logical_and(\n                target_output_action_indexs, output_action_indexs\n            )\n            common_output_indexs = torch.logical_or(\n                common_output_image_indexs, common_output_action_indexs\n            )\n            target_exclusive_output_image_indexs = torch.logical_and(\n                target_output_image_indexs, torch.logical_not(output_image_indexs",
        "type": "code",
        "location": "/conscious_struct.py:1422-1446"
    },
    "165": {
        "file_id": 9,
        "content": "This code calculates the common output indexes between target and actual outputs for image and action indexes.",
        "type": "comment"
    },
    "166": {
        "file_id": 9,
        "content": ")\n            )\n            target_exclusive_output_action_indexs = torch.logical_and(\n                target_output_action_indexs, torch.logical_not(output_action_indexs)\n            )\n            target_exclusive_output_indexs = torch.logical_or(\n                target_exclusive_output_image_indexs,\n                target_exclusive_output_action_indexs,\n            )\n        if target_output is not None:\n            selected_output_image_bits = output_data_bits[common_output_image_indexs, :]\n        else:\n            selected_output_image_bits = output_data_bits[output_image_indexs, :]\n        processed_output_image_bits = einops.repeat(\n            selected_output_image_bits, \"b d -> b d 9\"\n        )\n        processed_output_image_bits = einops.einsum(\n            processed_output_image_bits,\n            processed_output_image_bits,\n            \"b h c, b w c -> b c h w\",\n        )\n        processed_output_image_bits = self.ViTDecoder(processed_output_image_bits)\n        processed_output_image_bits = eino",
        "type": "code",
        "location": "/conscious_struct.py:1446-1470"
    },
    "167": {
        "file_id": 9,
        "content": "This code is part of a larger neural network. It checks if the target output is not None, and based on that, selects either common or specific output image indexes to process. Then it repeats and rearranges the selected image bits using Einops library. After this, it passes these processed image bits through a ViTDecoder module and continues processing in the next chunk of code.",
        "type": "comment"
    },
    "168": {
        "file_id": 9,
        "content": "ps.rearrange(\n            processed_output_image_bits, \"b c h w -> b (c h w)\"\n        )\n        if target_output is not None:\n            selected_output_action_bits = output_data_bits[\n                common_output_action_indexs, :\n            ]\n        else:\n            selected_output_action_bits = output_data_bits[output_action_indexs, :]\n        processed_output_action_bits = self.HIDDecoder(selected_output_action_bits)\n        # preparing blank output\n        output_0 = torch.zeros((batch_size, ConsciousBase.length))\n        (\n            output_datatype_bits_0,\n            output_special_token_bits_0,\n            output_image_bits_0,\n            output_action_bits_0,\n        ) = einops.unpack(output_0, [[s] for s in ConsciousBase.split_sizes], \"b *\")\n        #         output_datatype_bits_0, output_special_token_bits_0, output_image_bits_0, output_action_bits_0 = size_splits(output, ConsciousBase.split_sizes, dim=1)\n        if target_output is not None:\n            output_datatype_bits_0[\n          ",
        "type": "code",
        "location": "/conscious_struct.py:1470-1495"
    },
    "169": {
        "file_id": 9,
        "content": "Rearranging processed output image bits and preparing blank output.",
        "type": "comment"
    },
    "170": {
        "file_id": 9,
        "content": "      target_exclusive_output_indexs, :\n            ] = target_datatype_bits[target_exclusive_output_indexs, :]\n            output_datatype_bits_0[common_output_indexs, :] = output_datatype_bits[\n                common_output_indexs, :\n            ]\n            output_special_token_bits_0[\n                target_exclusive_output_indexs, :\n            ] = output_special_bits[target_exclusive_output_indexs, :]\n            output_special_token_bits_0[common_output_indexs, :] = output_special_bits[\n                common_output_indexs, :\n            ]\n            output_special_token_bits_0[common_output_image_indexs, 2] = 0\n            output_special_token_bits_0[common_output_action_indexs, :2] = 0\n            output_image_bits_0[\n                target_exclusive_output_image_indexs, :\n            ] = target_image_bits[target_exclusive_output_image_indexs, :]\n            output_action_bits_0[\n                target_exclusive_output_action_indexs, :\n            ] = target_action_bits[target_exclusive_outpu",
        "type": "code",
        "location": "/conscious_struct.py:1495-1516"
    },
    "171": {
        "file_id": 9,
        "content": "Copying target data to output arrays based on exclusive and common indices.",
        "type": "comment"
    },
    "172": {
        "file_id": 9,
        "content": "t_action_indexs, :]\n            output_image_bits_0[\n                common_output_image_indexs, :\n            ] = processed_output_image_bits\n            output_action_bits_0[\n                common_output_action_indexs, :\n            ] = processed_output_action_bits\n        else:\n            output_datatype_bits_0 = output_datatype_bits\n            output_special_bits_0 = output_special_bits\n            output_special_bits_0[output_image_indexs, 2] = 0\n            output_special_bits_0[output_action_indexs, :2] = 0\n            output_image_bits_0[output_image_indexs, :] = processed_output_image_bits\n            output_action_bits_0[output_action_indexs, :] = processed_output_action_bits\n        output, _ = einops.pack(\n            (\n                output_datatype_bits_0,\n                output_special_token_bits_0,\n                output_image_bits_0,\n                output_action_bits_0,\n            ),\n            \"b *\",\n        )\n        #         output = torch.concat((output_datatype_bits_0, output_sp",
        "type": "code",
        "location": "/conscious_struct.py:1516-1544"
    },
    "173": {
        "file_id": 9,
        "content": "This code is assigning values to different parts of the output based on a condition. If the condition is true, it assigns processed_output_image_bits and processed_output_action_bits to their respective locations in the output arrays. Otherwise, it sets certain bits to 0 and then concatenates output_datatype_bits_0, output_special_token_bits_0, output_image_bits_0, and output_action_bits_0 into the final output using einops.pack.",
        "type": "comment"
    },
    "174": {
        "file_id": 9,
        "content": "ecial_token_bits_0, output_image_bits_0, output_action_bits_0), dim=1)\n        return output",
        "type": "code",
        "location": "/conscious_struct.py:1544-1546"
    },
    "175": {
        "file_id": 9,
        "content": "Performs 1D convolution with given filters and input data, returning the output.",
        "type": "comment"
    },
    "176": {
        "file_id": 10,
        "content": "/decorator_method_registry.py",
        "type": "filepath"
    },
    "177": {
        "file_id": 10,
        "content": "MyDecorator is a function decorator that executes code before and after the decorated function, while MyMeta adds code execution right after the 'def' block of the decorated function. A decorator function \"my_decorator\" is defined which executes code before and after the decorated function by using \"@\" syntax to decorate the function \"my_function\".",
        "type": "summary"
    },
    "178": {
        "file_id": 10,
        "content": "class MyDecorator:\n    def __init__(self, func):\n        self.func = func\n        self.invoke_decorated_function()\n    def invoke_decorated_function(self):\n        # Code to execute after the 'def' block of the decorated function\n        print(\"Executing code right after the 'def' block\")\n        self.func()\n# Decorate a function with the decorator\n@MyDecorator\ndef my_function():\n    print(\"Executing the decorated function\")\nprint(\"-\" * 40)\nclass MyMeta(type):\n    def __new__(cls, name, bases, attrs):\n        # Code to execute right after the 'def' block of the decorated function\n        print(\"Executing code right after the 'def' block\")\n        return super().__new__(cls, name, bases, attrs)\n# Define the metaclass for the decorator\nclass MyDecorator(metaclass=MyMeta):\n    def __call__(self, func):\n        print(\"Executing code before the function is invoked\")\n        def wrapper(*args, **kwargs):\n            # Code to execute before invoking the decorated function\n            # Invoke the decorated function\n",
        "type": "code",
        "location": "/decorator_method_registry.py:1-38"
    },
    "179": {
        "file_id": 10,
        "content": "MyDecorator class is a function decorator that executes code before and after the decorated function.\nMetaclass MyMeta adds code execution right after the 'def' block of the decorated function.",
        "type": "comment"
    },
    "180": {
        "file_id": 10,
        "content": "            return func(*args, **kwargs)\n        return wrapper\n# Decorate a function with the decorator\n@MyDecorator()\ndef my_function():\n    print(\"Executing the decorated function\")\nprint(\"-\" * 20)\ndef my_decorator(func):\n    print(\"Executing code before the function is invoked.\")\n    print(\"Function name:\", func.__name__)\n    def wrapper(*args, **kwargs):\n        # Code to execute before the invocation of the decorated function\n        # Invoke the decorated function\n        return func(*args, **kwargs)\n    # Return the wrapper function\n    return wrapper\n# Decorate a function with the decorator\n@my_decorator\ndef my_function():\n    print(\"Executing the decorated function.\")",
        "type": "code",
        "location": "/decorator_method_registry.py:38-68"
    },
    "181": {
        "file_id": 10,
        "content": "Defines a decorator function \"my_decorator\" that executes code before and after the decorated function. Uses the \"@\" syntax to decorate the function \"my_function\".",
        "type": "comment"
    },
    "182": {
        "file_id": 11,
        "content": "/generate_datamodel_code.sh",
        "type": "filepath"
    },
    "183": {
        "file_id": 11,
        "content": "Generating Python code from a YAML datamodel file, with class name \"MyModel\".",
        "type": "summary"
    },
    "184": {
        "file_id": 11,
        "content": "datamodel-codegen --input mydatamodel.yaml --output mydatamodel.py --class-name MyModel\n# cannot process multiple instances of datamodel.",
        "type": "code",
        "location": "/generate_datamodel_code.sh:1-3"
    },
    "185": {
        "file_id": 11,
        "content": "Generating Python code from a YAML datamodel file, with class name \"MyModel\".",
        "type": "comment"
    },
    "186": {
        "file_id": 12,
        "content": "/hid_utils.py",
        "type": "filepath"
    },
    "187": {
        "file_id": 12,
        "content": "This code defines classes and functions for input operations, specifically handling mouse scroll events and translating key literals to XK_KEYSYMs using abstract methods. It checks the starting key and sorts the XK_KEYSYMs based on distance before storing the translation in a dictionary and writing it to a file.",
        "type": "summary"
    },
    "188": {
        "file_id": 12,
        "content": "# TODO: display grabbing abstract class\n# TODO: for relative control apis, use them as fallback for absolute apis (somehow calibrated, with feedback) when possible\n# TODO: check out ROS package index: https://index.ros.org/packages/\nfrom enum import Enum, auto, Flag\nfrom beartype.vale import Is\nfrom typing_extensions import Annotated, TypeAlias\nfrom conscious_struct import HIDActionTypes, HIDActionBase\nfrom log_utils import logger_print\nfrom typing import Dict, Tuple, Union\nfrom abc import ABC, abstractmethod\n# TODO: use abstract implementation pattern (template)\nclass HIDInterface(ABC):\n    def key_press(self, key_literal: HIDActionTypes.keys):\n        \"\"\"\n        Press one of key literals.\n        \"\"\"\n        return self._key_press(key_literal=key_literal)\n    @abstractmethod\n    def _key_press(self, key_literal: HIDActionTypes.keys):\n        ...\n    def key_release(self, key_literal: HIDActionTypes.keys):\n        \"\"\"\n        Release one of key literals.\n        \"\"\"\n        return self._key_release(key_lite",
        "type": "code",
        "location": "/hid_utils.py:1-30"
    },
    "189": {
        "file_id": 12,
        "content": "Code is importing required modules and defining an abstract class called HIDInterface with methods for key press and release.",
        "type": "comment"
    },
    "190": {
        "file_id": 12,
        "content": "ral=key_literal)\n    @abstractmethod\n    def _key_release(self, key_literal: HIDActionTypes.keys):\n        ...\n    def mouse_move(self, x: Union[int, float], y: Union[int, float]):\n        \"\"\"\n        Move mouse to absolute position (x, y).\n        \"\"\"\n        return self._mouse_move(x=x, y=y)\n    @abstractmethod\n    def _mouse_move(self, x: Union[int, float], y: Union[int, float]):\n        ...\n    def mouse_click(\n        self,\n        x: Union[int, float],\n        y: Union[int, float],\n        button_literal: HIDActionTypes.mouse_buttons,\n        pressed: bool,\n    ):\n        \"\"\"\n        Press or release one of mouse button literals at absolute position (x, y).\n        \"\"\"\n        return self._mouse_click(\n            x=x, y=y, button_literal=button_literal, pressed=pressed\n        )\n    @abstractmethod\n    def _mouse_click(\n        self,\n        x: Union[int, float],\n        y: Union[int, float],\n        button_literal: HIDActionTypes.mouse_buttons,\n        pressed: bool,\n    ):\n        ...\n    def mouse_scroll(\n    ",
        "type": "code",
        "location": "/hid_utils.py:30-71"
    },
    "191": {
        "file_id": 12,
        "content": "This code defines abstract methods for mouse movement, click actions, and key releases in a HID (Human Interface Device) interface. The class must implement these methods to interact with the device. The methods take parameters like position (x, y), button literal, and whether to press or release.",
        "type": "comment"
    },
    "192": {
        "file_id": 12,
        "content": "    self,\n        x: Union[int, float],\n        y: Union[int, float],\n        dx: Union[int, float],\n        dy: Union[int, float],\n    ):\n        \"\"\"\n        Scroll mouse (dx, dy) at absolute position (x, y).\n        \"\"\"\n        return self._mouse_scroll(x=x, y=y, dx=dx, dy=dy)\n    @abstractmethod\n    def _mouse_scroll(\n        self,\n        x: Union[int, float],\n        y: Union[int, float],\n        dx: Union[int, float],\n        dy: Union[int, float],\n    ):\n        ...\ndef length_limit(l):\n    return Is[lambda b: len(b) == l]\n# import Xlib\n# python-xlib\nimport os\nsourcefile_dirname = os.path.dirname(os.path.abspath(__file__))\nkey_literal_to_xk_keysym_translation_table_path = os.path.join(\n    sourcefile_dirname, \"KL2XKS.json\"\n)\nfrom functools import lru_cache\nimport json\n@lru_cache\ndef getKL2XKS() -> Dict[str, str]:\n    with open(key_literal_to_xk_keysym_translation_table_path, \"r\") as f:\n        KL2XKS = json.loads(f.read())\n    return KL2XKS\nfrom beartype import beartype\n@beartype\ndef strip_key_literal(key_literal: ",
        "type": "code",
        "location": "/hid_utils.py:71-121"
    },
    "193": {
        "file_id": 12,
        "content": "The code appears to be part of a larger module and seems to define functions for handling mouse scroll events. It first defines an abstract method for the `_mouse_scroll` function, which takes in x and y coordinates along with dx and dy (amount of scroll) as parameters. The `length_limit` function is defined as a helper function that checks if a byte-like object's length equals to specified length. Additionally, there are imports from Xlib and other modules, but the exact functionality of these is not clear without more context.",
        "type": "comment"
    },
    "194": {
        "file_id": 12,
        "content": "HIDActionTypes.keys) -> Tuple[bool, bool, str]:\n    # defer strip/lstrip.\n    # it is not a bug. do not report.\n    is_special, is_media = False, False\n    keychar = ...\n    if key_literal.startswith(prefix := \"Key.\"):\n        is_special = True\n        keychar = key_literal.replace(prefix, \"\")\n        if keychar.startswith(prefix := \"media_\"):\n            is_media = True\n            keychar = keychar.replace(prefix, \"\")\n    if len(key_literal) == 3:\n        if not (key_literal[0] == key_literal[2] != (keychar := key_literal[1])):\n            raise Exception(f\"Abnormal enclosed keychar: {repr(key_literal)}\")\n    if keychar == Ellipsis:\n        raise Exception(f\"Unable to strip key literal: {repr(key_literal)}\")\n    else:\n        return is_special, is_media, keychar\n@beartype\ndef key_literal_to_xk_keysym(key_literal: HIDActionTypes.keys) -> Union[None, str]:\n    # is_special, is_media, stripped_key_literal = strip_key_literal(key_literal)\n    KL2XKS = getKL2XKS()\n    xk_keysym = KL2XKS.get(key_literal, None)\n ",
        "type": "code",
        "location": "/hid_utils.py:121-146"
    },
    "195": {
        "file_id": 12,
        "content": "Function \"strip_key_literal\" checks if the given key literal is a special or media key and returns its stripped name. If the key literal has an abnormal enclosed keychar, it raises an exception. If the keychar is Ellipsis, it raises an exception as well. Otherwise, it returns a tuple of whether it's a special or media key, the stripped key character if it is a special key with a \"media_\" prefix, and the original key literal without prefixes if not special.",
        "type": "comment"
    },
    "196": {
        "file_id": 12,
        "content": "   if xk_keysym is None:\n        print(f\"skipping translating key literal {repr(key_literal)} to xk_keysym.\")\n    return xk_keysym\n    # Xlib.XK.string_to_keysym(stripped_key_literal)\n    # generate this translation table statically, then we will review.\ndef byte_with_length_limit(l):\n    return Annotated[bytes, length_limit(l)]\none_byte: TypeAlias = byte_with_length_limit(1)\ntwo_bytes: TypeAlias = byte_with_length_limit(2)\nfour_bytes: TypeAlias = byte_with_length_limit(4)\nsix_bytes: TypeAlias = byte_with_length_limit(6)\neight_bytes: TypeAlias = byte_with_length_limit(8)\nnon_neg_int: TypeAlias = Annotated[int, Is[lambda i: i >= 0]]\npos_int: TypeAlias = Annotated[int, Is[lambda i: i > 0]]\nmovement: TypeAlias = Annotated[\n    int, Is[lambda i: i >= -126 and i <= 126]\n]  # this is hardware limit. software might not be limited. (shall we adapt to software limit instead of hardware)\nclass ControlCode(Flag):\n    # @staticmethod\n    # def _generate_next_value_(name, start, count, last_values):\n    #     return 2",
        "type": "code",
        "location": "/hid_utils.py:146-174"
    },
    "197": {
        "file_id": 12,
        "content": "This code defines several type aliases for different byte lengths and some integer ranges. It also has a class called ControlCode which is defined as a Flag. The code seems to be related to handling key literals, XK keysyms, and potentially input validation based on these types.",
        "type": "comment"
    },
    "198": {
        "file_id": 12,
        "content": " ** (count)\n    NULL = 0\n    LEFT_CONTROL = auto()\n    LEFT_SHIFT = auto()\n    LEFT_ALT = auto()\n    LEFT_GUI = auto()\n    RIGHT_CONTROL = auto()\n    RIGHT_SHIFT = auto()\n    RIGHT_ALT = auto()\n    RIGHT_GUI = auto()\nclass MouseButton(Flag):\n    # class MouseButton(Enum):\n    # @staticmethod\n    # def _generate_next_value_(name, start, count, last_values):\n    #     return 2 ** (count)\n    NULL = 0\n    LEFT = auto()\n    RIGHT = auto()\n    MIDDLE = auto()\nclass MultimediaKey(Flag):\n    # class MultimediaKey(Enum):\n    # @staticmethod\n    # def _generate_next_value_(name, start, count, last_values):\n    #     return 2 ** (count)\n    Null = 0\n    # row 1\n    VolumeUp = auto()\n    VolumeDown = auto()\n    Mute = auto()\n    PlayPause = auto()\n    NextTrack = auto()\n    PreviousTrack = auto()\n    CDStop = auto()\n    Eject = auto()\n    # row 2\n    EMail = auto()\n    WWWSearch = auto()\n    WWWFavourites = auto()\n    WWWHome = auto()\n    WWWBack = auto()\n    WWWForward = auto()\n    WWWStop = auto()\n    Refresh = auto()\n    # row 3\n    Med",
        "type": "code",
        "location": "/hid_utils.py:174-229"
    },
    "199": {
        "file_id": 12,
        "content": "This code defines classes for different types of buttons (MouseButton, MultimediaKey) using the Flag and Enum classes. It represents each button as a power of 2, with NULL or Null being 0. The code also includes definitions for various multimedia keys organized into three rows.",
        "type": "comment"
    }
}