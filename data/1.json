{
    "100": {
        "file_id": 12,
        "content": "beat_server_address = dict(\n    host=\"0.0.0.0\",\n    port=(server_port := 8771),\n    beat_url=\"/beat_request\",\n    info_url=\"/info\",\n    url_prefix=f\"http://localhost:{server_port}\",\n)\nbeat_client_data = dict(\n    url=(f\"{beat_server_address['url_prefix']}{beat_server_address['beat_url']}\"),\n    info_url=f\"{beat_server_address['url_prefix']}{beat_server_address['info_url']}\",\n    timeout=2,\n    access_time_key=\"access_time\",\n)\n# BEAT_FAILED = 255\nfrom functools import lru_cache, wraps\nfrom time import monotonic_ns\n# use cacheout instead.\n# ref: https://github.com/dgilland/cacheout\ndef timed_lru_cache(\n    _func=None, *, seconds: int = 7000, maxsize: int = 128, typed: bool = False\n):\n    \"\"\"Extension over existing lru_cache with timeout\n    :param seconds: timeout value\n    :param maxsize: maximum size of the cache\n    :param typed: whether different keys for different types of cache keys\n    \"\"\"\n    def wrapper_cache(f):\n        # create a function wrapped with traditional lru_cache\n        f = lru_cache(maxsize=m",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_common.py:1-32"
    },
    "101": {
        "file_id": 12,
        "content": "beat_server_address is defined with host, port, beat_url, info_url, and url_prefix.\nbeat_client_data contains the URL for beat_server_address and other parameters like timeout and access_time_key.\ntimed_lru_cache is a function that extends lru_cache with timeout, maxsize, and typed parameters.",
        "type": "comment"
    },
    "102": {
        "file_id": 12,
        "content": "axsize, typed=typed)(f)\n        # convert seconds to nanoseconds to set the expiry time in nanoseconds\n        f.delta = seconds * 10**9\n        f.expiration = monotonic_ns() + f.delta\n        @wraps(f)  # wraps is used to access the decorated function attributes\n        def wrapped_f(*args, **kwargs):\n            if monotonic_ns() >= f.expiration:\n                # if the current cache expired of the decorated function then\n                # clear cache for that function and set a new cache value with new expiration time\n                f.cache_clear()\n                f.expiration = monotonic_ns() + f.delta\n            return f(*args, **kwargs)\n        wrapped_f.cache_info = f.cache_info\n        wrapped_f.cache_clear = f.cache_clear\n        return wrapped_f\n    # To allow decorator to be used without arguments\n    if _func is None:\n        return wrapper_cache\n    else:\n        return wrapper_cache(_func)\nimport requests\n# from frozendict import frozendict\n# create a session object\nsession = requests.Sessio",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_common.py:32-61"
    },
    "103": {
        "file_id": 12,
        "content": "This code defines a function decorator that adds caching functionality to the decorated function, with expiration time in nanoseconds. If the cache has expired, it clears the cache and sets a new expiration time.",
        "type": "comment"
    },
    "104": {
        "file_id": 12,
        "content": "n()  # effectively faster. really?\n@timed_lru_cache(seconds=1, maxsize=1)\ndef heartbeat_base(uuid: str, action: str, pid: int, role: str):\n    return heartbeat_base_nocache(uuid, action, pid, role)\ndef heartbeat_base_nocache(uuid: str, action: str, pid: int, role: str):\n    params = dict(uuid=uuid, action=action, pid=pid, role=role)\n    url = beat_client_data[\"url\"]\n    data = request_with_timeout_and_get_json_data(params, url)\n    access_time = data[beat_client_data[\"access_time_key\"]]\n    return access_time\ndef query_info():\n    return request_with_timeout_and_get_json_data(dict(), beat_client_data[\"info_url\"])\nfrom log_common import log_and_print_unknown_exception\nimport os\nimport signal\nimport func_timeout\ndef request_with_timeout_and_get_json_data(params: dict, url: str, success_code=200):\n    try:\n        r = session.get(url, params=params, timeout=beat_client_data[\"timeout\"])\n        assert r.status_code == success_code\n        data = r.json()\n    except func_timeout.dafunc.FunctionTimedOut as exc:\n ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_common.py:61-90"
    },
    "105": {
        "file_id": 12,
        "content": "The code contains a function heartbeat_base that caches the result for 1 second and retrieves it from a server. It also has a query_info function to get information from the server, and request_with_timeout_and_get_json_data function that handles HTTP GET requests with timeout handling.",
        "type": "comment"
    },
    "106": {
        "file_id": 12,
        "content": "       print(exc)\n        print(\"timed out by external wrapper\")\n    except:\n        log_and_print_unknown_exception()\n        print(\"fatal error. cannot beat.\")\n        self_pid = os.getpid()\n        print(f'suicide now. (pid: {self_pid})')\n        kill_by_pid(self_pid)\n    return data\ndef kill_by_pid(pid):\n    kill_signal = getattr(signal, 'SIGKILL', signal.SIGTERM) # adaption for windows\n    os.kill(pid, kill_signal)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_common.py:90-102"
    },
    "107": {
        "file_id": 12,
        "content": "This code handles exceptions and potential timeouts in a program. It prints the exception, indicates a fatal error, logs an unknown exception, and then attempts to kill the process with `os.kill()` function.",
        "type": "comment"
    },
    "108": {
        "file_id": 13,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py",
        "type": "filepath"
    },
    "109": {
        "file_id": 13,
        "content": "The code initializes a FastAPI app with timezone, roles, and PIDs, manages UUIDs, handles HTTP requests, updates statuses, raises exceptions for mismatches, calculates client status in a network, tracks alive/dead counts, schedules tasks to monitor status, and runs the app using uvicorn.",
        "type": "summary"
    },
    "110": {
        "file_id": 13,
        "content": "import fastapi\n# TODO: replace this with gui-attached panel & advanced rescue/countermeasures\napp = fastapi.FastAPI()\nimport datetime\nfrom beat_common import beat_server_address, beat_client_data\nimport pytz\n# with respect to our dearly Py3.6\ntimezone_str = \"Asia/Shanghai\"\n# timezone = pytz.timezone(timezone_str:='Asia/Shanghai')\ntimezone = pytz.timezone(timezone_str)\nimport schedule\n# import threading\nfrom typing import Literal\ndef get_now_and_timestamp():\n    now = get_now()\n    timestamp = now.timestamp()\n    return now, timestamp\nUUID_TO_TIMESTAMP = {}\nUUID_TO_REGISTERED_TIMESTAMP = {}\nUUID_TO_STATUS = {}  # alive -> True; dead -> False\nUUID_TO_PID = {}\nUUID_TO_ROLE = {}\nALIVE_THRESHOLD = 30 * 2  # 30 is a bit of low.\n@app.get(beat_server_address[\"info_url\"])\ndef get_info():\n    schedule.run_pending()\n    _, timestamp = get_now_and_timestamp()\n    return {\n        \"info\": {\n            k: {\n                \"status\": v,\n                \"pid\": UUID_TO_PID[k],\n                \"role\": UUID_TO_ROLE[k],\n                ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py:1-45"
    },
    "111": {
        "file_id": 13,
        "content": "Imports necessary packages and initializes a FastAPI application with timezone and scheduling functionalities.",
        "type": "comment"
    },
    "112": {
        "file_id": 13,
        "content": "\"timestamp\": UUID_TO_TIMESTAMP[k],\n            }\n            for k, v in UUID_TO_STATUS.items()\n        },\n        \"timestamp\": timestamp,\n    }\n# TODO: delegate this kill signal to other process\n# TODO: pass pid with uuid\n# TODO: get unassigned uuid from here, instead of anywhere else\n# TODO: distributed watchdog & recursive keep alive signal mechanism\n# TODO: count revive time & frequencies\n@app.get(beat_server_address[\"beat_url\"])\ndef beat_request(\n    uuid: str,\n    action: Literal[\"hello\", \"heartbeat\", \"kill\"],\n    role: Literal[\"killer\", \"client\", \"server\"],  # can also be server?\n    pid: int,\n):\n    # start = time.time()\n    strtime, timestamp = get_strtime_and_timestamp()\n    if action != \"kill\":\n        for data_dict, it, it_name in [\n            (UUID_TO_PID, pid, \"PID\"),\n            (UUID_TO_ROLE, role, \"ROLE\"),\n        ]:\n            if uuid not in data_dict.keys():\n                data_dict[uuid] = it\n            elif (old_it := data_dict[uuid]) != it:\n                raise Exception(f\"{it_name} mis",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py:45-75"
    },
    "113": {
        "file_id": 13,
        "content": "This code handles HTTP requests for a beat server, which seems to involve managing UUIDs, roles, and PIDs. It receives requests with specific actions (hello, heartbeat, or kill) and from certain roles (killer, client, or possibly server). It updates the status of the UUID accordingly and raises an exception if there's a mismatch in PID or role values.",
        "type": "comment"
    },
    "114": {
        "file_id": 13,
        "content": "match! (old: {old_it}, new: {it})\")\n    if action == \"hello\":\n        print(f\"client {uuid} hello at:\", strtime)\n        UUID_TO_REGISTERED_TIMESTAMP[uuid] = timestamp\n    elif action == \"kill\":\n        print(f\"client {uuid} killed at:\", strtime)\n        for data_dict in [\n            UUID_TO_TIMESTAMP,\n            UUID_TO_REGISTERED_TIMESTAMP,\n            UUID_TO_STATUS,\n            UUID_TO_PID,\n            UUID_TO_ROLE,\n        ]:\n            if uuid in data_dict.keys():\n                del data_dict[uuid]\n    elif action == \"heartbeat\":\n        print(f\"received heartbeat from {uuid} at time {strtime}\")\n    else:\n        raise Exception(f\"client {uuid} with unknown action:\" + action)\n    # end = time.time()\n    if action != \"kill\":\n        if uuid not in UUID_TO_REGISTERED_TIMESTAMP.keys():\n            print(f\"client {uuid} not registered. registering.\")\n            UUID_TO_REGISTERED_TIMESTAMP[uuid] = timestamp\n            # raise Exception(f\"client {uuid} not registered.\")\n        UUID_TO_TIMESTAMP[uuid] =",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py:75-100"
    },
    "115": {
        "file_id": 13,
        "content": "Code handles client actions (\"hello\", \"kill\", \"heartbeat\") and performs corresponding operations on the dictionaries. If an unknown action is received, it raises an exception.",
        "type": "comment"
    },
    "116": {
        "file_id": 13,
        "content": " timestamp\n    # print(f'request processing time: {end-start} secs', )\n    return {beat_client_data[\"access_time_key\"]: strtime}\ndef get_strtime_and_timestamp():\n    now, timestamp = get_now_and_timestamp()\n    strtime = now.strftime(r\"%Y-%m-%d %H:%M:%S\")\n    return strtime, timestamp\ndef get_now():\n    now = datetime.datetime.now(tz=timezone)\n    return now\ndef check_alive():\n    now_strtime, now_timestamp = get_strtime_and_timestamp()\n    alive_roles = []\n    dead_roles = []\n    print(f\"checking clients at {now_strtime}\")\n    for uuid, registered_timestamp in UUID_TO_REGISTERED_TIMESTAMP.items():\n        timestamp = UUID_TO_TIMESTAMP.get(uuid, registered_timestamp)\n        role = UUID_TO_ROLE.get(uuid, \"unknown\")\n        pid = UUID_TO_PID.get(uuid, -1)\n        uptime = now_timestamp - registered_timestamp\n        alive = True\n        life = ALIVE_THRESHOLD - (now_timestamp - timestamp)\n        if life < 0:\n            alive = False\n        UUID_TO_STATUS[uuid] = alive\n        up_status = f\"up: {uptime:.3f} s",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py:100-131"
    },
    "117": {
        "file_id": 13,
        "content": "Function get_strtime_and_timestamp():\n- Returns current date and time as a string and timestamp.\n\nFunction get_now():\n- Returns the current datetime object with the correct timezone.\n\nFunction check_alive():\n- Checks clients' statuses based on their uptime and remaining lifetimes.",
        "type": "comment"
    },
    "118": {
        "file_id": 13,
        "content": "ecs\"\n        pid_info = f\"pid: {pid}\"\n        if alive:\n            print(\n                f\"client {uuid} alive ({pid_info}, {life:.3f} secs to death, {up_status})\"\n            )\n            alive_roles.append(role)\n        else:\n            print(\n                f\"client {uuid} ({pid_info}, {up_status}) dead for {-life:.3f} seconds\"\n            )\n            dead_roles.append(role)\n    status_list = UUID_TO_STATUS.values()\n    print(\"summary\".center(60, \"=\"))\n    print(\"total clients:\", len(status_list))\n    print(\"alive clients:\", *role_statistics(alive_roles))\n    print(\"dead clients:\", *role_statistics(dead_roles))\nfrom typing import List\ndef role_statistics(roles: List[str]):\n    details = \", \".join([f\"{r}: {roles.count(r)}\" for r in set(roles)])\n    return len(roles), f\"({details})\" if details else \"\"\n# import time\nschedule.every(int(ALIVE_THRESHOLD / 3)).seconds.do(check_alive)\n# def check_alive_thread():\n#     while True:\n#         time.sleep(1)\n#         schedule.run_pending()\nif __name__ == \"__main__",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py:131-169"
    },
    "119": {
        "file_id": 13,
        "content": "Code snippet is checking the status of clients in a network and providing summary statistics. It tracks the client's uuid, role, pid (Process ID), if it's alive or dead, and how many seconds remaining/dead for. The code also calculates and prints the total number of alive and dead clients. It schedules tasks to check alive status every 1/3 of ALIVE_THRESHOLD time interval.",
        "type": "comment"
    },
    "120": {
        "file_id": 13,
        "content": "\":\n    import uvicorn\n    # thread = threading.Thread(target=check_alive_thread, daemon=True)\n    # thread.start()\n    uvicorn.run(app, **{k: beat_server_address[k] for k in [\"host\", \"port\"]})",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/beat_server.py:169-174"
    },
    "121": {
        "file_id": 13,
        "content": "Importing uvicorn and running the app with specific host and port values.",
        "type": "comment"
    },
    "122": {
        "file_id": 14,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/bytes_actor.py",
        "type": "filepath"
    },
    "123": {
        "file_id": 14,
        "content": "Imports NaiveActor and defines BytesActor, using write_method to send messages. Runs the main loop with run_naive.",
        "type": "summary"
    },
    "124": {
        "file_id": 14,
        "content": "from naive_actor import NaiveActor, run_naive\nclass BytesActor(NaiveActor):\n    write_method = lambda proc: proc.send\nif __name__ == \"__main__\":\n    run_naive(BytesActor)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/bytes_actor.py:1-9"
    },
    "125": {
        "file_id": 14,
        "content": "Imports NaiveActor and defines BytesActor, using write_method to send messages. Runs the main loop with run_naive.",
        "type": "comment"
    },
    "126": {
        "file_id": 15,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/dependent_task_timeout_exec.py",
        "type": "filepath"
    },
    "127": {
        "file_id": 15,
        "content": "This code sets a future with \"hello\" after 1 second, waits for the result (expected \"hello world\"), and if the timeout expires before setting, prints \"unset future\".",
        "type": "summary"
    },
    "128": {
        "file_id": 15,
        "content": "import asyncio\nimport os\n# import multitasking\n# after all, we are single threaded\nimport time\nfrom timeout_utils import *\n@retrying_timeout_func(1, 11)  # passed\ndef os_sleep():\n    print(\"running os sleep\")\n    os.system(\"sleep 10\")\n    os.system(\"echo hello world\")\n    print(\"exit os sleep\")\n@retrying_timeout_func(3, 2)  # not pass, fail attempts count: 3\ndef time_sleep():\n    print(\"running time sleep\")\n    time.sleep(3)\n    print(\"exit time sleep\")\nasync def set_after(fut, delay, value):\n    # Sleep for *delay* seconds.\n    await asyncio.sleep(delay)\n    # Set *value* as a result of *fut* Future.\n    fut.set_result(value)\nasync def main():\n    # Get the current event loop.\n    loop = asyncio.get_running_loop()\n    # Create a new Future object.\n    fut = loop.create_future()\n    # Run \"set_after()\" coroutine in a parallel Task.\n    # We are using the low-level \"loop.create_task()\" API here because\n    # we already have a reference to the event loop at hand.\n    # Otherwise we could have just used \"asyncio.creat",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/dependent_task_timeout_exec.py:1-44"
    },
    "129": {
        "file_id": 15,
        "content": "Code snippet imports necessary libraries, defines a function for OS sleep, a function for time sleep, and two async functions. The async main function creates a new Future object and runs a set_after coroutine in parallel Task using the event loop.",
        "type": "comment"
    },
    "130": {
        "file_id": 15,
        "content": "e_task()\".\n    loop.create_task(set_after(fut, 1, \"... world\"))\n    print(\"hello ...\")\n    # Wait until *fut* has a result (1 second) and print it.\n    # print(await asyncio.wait_for(fut, timeout=1.5))\n    print(await asyncio.wait_for(fut, timeout=0.5))\nif __name__ == \"__main__\":\n    os_sleep()\n    time_sleep()\n    # asyncio.run(main())",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/dependent_task_timeout_exec.py:44-57"
    },
    "131": {
        "file_id": 15,
        "content": "This code is creating a task that sets a future with the string \"hello\" after 1 second, and then waits for the result of the future (expected to be \"hello world\") before printing it. If the timeout of 0.5 seconds expires before the future is set, it will print \"unset future\".",
        "type": "comment"
    },
    "132": {
        "file_id": 16,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/entropy_utils.py",
        "type": "filepath"
    },
    "133": {
        "file_id": 16,
        "content": "The code defines classes `EntropyCalculator` and `ContentEntropyCalculator` to calculate entropy of categorical data using context managers, histograms, and compares results for different test cases.",
        "type": "summary"
    },
    "134": {
        "file_id": 16,
        "content": "from contextlib import contextmanager\nimport numpy as np\nfrom scipy.stats import entropy\nclass EntropyCalculator:\n    def __init__(self, base=2):\n        self.cats_to_count = {}\n        self.base = base\n    def count(self, elem):\n        self._count(elem)\n    def _count(self, elem):\n        self.cats_to_count[elem] = self.cats_to_count.get(elem, 0) + 1\n    @property\n    def entropy(self):\n        vals = list(self.cats_to_count.values())\n        if vals != []:\n            hist = np.array(vals)\n            total_count = sum(hist)\n            hist = hist / total_count\n        else:\n            hist = []\n        ent = entropy(hist, base=self.base)\n        return ent\nclass ContentEntropyCalculator(EntropyCalculator):\n    def count(self, content):\n        if isinstance(content, str):\n            content = content.encode()\n        if not isinstance(content, bytes):\n            raise Exception(\"unknown content type:\", type(content))\n        content_int_arr = list(content)\n        for i in content_int_arr:\n            self.",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/entropy_utils.py:1-39"
    },
    "135": {
        "file_id": 16,
        "content": "The code defines a class `EntropyCalculator` that calculates the entropy of categorical data and a subclass `ContentEntropyCalculator` for calculating content entropy. The `EntropyCalculator` has methods to count categories, and the `ContentEntropyCalculator` extends this by counting bytes in content data. Both classes use numpy and scipy for efficient calculations.",
        "type": "comment"
    },
    "136": {
        "file_id": 16,
        "content": "_count(i)\n@contextmanager\ndef entropyContext(is_content=False):\n    if is_content:\n        calc = ContentEntropyCalculator()\n    else:\n        calc = EntropyCalculator()\n    try:\n        yield calc\n    finally:\n        del calc\ndef calculate_content_entropy(content):\n    with entropyContext(is_content=True) as calc:\n        calc.count(content)\n        return calc.entropy\n# def calculate_content_entropy(content, full=False):\n# if isinstance(content, str):\n#     content = content.encode()\n# if not isinstance(content,bytes):\n#     raise Exception(\"unknown content type:\", type(content))\n# content_int_arr = list(content)\n# if full:\n#     # use 256-1 bins\n#     hist, _ = np.histogram(content_int_arr, bins=255, range=(0,255))\n# else:\n#     cats = list(set(content_int_arr))\n#     cat_to_index = {cat:i for i, cat in enumerate(cats)}\n#     hist = np.zeros(len(cats))\n#     for elem in content_int_arr:\n#         index = cat_to_index[elem]\n#         hist[index] += 1\n# # normalize histogram.\n# hist = hist.astype(float)\n# norm_hist ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/entropy_utils.py:39-78"
    },
    "137": {
        "file_id": 16,
        "content": "This code defines a function `calculate_content_entropy` that calculates the entropy of content data. It uses a context manager `entropyContext` to create an instance of either `ContentEntropyCalculator` or `EntropyCalculator` based on whether the input is content or not. The function then counts the occurrences of different values in the content using histograms and returns the entropy value.",
        "type": "comment"
    },
    "138": {
        "file_id": 16,
        "content": "= hist/len(content_int_arr)\n# # print('normalized histogram:', norm_hist)\n# ent = entropy(norm_hist, base=2)\n# return ent\nif __name__ == \"__main__\":\n    testcases = [\"aa\", \"ab\", \"abcdesd\", \"def\", \"hijklmn\", bytes(range(256))]\n    # the more chars the more entropy.\n    for case in testcases:\n        ent = calculate_content_entropy(case)\n        # ent_full = calculate_content_entropy(case, full=True)\n        print(\"testcase:\", case)\n        # identical!\n        print(\"entropy:\", ent)\n        # print(\"local entropy:\", ent)\n        # print(\"global entropy:\", ent_full)\n        print()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/entropy_utils.py:78-97"
    },
    "139": {
        "file_id": 16,
        "content": "This code calculates the entropy of different test cases and prints the results. The more characters in the test case, the higher the entropy.",
        "type": "comment"
    },
    "140": {
        "file_id": 17,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/get_pid_of_self.py",
        "type": "filepath"
    },
    "141": {
        "file_id": 17,
        "content": "Gets the PID of the current interpreter.",
        "type": "summary"
    },
    "142": {
        "file_id": 17,
        "content": "import os\npid = os.getpid()\nprint(\"PID of the current interpreter is: \", pid)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/get_pid_of_self.py:1-4"
    },
    "143": {
        "file_id": 17,
        "content": "Gets the PID of the current interpreter.",
        "type": "comment"
    },
    "144": {
        "file_id": 18,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/kill_server.py",
        "type": "filepath"
    },
    "145": {
        "file_id": 18,
        "content": "The code manages client processes and communicates with a server, monitoring their status, killing dead clients, logging actions, and sending heartbeats.",
        "type": "summary"
    },
    "146": {
        "file_id": 18,
        "content": "# kill dead process by pid and remove them from beat server\n# the kill server must emit beat signals, and can kill other processes to prove its effectiveness\n# maybe we need to elevate\n# import elevate\nfrom beat_common import *\nimport os\nimport signal\nimport uuid\nfrom log_common import *\nkiller_pid = os.getpid()\nkiller_uuid = str(uuid.uuid4())\ndef commit_kill(client_uuid: str, client_pid: int, client_role:str):\n    kill_time = heartbeat_base_nocache(client_uuid, \"kill\", client_pid, client_role)\n    print(f\"commit kill for client {client_uuid} (pid: {client_pid}) at {kill_time}\")\ndef kill_dead_process():\n    dead_clients = []\n    info = query_info()\n    client_info= info['info']\n    for client_uuid, client_info_dict in client_info.items():\n        client_status = client_info_dict['status']\n        client_pid = client_info_dict['pid']\n        client_role = client_info_dict['role']\n        if client_status is False:\n            print(f\"client {client_uuid} is dead.\")\n            if client_pid == killer_pid:\n     ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/kill_server.py:1-30"
    },
    "147": {
        "file_id": 18,
        "content": "This code is a part of a kill server that monitors running processes and can terminate them if necessary. It checks the status of clients, their PIDs, and roles, and kills any dead clients. It also logs relevant information for each client.",
        "type": "comment"
    },
    "148": {
        "file_id": 18,
        "content": "           print(\"cannot suicide\")\n                continue\n            dead_clients.append((client_uuid, client_pid, client_role))\n    for client_uuid, client_pid, client_role in dead_clients:\n        print(\"killing client:\", client_uuid)\n        try:\n            kill_by_pid(client_pid)\n        except ProcessLookupError:\n            print(f'client {client_uuid} (pid: {client_pid}) is already killed')\n        except:\n            log_and_print_unknown_exception()\n        # remove from history.\n        commit_kill(client_uuid, client_pid, client_role)\ndef kill_server_beat(action = 'heartbeat'):\n    atime = heartbeat_base(killer_uuid, action, killer_pid, 'killer')\n    print(f\"killer {killer_uuid} beat at:\", atime)\nimport time\nif __name__ == \"__main__\":\n    print(f\"killer {killer_uuid} started (pid: {killer_pid})\")\n    kill_server_beat('hello')\n    while True:\n        kill_dead_process()\n        time.sleep(1)\n        kill_server_beat()",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/kill_server.py:30-57"
    },
    "149": {
        "file_id": 18,
        "content": "Code is for managing client processes and communicating with a server. It keeps track of dead clients, attempts to kill them using their PIDs, and logs the actions. The `kill_server_beat()` function sends heartbeats to the server and the main loop continuously kills dead processes and sends heartbeats.",
        "type": "comment"
    },
    "150": {
        "file_id": 19,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/log_common.py",
        "type": "filepath"
    },
    "151": {
        "file_id": 19,
        "content": "Imports necessary libraries and sets up a logger with rotating file handler, debug level logging to a file and console in case of exceptions. Provides a function for logging and printing unknown exceptions.",
        "type": "summary"
    },
    "152": {
        "file_id": 19,
        "content": "import sys\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport better_exceptions\nlog_filename = \"actors.log\"\nrthandler = RotatingFileHandler(\n    log_filename, maxBytes=1024 * 1024 * 15, backupCount=3, encoding=\"utf-8\"\n)\nlogger = logging.getLogger(\"actors\")\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(rthandler)\nlogger.addHandler(logging.StreamHandler(sys.stderr))\nbetter_exceptions.SUPPORTS_COLOR = False\ndef log_and_print_unknown_exception():\n    exc_type, exc_info, exc_tb = sys.exc_info()\n    # traceback.print_exc()\n    if exc_type is not None:\n        exc_str = \"\\n\".join(\n            better_exceptions.format_exception(exc_type, exc_info, exc_tb)\n        )\n        logger.debug(exc_str)\n        print(exc_str)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/log_common.py:2-30"
    },
    "153": {
        "file_id": 19,
        "content": "Imports necessary libraries and sets up a logger with rotating file handler, debug level logging to a file and console in case of exceptions. Provides a function for logging and printing unknown exceptions.",
        "type": "comment"
    },
    "154": {
        "file_id": 20,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/loop_forever.py",
        "type": "filepath"
    },
    "155": {
        "file_id": 20,
        "content": "Infinite loop, continuously adds 1 to 'a'.",
        "type": "summary"
    },
    "156": {
        "file_id": 20,
        "content": "while True:\n    a = 1 + 1",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/loop_forever.py:1-2"
    },
    "157": {
        "file_id": 20,
        "content": "Infinite loop, continuously adds 1 to 'a'.",
        "type": "comment"
    },
    "158": {
        "file_id": 21,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py",
        "type": "filepath"
    },
    "159": {
        "file_id": 21,
        "content": "The code introduces a metaheuristic predictive actor class, incorporates kernel weight updates and average performance refreshes, but the scoring criteria remain unclear. The score is calculated using loop count and MetaheuristicPredictiveWrapper to generate an actor for predictive_alpine_actor module, running it forever with run_actor_forever function.",
        "type": "summary"
    },
    "160": {
        "file_id": 21,
        "content": "import math\n# TODO: rerun the same kernel for several times and get average performance (to eliminate residual errors)\n# extract the kernel from the dead ones.\n# the new kernel will be added to the random kernel.\n# import copy\nimport weakref\nfrom typing import Callable, List\nimport numpy as np\nfrom typing_extensions import Literal\nfrom alpine_actor import run_actor_forever\nfrom naive_actor import ActorStats\nfrom sequence_learner import PredictorWrapper\nACTIVATION_FUNCMAP = {\n    \"atan\": lambda n: math.atan(n) / math.pi * 2,\n    \"tanh\": math.tanh,\n}  # input: -oo, oo; output: -1, 1\nclass MetaheuristicActorStats(ActorStats):\n    ...\nimport copy\nclass MetaheuristicPredictiveWrapper:\n    top_k = 100\n    def __init__(\n        self,\n        ksize: int,\n        predictiveActorClass,\n        # predictorClass,\n        activation: Literal[\"atan\", \"tanh\"],\n        eps=1e-5,\n        trials_on_same_kernel=3,\n    ):\n        class MetaheuristicPredictiveActor(predictiveActorClass):\n            actorStatsClass = MetaheuristicActorS",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:1-44"
    },
    "161": {
        "file_id": 21,
        "content": "This code defines a class `MetaheuristicPredictiveWrapper` that extends an existing predictive actor with additional metaheuristics. It takes in parameters such as kernel size, predictive actor class, activation function (\"atan\" or \"tanh\"), and other settings. It also creates a new class `MetaheuristicPredictiveActor` that inherits from the specified predictive actor class and uses the defined metaheuristics.",
        "type": "comment"
    },
    "162": {
        "file_id": 21,
        "content": "tats\n            def __init__(\n                self,\n                *args,\n                metaWrapperWeakref: Callable[[], MetaheuristicPredictiveWrapper] = ...,\n                # metaInfo: List[List[str]] = ...,\n                **kwargs,\n            ):\n                self.metaWrapperWeakref = metaWrapperWeakref\n                super().__init__(*args, **kwargs)\n            def setMetaInfo(self, metaInfo):\n                setattr(self, \"metaInfo\", metaInfo)\n            def __del__(self):\n                metaWrapper = self.metaWrapperWeakref()\n                trial_count = metaWrapper.trial_count\n                average_performance = metaWrapper.average_performance\n                metaInfo = copy.deepcopy(getattr(self, \"metaInfo\", []))\n                try:\n                    super().__del__()\n                finally:\n                    print(\"metaheuristic\".center(50, \"=\"))\n                    print(\"trial count:\", trial_count)\n                    print(\"average performance:\", average_performance)\n       ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:44-70"
    },
    "163": {
        "file_id": 21,
        "content": "The code defines a class with an optional metaWrapperWeakref, sets metaInfo using setMetaInfo method and deletes the instance by calling its parent's del method after logging trial count and average performance.",
        "type": "comment"
    },
    "164": {
        "file_id": 21,
        "content": "             for print_params in metaInfo:\n                        if len(print_params) > 1:\n                            print(print_params[0] + \":\", *print_params[1:])\n                        elif len(print_params) == 1:\n                            print(print_params[0])\n                        else:\n                            print()\n            def getStatsDict(self):\n                statsDict = super().getStatsDict()\n                statsDict.update(dict())\n                return statsDict\n        self.predictiveActorClass = MetaheuristicPredictiveActor\n        # self.predictorClass = predictorClass\n        self.ksize = ksize\n        self.trial_count = 0\n        self.trials_on_same_kernel = trials_on_same_kernel\n        self.average_performance = 0\n        self.activation = ACTIVATION_FUNCMAP[activation]\n        self.eps = eps\n        self.new()\n    def __next__(self):\n        # use inheritance instead of this!\n        # use weakref of self\n        self.remix()\n        return self.actor\n        # actor_in",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:70-98"
    },
    "165": {
        "file_id": 21,
        "content": "This code is defining a class with methods to create and manage a predictive actor. It has methods for getting statistics, updating the actor, and remixing the actor. It also initializes class variables such as trial count and average performance.",
        "type": "comment"
    },
    "166": {
        "file_id": 21,
        "content": "stance = self.actorClass()\n        # actor_instance.metaWrapperWeakref = weakref.ref(self)\n        # return actor_instance\n    def new(self):\n        if hasattr(self, \"actor\"):\n            delattr(self, \"actor\")\n        actor = self.predictiveActorClass(\n            metaWrapperWeakref=weakref.ref(self), ksize=self.ksize\n        )\n        setattr(self, \"actor\", actor)\n    def get_kernel(self) -> np.ndarray:\n        return self.actor.predictorWrapper.predictor.kernel.copy()\n    def set_kernel(self, kernel: np.ndarray):\n        kernel_shape = kernel.shape\n        desired_shape = (self.ksize,)\n        assert (\n            kernel_shape == desired_shape\n        ), f\"kernel shape mismatch: {kernel_shape} != {desired_shape}\"\n        self.actor.predictorWrapper.predictor.kernel = kernel\n    kernel = property(fget=get_kernel, fset=set_kernel)\n    def remix(self):\n        old_kernel = self.kernel\n        old_score = self.score()\n        avg_performance = self.refresh_average_performance(\n            old_score\n        )  ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:98-128"
    },
    "167": {
        "file_id": 21,
        "content": "This code is related to an actor class in a metaheuristic predictive model. The \"new\" function initializes the actor, the \"get_kernel\" and \"set_kernel\" functions retrieve or update the kernel, and the \"remix\" function keeps track of performance while refreshing the average performance.",
        "type": "comment"
    },
    "168": {
        "file_id": 21,
        "content": "# warning! base shall never be 1\n        # log (avg performance as base) & tanh/atan\n        old_add_weight = math.log(old_score / avg_performance, avg_performance)\n        old_add_weight = self.activation(old_add_weight) / 2\n        # old_add_weight = self.activation(old_add_weight*self.trial_count) / 2\n        # old_add_weight = self.activation(old_add_weight*(1+math.log(self.trial_count)) / 2\n        new_kernel_weight = 0.5 - old_add_weight\n        old_kernel_weight = 0.5 + old_add_weight\n        self.actor.setMetaInfo(\n            [\n                (\"score\", old_score),\n                (\"old kernel weight\", old_kernel_weight),\n                (\"new kernel weight\", new_kernel_weight),\n            ]\n        )\n        self.new()\n        new_kernel = self.kernel\n        # emit noise if not doing well?\n        # harmony vice versa?\n        if (repeat_times := self.trial_count % self.trials_on_same_kernel) == 0:\n            print(f\"refreshing kernel (every {self.trials_on_same_kernel} time(s))\")\n            ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:128-151"
    },
    "169": {
        "file_id": 21,
        "content": "Updating kernel weights and setting meta information.\n\nThis code snippet updates the old and new kernel weights based on the old_score, activates them using a specific activation function, and sets the actor's meta information with the updated scores. If the trial count is a multiple of trials_on_same_kernel, it prints a message indicating that the kernel is being refreshed.",
        "type": "comment"
    },
    "170": {
        "file_id": 21,
        "content": "self.kernel = (\n                new_kernel * new_kernel_weight + old_kernel * old_kernel_weight\n            )\n        else:\n            print(f\"using old kernel (repeat: {repeat_times} time(s))\")\n            self.kernel = old_kernel\n    def refresh_average_performance(self, score: float):\n        self.average_performance = (\n            self.average_performance * self.trial_count + score\n        ) / (self.trial_count + 1)\n        self.trial_count += 1\n        if self.average_performance == 1:\n            self.average_performance += self.eps\n        return self.average_performance\n    def score(self):\n        stats: ActorStats = self.actor.stats\n        # score by what?\n        # example:\n        \"\"\"\n        =====================summary======================\n        start time:     2023-09-01T09:54:11.270057+08:00\n        end time:       2023-09-01T09:54:43.327770+08:00\n        up time:        32.05771350860596\n        loop count:     290\n        total bytes read:       237\n        total bytes write:      2476\n ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:151-179"
    },
    "171": {
        "file_id": 21,
        "content": "Code is initializing the kernel value based on new and old kernel weights.\nIt refreshes the average performance by adding the current score to the previous average and updates trial count.\nThe score function calculates the score based on actor's stats, but it's not clear what the scoring criteria are.",
        "type": "comment"
    },
    "172": {
        "file_id": 21,
        "content": "       r/w ratio: 0.09571890145395799\n        w/r ratio: 10.447257383966244\n        read bytes entropy: 4.946365138818157\n        write bytes entropy: 6.148352516530523\n        r/w entropy ratio: 0.8045025273875089\n        w/r entropy ratio: 1.2430041745764768\n        \"\"\"\n        # for now, just take the up time\n        # uptime seems to be less universal.\n        # let's use loop count for now.\n        score = stats.loop_count + self.eps\n        # score = stats.up_time + self.eps\n        return score\nif __name__ == \"__main__\":\n    # from alpine_actor import AlpineActor\n    from predictive_alpine_actor import PredictiveAlpineActor  # PredictorWrapper\n    actor_generator = MetaheuristicPredictiveWrapper(\n        ksize=256,  # too small!\n        # ksize=100,\n        predictiveActorClass=PredictiveAlpineActor,\n        # predictorClass=PredictorWrapper,\n        activation=\"tanh\",\n    )\n    # breakpoint()\n    run_actor_forever(actor_generator)",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/metaheuristic_predictive_actor.py:179-206"
    },
    "173": {
        "file_id": 21,
        "content": "This code calculates the score based on loop count and uses MetaheuristicPredictiveWrapper to generate an actor for the predictive_alpine_actor module. It then runs the actor forever using run_actor_forever function.",
        "type": "comment"
    },
    "174": {
        "file_id": 22,
        "content": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py",
        "type": "filepath"
    },
    "175": {
        "file_id": 22,
        "content": "The code performs performance benchmarking on actor processes using non-blocking socket readers, measures execution time, and outputs results in tabular format. It also defines a class-based actor running until a condition is met and serves as the main program.",
        "type": "summary"
    },
    "176": {
        "file_id": 22,
        "content": "# import twisted\n# this could be used as test case.\n# TODO: survive reopening the laptop lid\n# TODO: improve task execution logic, eliminate long running blocking tasks.\n# TODO: use celery to schedule tasks\nimport datetime\nfrom beat_common import heartbeat_base\nimport os\nimport sys\nimport time\nimport copy\nimport traceback\nfrom cmath import nan\nfrom log_common import *\nimport uuid\ncurrent_pid = os.getpid()\nprint(\"current_pid:\", current_pid)\nactor_uuid = str(uuid.uuid4())\nstrtime = heartbeat_base(uuid=actor_uuid, action = 'hello', pid=current_pid, role='client')\nprint('beat server hello: %s' % strtime)\nclass InteractiveChallengeFailed(Exception):\n    \"\"\"\n    If \"expect\" like challenge failed for some reason, raise this exception.\n    \"\"\"\n    ...\n# https://code.activestate.com/recipes/440554/\n# wxpython, wexpect/winpexpect, pexpect\n# https://peps.python.org/pep-3145/\n# https://peps.python.org/pep-3156/\nfrom collections import deque\nimport func_timeout\nimport pytz\nfrom pydantic import BaseModel\nfrom entropy_utils import C",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:1-40"
    },
    "177": {
        "file_id": 22,
        "content": "The code is importing various libraries and modules such as Twisted, beat_common, log_common, entropy_utils, and pydantic. It also defines a class InteractiveChallengeFailed for handling failed challenges. The code sets up variables and imports other collections and functions. It seems to be related to interactive programs, challenges, and time-related tasks.",
        "type": "comment"
    },
    "178": {
        "file_id": 22,
        "content": "ontentEntropyCalculator\nfrom type_utils import *\nfrom vocabulary import NaiveVocab\ndef unicodebytes(string: str):\n    return bytes(string, encoding=\"utf8\")\nclass ActorStats(BaseModel):\n    start_time: float\n    end_time: float\n    up_time: float\n    loop_count: int\n    read_bytes: int\n    write_bytes: int\n    read_ent: float\n    write_ent: float\n    rw_ratio: float\n    wr_ratio: float\n    rw_ent_ratio: float\n    wr_ent_ratio: float\ndef safeDiv(a, b):\n    \"\"\"\n    Return a/b if no exception is raised, otherwise nan.\n    \"\"\"\n    ret = nan\n    try:\n        ret = a / b\n    except ZeroDivisionError:\n        pass\n    return ret\ndef leftAndRightSafeDiv(a, b):\n    \"\"\"\n    Return a/b and b/a, in safe division manner.\n    \"\"\"\n    left_div = safeDiv(a, b)\n    right_div = safeDiv(b, a)\n    return left_div, right_div\nREAD_KNOWN_EXCEPTIONS = []\n# SOCKET_TIMEOUT = .2\n# SOCKET_TIMEOUT = .01\nSOCKET_TIMEOUT = 0.001\nfrom contextlib import contextmanager\nif os.name == \"nt\":\n    NT_CONTEXT = dict(NT_READ_NONBLOCKING_DECODE=False, NT_ENCODING=\"utf-",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:40-92"
    },
    "179": {
        "file_id": 22,
        "content": "The code is defining an ActorStats class that stores various statistics about the actor's performance, such as start and end times, up-time, loop count, read/write bytes, entropy of read/write data, and ratios between these values. The code also includes a safeDiv function for safe division operations, and a context manager that sets specific properties when running on Windows (NT_READ_NONBLOCKING_DECODE and NT_ENCODING).",
        "type": "comment"
    },
    "180": {
        "file_id": 22,
        "content": "8\")\n    @contextmanager\n    def nt_read_nonblocking_decode_context():\n        NT_CONTEXT[\"NT_READ_NONBLOCKING_DECODE\"] = True\n        try:\n            yield\n        finally:\n            NT_CONTEXT[\"NT_READ_NONBLOCKING_DECODE\"] = False\n    import wexpect as pexpect\n    expected_wexpect_version = \"4.0.0\"\n    wexp_version = pexpect.__version__\n    assert (\n        wexp_version == expected_wexpect_version\n    ), \"wexpected version should be: {}\\ncurrently: {}\".format(\n        expected_wexpect_version, wexp_version\n    )\n    import wexpect.host as host\n    import socket\n    def spawnsocket_connect_to_child(self):\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.connect((self.host, self.port))\n        self.sock.settimeout(SOCKET_TIMEOUT)\n    def spawnsocket_read_nonblocking(self, size=1):\n        \"\"\"This reads at most size characters from the child application. If\n        the end of file is read then an EOF exception will be raised.\n        This is not effected by the 'size' pa",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:92-123"
    },
    "181": {
        "file_id": 22,
        "content": "This code defines a context manager for non-blocking reading and a function to connect to a child application via socket, checking the wexpect version.",
        "type": "comment"
    },
    "182": {
        "file_id": 22,
        "content": "rameter, so if you call\n        read_nonblocking(size=100, timeout=30) and only one character is\n        available right away then one character will be returned immediately.\n        It will not wait for 30 seconds for another 99 characters to come in.\n        This is a wrapper around Wtty.read().\"\"\"\n        logger = host.logger\n        EOF_CHAR = host.EOF_CHAR\n        EOF = host.EOF\n        if self.closed:\n            logger.info(\"I/O operation on closed file in read_nonblocking().\")\n            raise ValueError(\"I/O operation on closed file in read_nonblocking().\")\n        try:\n            s = self.sock.recv(size)\n            if s:\n                logger.debug(f\"Readed: {s}\")\n            else:\n                logger.spam(f\"Readed: {s}\")\n            if EOF_CHAR in s:\n                self.flag_eof = True\n                logger.info(\"EOF: EOF character has been arrived\")\n                s = s.split(EOF_CHAR)[0]\n        except ConnectionResetError:\n            self.flag_eof = True\n            logger.info(\"EOF(",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:123-153"
    },
    "183": {
        "file_id": 22,
        "content": "This code snippet is a part of the `naive_actor.py` file and it's a function called `read_nonblocking()`. It reads data from a socket in non-blocking mode, returns one character immediately if available, and doesn't wait for additional characters to reach the timeout limit. If an EOF (End of File) character is detected, it sets the `flag_eof` flag and removes the EOF character from the received data before returning it.",
        "type": "comment"
    },
    "184": {
        "file_id": 22,
        "content": "'ConnectionResetError')\")\n            raise EOF(\"ConnectionResetError\")\n        except socket.timeout:\n            return \"\" if NT_CONTEXT[\"NT_READ_NONBLOCKING_DECODE\"] else b\"\"\n        return (\n            s.decode(NT_CONTEXT[\"NT_ENCODING\"])\n            if NT_CONTEXT[\"NT_READ_NONBLOCKING_DECODE\"]\n            else s\n        )\n    def spawnpipe_read_nonblocking(self, size=1):\n        \"\"\"This reads at most size characters from the child application. If\n        the end of file is read then an EOF exception will be raised.\n        This is not effected by the 'size' parameter, so if you call\n        read_nonblocking(size=100, timeout=30) and only one character is\n        available right away then one character will be returned immediately.\n        It will not wait for 30 seconds for another 99 characters to come in.\n        This is a wrapper around Wtty.read().\"\"\"\n        logger = host.logger\n        EOF_CHAR = host.EOF_CHAR\n        EOF = host.EOF\n        if self.closed:\n            logger.warning(\"I/O operatio",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:153-180"
    },
    "185": {
        "file_id": 22,
        "content": "This function reads at most \"size\" characters from the child application. If it reaches the end of file, an EOF exception is raised. The function does not wait for more characters based on the specified timeout. This uses the Wtty.read() method as a wrapper.",
        "type": "comment"
    },
    "186": {
        "file_id": 22,
        "content": "n on closed file in read_nonblocking().\")\n            raise ValueError(\"I/O operation on closed file in read_nonblocking().\")\n        try:\n            s = host.win32file.ReadFile(self.pipe, size)[1]\n            if s:\n                logger.debug(f\"Readed: {s}\")\n            else:\n                logger.spam(f\"Readed: {s}\")\n            if EOF_CHAR in s:\n                self.flag_eof = True\n                logger.info(\"EOF: EOF character has been arrived\")\n                s = s.split(EOF_CHAR)[0]\n            # return s\n            return (\n                s.decode(NT_CONTEXT[\"NT_ENCODING\"])\n                if NT_CONTEXT[\"NT_READ_NONBLOCKING_DECODE\"]\n                else s\n            )\n            # return s.decode()\n        except host.pywintypes.error as e:\n            if e.args[0] == host.winerror.ERROR_BROKEN_PIPE:  # 109\n                self.flag_eof = True\n                logger.info(\"EOF('broken pipe, bye bye')\")\n                raise EOF(\"broken pipe, bye bye\")\n            elif e.args[0] == host.winerror",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:180-208"
    },
    "187": {
        "file_id": 22,
        "content": "Reading non-blocking data from a pipe, handling EOF and error cases.",
        "type": "comment"
    },
    "188": {
        "file_id": 22,
        "content": ".ERROR_NO_DATA:\n                \"\"\"232 (0xE8): The pipe is being closed.\"\"\"\n                self.flag_eof = True\n                logger.info(\"EOF('The pipe is being closed.')\")\n                raise EOF(\"The pipe is being closed.\")\n            else:\n                raise\n    host.SpawnSocket.connect_to_child = spawnsocket_connect_to_child\n    host.SpawnSocket.read_nonblocking = spawnsocket_read_nonblocking\n    host.SpawnPipe.read_nonblocking = spawnpipe_read_nonblocking\n    def spawnbase_sendline(self, s=b\"\"):\n        s = enforce_bytes(s)\n        n = self.send(s + b\"\\r\\n\")\n        return n\n    host.SpawnBase.sendline = spawnbase_sendline\nelse:\n    import pexpect\n    # let's skip version check, for kail.\n    # expected_pexpect_version = \"4.6.0\"\n    # pexp_version = pexpect.__version__\n    # assert (\n    #     pexp_version == expected_pexpect_version\n    # ), \"pexpected version should be: {}\\ncurrently: {}\".format(\n    #     expected_pexpect_version, pexp_version\n    # )\n    READ_KNOWN_EXCEPTIONS.append(pexpect.",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:208-239"
    },
    "189": {
        "file_id": 22,
        "content": "Raises EOF error when pipe is closed and defines functions for sending data.",
        "type": "comment"
    },
    "190": {
        "file_id": 22,
        "content": "pty_spawn.TIMEOUT)\n    READ_KNOWN_EXCEPTIONS.append(pexpect.spawnbase.EOF)  # are you sure?\n    def spawn_sendline(self, s=b\"\"):\n        s = enforce_bytes(s)\n        return self.send(s + unicodebytes(os.linesep))\n    pexpect.spawn.sendline = spawn_sendline\n    def spawnbase_read_nonblocking(self, size=1, timeout=None):\n        \"\"\"This reads data from the file descriptor.\n        This is a simple implementation suitable for a regular file. Subclasses using ptys or pipes should override it.\n        The timeout parameter is ignored.\n        \"\"\"\n        try:\n            s = os.read(self.child_fd, size)\n        except OSError as err:\n            if err.args[0] == pexpect.spawnbase.errno.EIO:\n                # Linux-style EOF\n                self.flag_eof = True\n                raise pexpect.spawnbase.EOF(\n                    \"End Of File (EOF). Exception style platform.\"\n                )\n            raise\n        if s == b\"\":\n            # BSD-style EOF\n            self.flag_eof = True\n            raise pexpect.s",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:239-269"
    },
    "191": {
        "file_id": 22,
        "content": "This code modifies the `spawnbase_read_nonblocking` function to handle different types of EOF. It also adds a new function `spawn_sendline` that sends data with OS line separator and overrides `spawn_sendline` in `pexpect.spawn`. The code also includes an exception handling block for OSError.",
        "type": "comment"
    },
    "192": {
        "file_id": 22,
        "content": "pawnbase.EOF(\n                \"End Of File (EOF). Empty string style platform.\"\n            )\n        # s = self._decoder.decode(s, final=False)\n        self._log(s, \"read\")\n        return s\n    pexpect.spawnbase.SpawnBase.read_nonblocking = spawnbase_read_nonblocking\ndef get_repr(content):\n    if isinstance(content, str):\n        content = content.encode()\n    repr_content = content.hex()\n    len_content = len(repr_content) / 2\n    assert (\n        len_content % 1 == 0.0\n    ), f\"possible counting mechanism failure\\nnon-integral content length detected: {len_content}\"\n    len_content = int(len_content)\n    cut_len = 10\n    return f\"[{len_content}\\tbyte{'s' if len_content != 0 else ''}] {repr_content[:cut_len*2]}{'...' if len_content > cut_len else ''}\"\ntimezone_str = \"Asia/Shanghai\"\ntimezone = pytz.timezone(timezone_str)\ndef formatTimeAtShanghai(timestamp):\n    dt = datetime.datetime.fromtimestamp(timestamp, tz=timezone)\n    return dt.isoformat()\nclass NaiveActor:\n    write_method = lambda proc: proc.sendlin",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:269-303"
    },
    "193": {
        "file_id": 22,
        "content": "- EOF: End Of File, empty string style platform\n- read_nonblocking: function to read from a spawned process non-blockingly\n- get_repr: convert content to hex representation with byte count and truncation if needed\n- formatTimeAtShanghai: convert Unix timestamp to ISO format with Shanghai timezone\n- NaiveActor: class with write_method as proc.sendline lambda function",
        "type": "comment"
    },
    "194": {
        "file_id": 22,
        "content": "e\n    actorStatsClass = ActorStats\n    @staticmethod\n    def timeit(func):\n        def inner_func(self):\n            start_time = time.time()\n            # func(self)\n            try:\n                ret = func_timeout.func_timeout(self.max_loop_time, func, args=(self,))\n            except func_timeout.FunctionTimedOut:\n                print(\"Loop timeout %d exceeded.\" % self.max_loop_time)\n                return\n            finally:\n                end_time = time.time()\n                rw_time = end_time - start_time\n                print(\"rw time:\", rw_time, sep=\"\\t\")\n                if rw_time > self.max_rwtime:\n                    print(\n                        \"exit because of long rw time.\\nmax rw time:\", self.max_rwtime\n                    )\n                    return\n            return ret\n        return inner_func\n    def __init__(self, cmd, encoding=\"utf-8\"):\n        self.process = self.spawn(cmd)\n        self.encoding = encoding\n        if os.name == 'nt':\n            NT_CONTEXT['NT_ENCODING'] = enco",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:303-334"
    },
    "195": {
        "file_id": 22,
        "content": "This code defines a class with a method called \"timeit\" that measures the execution time of another function. It also initializes an instance variable \"process\" by spawning a new process using the provided command and encoding. If running on Windows (os.name == 'nt'), it sets NT_CONTEXT['NT_ENCODING'] to enco.",
        "type": "comment"
    },
    "196": {
        "file_id": 22,
        "content": "ding\n            win_expect_old = copy.copy(self.process.expect)\n            def win_expect_new(*args, **kwargs):\n                with nt_read_nonblocking_decode_context():\n                    return win_expect_old(*args, **kwargs)\n            self.process.expect = win_expect_new\n        self.timeout = SOCKET_TIMEOUT\n        self.max_loop_time = 3\n        self.max_init_time = 12\n        self.max_rwtime = 0.5\n        # self.timeout = 0.2 # equivalent to wexpect\n        # self.timeout = 0.001\n        # self.timeout = 1 # will cause havoc if set it too long\n        self.read_bytes = 0\n        self.write_bytes = 0\n        self.loop_count = 0\n        self.start_time = time.time()\n        self.read_head_bytes = 200\n        self.read_tail_bytes = 200\n        self.read_entropy_calc = ContentEntropyCalculator()\n        self.write_entropy_calc = ContentEntropyCalculator()\n        self._stats = ...\n        self.recent_loop_threshold = 300\n        \"\"\"\n        To limiting history data size for calculating recent statistic",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:334-358"
    },
    "197": {
        "file_id": 22,
        "content": "The code sets various attributes for managing timeouts, byte counts, and entropy calculations for an actor process. It also includes a comment explaining the purpose of one attribute to limit history data size for calculating recent statistics.",
        "type": "comment"
    },
    "198": {
        "file_id": 22,
        "content": "s.\n        \"\"\"\n    def spawn(self, cmd):\n        return pexpect.spawn(cmd)\n        # return pexpect.spawn(cmd, interact=True)  # will display\n    def write(self, content):\n        # if isinstance(content, bytes):\n        #     content = content.decode()\n        content = enforce_bytes(content)\n        print(\"write:\", get_repr(content), sep=\"\\t\")\n        self.write_bytes += len(content)\n        write_method = self.__class__.write_method(self.process)\n        write_method(content)\n        self.write_entropy_calc.count(content)\n        return content\n    def read(self):\n        # cannot read.\n        head_content = b\"\"\n        tail_content = deque([], maxlen=self.read_tail_bytes)\n        read_byte_len = 0\n        while True:\n            try:\n                kwargs = {}\n                if os.name == \"posix\":\n                    kwargs[\"timeout\"] = self.timeout\n                char = self.process.read_nonblocking(1, **kwargs)\n                # print('char:', char)\n                if isinstance(char, str):\n            ",
        "type": "code",
        "location": "/basic_interactive_program_emulation_and_image_with_docker_support/naive_actor.py:358-391"
    },
    "199": {
        "file_id": 22,
        "content": "This code defines a class with three methods: spawn, write, and read. The spawn method starts a new process using pexpect's spawn function with the given command. The write method writes content to the process' stdin, keeping track of the total bytes written, updating entropy calculation, and displaying the content being written. The read method attempts to continuously read nonblocking bytes from the process' stdout until a timeout or EOF is reached.",
        "type": "comment"
    }
}