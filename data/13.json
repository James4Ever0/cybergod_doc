{
    "1300": {
        "file_id": 169,
        "content": ",\n          middle_attn_type)\n      return [\n          tl.Dup(),\n          tl.ShiftRight(current_sf - 1, mode=mode), shortening_layer,\n          pre_stage_blocks, *create_hourglass_valley(\n              rest_shorten_factors[1:], rest_n_funnel_blocks[1:],\n              current_total_pooling * current_sf), post_stage_blocks,\n          upsampling_layer,\n          tl.LayerNorm(),\n          tl.Add()\n      ]\n    else:\n      blocks = create_decoder_blocks(current_n_layers,\n                                     current_total_pooling * current_sf,\n                                     middle_attn_type)\n      return [\n          tl.Dup(),\n          tl.ShiftRight(current_sf - 1), shortening_layer, blocks,\n          upsampling_layer,\n          tl.LayerNorm(),\n          tl.Add()\n      ]\n  pre_decoder_blocks = create_decoder_blocks(n_pre_decoder_blocks, 1,\n                                             vanilla_attn_type)\n  post_decoder_blocks = create_decoder_blocks(n_post_decoder_blocks, 1,\n                                      ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:269-299"
    },
    "1301": {
        "file_id": 169,
        "content": "This code defines a function to create a decoder network. It takes in parameters such as the number of layers, shortening factor, attention type, and other variables. Depending on these inputs, it creates different blocks for the decoder. If the \"middle_attn_type\" is not \"hourglass\", it creates a set of decoder blocks with specified parameters. Otherwise, it creates a more complex hourglass-shaped architecture with additional stages and blocks. The result is a list of layers that make up the decoder network.",
        "type": "comment"
    },
    "1302": {
        "file_id": 169,
        "content": "        vanilla_attn_type)\n  valley = create_hourglass_valley(hierarchy_shorten_factors,\n                                   hierarchy_n_layers, 1)\n  # Assemble and return the model.\n  return tl.Serial(  # tokens (or chunked tuple of tokens)\n      tl.ShiftRight(mode=mode),  # toks\n      token_encoder,  # vecs\n      pre_decoder_blocks,  # vecs\n      valley,  # shortened vecs\n      post_decoder_blocks,  # vecs\n      tl.Dense(vocab_size),  # vecs\n  )",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/hourglass_lm.py:299-312"
    },
    "1303": {
        "file_id": 169,
        "content": "Creates a model for hourglass language modeling, assembles it and returns.",
        "type": "comment"
    },
    "1304": {
        "file_id": 170,
        "content": "/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py",
        "type": "filepath"
    },
    "1305": {
        "file_id": 170,
        "content": "The code uses \"cl100k_base\" encoding to separate actions from content, defines a function 'l_tokens' for custom special tokens based on direction, value type, and index, and discusses optimization techniques such as allowing AI to emit multiple tokens and considering compression techniques.",
        "type": "summary"
    },
    "1306": {
        "file_id": 170,
        "content": "import tiktoken\n# are you sure you can encode everything? what about bytes?\n# why don't you just encode some bytes to the vocabulary?\n# don't worry we will handle that.\ncl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n# you can separate action from content.\n# we can compare the difference. eventually we will find out which is best and most performant.\n# like: [keyboard_input_start] [char1] [char2] [keyboard_input_end]\n# or: [keyboard_input] [char1] [keyboard_input] [char2]\n# compared to: [keyboard a] [keyboard upper a] [mouse move x 10] [mouse move y 10]\n# well these are some 'action' vocabularies worth learning.\n# how many special tokens can we add?\n# In production, load the arguments directly instead of accessing private attributes\n# See openai_public.py for examples of arguments for specific encodings\n# builtin special tokens: {'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276}\n# max token value: 100276\ndef make_specia",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py:1-27"
    },
    "1307": {
        "file_id": 170,
        "content": "Code imports the \"cl100k_base\" encoding from the tiktoken library, which allows encoding of text. The code aims to separate actions from content and compare different ways to represent them. It mentions loading arguments for specific encodings and includes built-in special tokens.",
        "type": "comment"
    },
    "1308": {
        "file_id": 170,
        "content": "l_tokens(token_name_list: list[str], start):\n    result = {}\n    for token_name in token_name_list:\n        result[token_name] = start\n        start += 1\n    return result\nbuild_special_token = lambda token_def: f\"<|{token_def}|>\"\ncustom_special_tokens = [build_special_token(\"my_new_token\")]\nfor direction in 'x', 'y':\n    for val_type in 'pos', 'neg':\n        for i in range(1024):\n            tk = build_special_token(f'{direction}_{val_type}_{i}')\n            custom_special_tokens.append(tk)\nenc = tiktoken.Encoding(\n    # If you're changing the set of special tokens, make sure to use a different name\n    # It should be clear from the name what behaviour to expect.\n    name=\"cl100k_im\",\n    pat_str=cl100k_base._pat_str,\n    mergeable_ranks=(mrs := cl100k_base._mergeable_ranks),\n    special_tokens={\n        **make_special_tokens(\n            list(cl100k_base._special_tokens.keys()) + custom_special_tokens,\n            start=max(mrs.values()) + 1,\n        )\n        # **cl100k_base._special_tokens,\n        # \"<|im",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py:27-56"
    },
    "1309": {
        "file_id": 170,
        "content": "This code defines a function 'l_tokens' that creates a dictionary with special tokens and their positions. It uses a lambda function, 'build_special_token', to create special token strings. The list of custom special tokens is generated from various combinations of direction, value type, and index. The encoding for an 'tiktoken' instance is then defined using this list of custom special tokens along with the existing set of special tokens.",
        "type": "comment"
    },
    "1310": {
        "file_id": 170,
        "content": "_start|>\": 100_264,\n        # \"<|im_end|>\": 100_265,\n        # \"<|my_new_token|>\": 200_000,\n    },\n)\n# 100255 is the max token number in mergeable ranks\n# you can add new tokens.\nprint(f\"{cl100k_base.max_token_value:=}\")\nprint(f\"{cl100k_base.n_vocab:=}\")\n# cl100k_base.special_tokens_set\n# breakpoint()\nencode_bytes_target = bytes([x for x in range(255)])\n# you could use unicode regex to encode bytes\n# tweaking `_encode_single_piece`, `_encode_only_native_bpe` under tiktoken.core.Encoding\ntext_target = f\"hello world{custom_special_tokens[0]}my name is andy{build_special_token('x_pos_1000')}\"\ntokens = enc.encode(\n    text_target, disallowed_special=()\n)  # this will pass but will not be converted into special tokens.\n# can we train the model new tokens by using different encodings? this could introduce duality.\n# you can inform the model about the encoding. so the model might not misbehave.\n# tokens = enc.encode(text_target, allowed_special={custom_special_tokens[0]})\n# tokens = enc.encode(text_target, allowe",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py:56-83"
    },
    "1311": {
        "file_id": 170,
        "content": "This code is initializing an encoding object and testing the encoding of a text string containing special tokens. The `cl100k_base` represents a predefined vocabulary, which has a maximum token number of 100255. The code is trying to encode a target text that includes custom special tokens (e.g., \"<|my_new_token|>\"), and the encoding object handles converting those tokens into their respective values.",
        "type": "comment"
    },
    "1312": {
        "file_id": 170,
        "content": "d_special={custom_special_tokens[0]}, disallowed_special = ())\ntokens_with_special = enc.encode(\n    text_target, allowed_special=\"all\", disallowed_special=()\n)\nprint(tokens)  # no special token!\nprint(tokens_with_special)\n# what if i allow the ai to emit multiple tokens a time?\n# i will sort the \"simutaneous\" tokens and order by priority\n# what about training? is that purely online? or shall we alter the training method?\n# like: [a,b,c,d,e,f] -> [a,c,e], [b,d,f] -> sample by priority\n# this is compression. this can speed up things. but not necessarily improve quality, unless you trade performance with quality.\n# or you could augment the training data, like input = x[:-3], target = x[3:]\n# either way, could possibly optimize the performance.\n# to use more tokens you need to change the first embedding layer",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py:83-99"
    },
    "1313": {
        "file_id": 170,
        "content": "The code is discussing the possibility of allowing the AI to emit multiple tokens at a time and how it could potentially impact performance and quality. It suggests considering compression techniques or augmenting training data as potential optimization strategies, but also mentions that changing the first embedding layer would be required to use more tokens.",
        "type": "comment"
    },
    "1314": {
        "file_id": 171,
        "content": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py",
        "type": "filepath"
    },
    "1315": {
        "file_id": 171,
        "content": "The code presents a hierarchical tokenization transformer model for text classification using Einops library. It incorporates multiple attention layers and sparse transforms for feature extraction, with a `sparseTransformerForward` function.",
        "type": "summary"
    },
    "1316": {
        "file_id": 171,
        "content": "# bytes -> char -> \"token\" -> sentences -> paragraphs\n# what to do in reverse? must recurse into bytes.\n# how to encode and decode?\nfrom torch import nn\nfrom torch import Tensor\nimport torch\nfrom typing import Literal\n# Convert bytes to binary representation as a list of integers using bitwise shift\ndef bytes_to_binary_int(byte_string):\n    binary_representation = [\n        int((byte_string[i] >> j) & 1)\n        for i in range(len(byte_string))\n        for j in range(7, -1, -1)\n    ]\n    return binary_representation\n# Example usage\nbyte_string = b\"hello world\"\nbinary_representation_int = bytes_to_binary_int(byte_string)\nprint(binary_representation_int)\n# you can pad with zeros when using bitlevel tokenizer\n# but how do you shift? char level or bit level? neither? self-determined?\n# encode:\n# nx2 * 2*d -> n*d\n# (nxn = nxd * dxn) * nxd = nxd\n# nxd -> convolution -> mxd, m = n/2\n# (mxm = mxd * dxm) * mxd = mxd\n# decode:\n# mxd -> deconvolution -> nxd\n# nxd * dx2 -> nx2\n# first, pad the output ahead of time.\n# # so for ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:1-43"
    },
    "1317": {
        "file_id": 171,
        "content": "Code converts bytes to binary representation as a list of integers using bitwise shift, then discusses encoding and decoding concepts for hierarchical tokenization.",
        "type": "comment"
    },
    "1318": {
        "file_id": 171,
        "content": "every bit in the byte, we have a vector of 768 dimensions\n# # try to reduce the bytes.\n# we've got four level of abstractions.\nfrom pydantic import BaseModel\nclass TransformerArguments(BaseModel, arbitrary_types_allowed=True):\n    key_padding_mask: Tensor | None = None\n    need_weights: bool = True\n    attn_mask: Tensor | None = None\n    average_attn_weights: bool = True\n    is_causal: bool = False\nclass MultiheadSelfAttentionStack(nn.Module):\n    def __init__(self, embed_dim: int, num_heads: int, num_layers: int, **kwargs):\n        super(MultiheadSelfAttentionStack, self).__init__()\n        self.layers = nn.ModuleList(\n            [\n                nn.MultiheadAttention(\n                    embed_dim=embed_dim, num_heads=num_heads, **kwargs\n                )\n                for _ in range(num_layers)\n            ]\n        )\n    def forward(self, input_tensor: Tensor, transformerArguments: TransformerArguments):\n        output = input_tensor\n        for layer in self.layers:\n            output, _ = layer(\n    ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:43-75"
    },
    "1319": {
        "file_id": 171,
        "content": "This code defines a `MultiheadSelfAttentionStack` class which is a module for self-attention layers. It has multiple layers of `nn.MultiheadAttention`, each with a specific number of heads and embed_dim. The `forward` method iterates over these layers to process the input tensor. The `TransformerArguments` class is used to specify optional arguments like `key_padding_mask`, `attn_mask`, etc.",
        "type": "comment"
    },
    "1320": {
        "file_id": 171,
        "content": "            query=output, key=output, value=output, **transformerArguments.dict()\n            )\n        return output\n# what if i use the inverse causal mask? backtracking mask?\n# import torch\n# a = torch.tril(torch.ones(10, 10))  # Create a lower triangular mask of ones\n# b = torch.tril(a.T)  # Transpose the mask and then take the lower triangular part to ensure backtracking\n# c = torch.tril(torch.flip(a, dims=[1]))  # Flip the mask along the horizontal axis and take the lower triangular part\n# d = torch.tril(torch.flip(a, dims=[0]))  # Flip the mask along the vertical axis and take the lower triangular part\nimport einops\nimport torch.nn.functional as F\n# hourglass replicate? not exactly. this is binary.\n# what about moe? lsm?\nclass HierachicalTokenizationTransformer(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=4, num_layers=1, abstraction_level=4):\n        # of course this is causal.\n        super().__init__()\n        self.TWO = 2\n        self.ONE = 1\n        self.ZERO = 0\n        self.bi",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:75-101"
    },
    "1321": {
        "file_id": 171,
        "content": "Code is defining a `HierachicalTokenizationTransformer` class for text classification tasks using hierarchical tokenization and a transformer model architecture. The code also includes functions to create causal, inverse causal, and backtracking masks for the attention mechanism in the transformer layers.",
        "type": "comment"
    },
    "1322": {
        "file_id": 171,
        "content": "nary_embedding = nn.Embedding(\n            num_embeddings=self.TWO, embedding_dim=embed_dim\n        )\n        self.abstractionLayers = []\n        self.deabstractionLayers = []\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n        self.abstraction_level = abstraction_level\n        assert abstraction_level > 0, \"abstraction level must be greater than zero\"\n        self.mainTransformer = MultiheadSelfAttentionStack(\n            embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers\n        )\n        for _ in range(abstraction_level - 1):\n            # for _ in range(abstraction_level):\n            # Create the attention and abstraction layers\n            att_layer = MultiheadSelfAttentionStack(\n                embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers\n            )\n            abstract_layer = nn.Linear(self.TWO, self.ONE)\n            self.abstractionLayers.append(\n                att_layer\n            )  # Add the atte",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:101-127"
    },
    "1323": {
        "file_id": 171,
        "content": "This code initializes an abstraction-level transformer model with multiple attention and abstraction layers. The `nary_embedding` is an embedding layer, and the `abstractionLayers` and `deabstractionLayers` will contain these layers for different abstraction levels. The code also asserts that the abstraction level must be greater than zero.",
        "type": "comment"
    },
    "1324": {
        "file_id": 171,
        "content": "ntion layer to the list\n            self.abstractionLayers.append(\n                abstract_layer\n            )  # Add the abstraction layer to the list\n            # Create the inverse attention and deabstraction layers\n            datt_layer = MultiheadSelfAttentionStack(\n                embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers\n            )\n            deabstract_layer = nn.Linear(self.ONE, self.TWO)\n            self.deabstractionLayers.append(\n                datt_layer\n            )  # Add the inverse attention layer to the list\n            self.deabstractionLayers.append(\n                deabstract_layer\n            )  # Add the deabstraction layer to the list\n        self.decode_embedding = nn.Linear(self.embed_dim, self.TWO)\n        self.pad_size = self.TWO**abstraction_level\n    def _sparseTransformerForwardImpl(\n        self,\n        embedding: Tensor,\n        transformer: MultiheadSelfAttentionStack,\n        transformerArguments: TransformerArguments,\n    ):\n        embeddin",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:127-153"
    },
    "1325": {
        "file_id": 171,
        "content": "Appending abstraction and deabstraction layers to the list.\nCreating inverse attention and deabstraction layers, adding them to the list.\nDefining decode embedding layer and setting pad size.\nImplementing sparseTransformerForward function with parameters.",
        "type": "comment"
    },
    "1326": {
        "file_id": 171,
        "content": "g = einops.rearrange(embedding, \"b (s1 g) d -> (b g) s1 d\", g=self.TWO)\n        embedding = transformer(embedding, transformerArguments=transformerArguments)\n        embedding = einops.rearrange(embedding, \"(b g) s1 d -> b (s1 g) d\", g=self.TWO)\n        return embedding\n    def sparseTransformerForward(\n        self,\n        embedding: Tensor,\n        transformer: MultiheadSelfAttentionStack,\n        transformerArguments: TransformerArguments,\n        use_sliding=True,\n    ):\n        _embedding = self._sparseTransformerForwardImpl(\n            embedding, transformer, transformerArguments\n        )\n        if use_sliding:\n            slide_embedding = self.sparseTransformerForward(\n                embedding[:, 1:-1, :],\n                transformer,\n                transformerArguments,\n                use_sliding=False,\n            )\n            _embedding[:, 1:-1, :] = (_embedding[:, 1:-1, :] + slide_embedding) / 2\n        return _embedding\n    def calculateInputPadSizeFromSequenceLength(self, sequence_lengt",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:153-178"
    },
    "1327": {
        "file_id": 171,
        "content": "This code defines a function `sparseTransformerForward` that applies the transformer model to an input embedding. It also includes a helper function `_sparseTransformerForwardImpl` for internal use. If the `use_sliding` parameter is set, it applies the transformer model to non-boundary elements and averages them with boundary elements to reduce boundary effects. The code also includes another incomplete function `calculateInputPadSizeFromSequenceLength`.",
        "type": "comment"
    },
    "1328": {
        "file_id": 171,
        "content": "h: int):\n        input_pad_size = []\n        msequence_length = int(sequence_length)\n        for _ in range(self.abstraction_level):\n            div, mod = divmod(msequence_length, self.TWO)\n            if mod == self.ONE:\n                input_pad_size.append(self.ONE)\n                div += self.ONE\n            else:\n                input_pad_size.append(self.ZERO)\n            msequence_length = div\n        return input_pad_size\n    def padEmbedding(self, embedding: Tensor, pad_direction: Literal[\"left\", \"right\"]):\n        embedding = F.pad(\n            embedding,\n            (self.ZERO, self.ZERO, self.ZERO, self.ONE)\n            if pad_direction == \"right\"\n            else (self.ZERO, self.ZERO, self.ONE, self.ZERO),\n            \"constant\",\n            self.ZERO,\n        )\n        return embedding\n    def chopEmbedding(self, embedding: Tensor, pad_direction: Literal[\"left\", \"right\"]):\n        embedding = (\n            embedding[:, : -self.ONE, :]\n            if pad_direction == \"right\"\n            else embe",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:178-206"
    },
    "1329": {
        "file_id": 171,
        "content": "This code is part of a hierarchical tokenization model. It calculates the input padding size for each abstraction level, pads and chops embeddings based on the padding direction specified.",
        "type": "comment"
    },
    "1330": {
        "file_id": 171,
        "content": "dding[:, self.ONE :, :]\n        )\n        return embedding\n    def abstractionForward(self, embedding: Tensor, abstractionLayer: nn.Linear):\n        embedding = einops.rearrange(embedding, \"b (s1 g) d -> b s1 d g\", g=self.TWO)\n        embedding = abstractionLayer(embedding)  # Apply attention and abstraction\n        embedding = einops.rearrange(embedding, f\"b s d {self.ONE} -> b s d\")\n        return embedding\n    def deabstractionForward(self, embedding: Tensor, deabstractionLayer: nn.Linear):\n        embedding = einops.rearrange(embedding, f\"b s d -> b s d {self.ONE}\")\n        embedding = deabstractionLayer(embedding)\n        embedding = einops.rearrange(embedding, \"b s1 d g -> b (s1 g) d\", g=self.TWO)\n        return embedding\n    def forward(\n        self,\n        input_logits: Tensor,\n        pad_direction: Literal[\"left\", \"right\"],\n        transformerArguments: TransformerArguments,\n    ):  # it is trimmed from one side. is it causal?\n        assert (\n            len(input_logits.shape) == self.TWO\n   ",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:206-230"
    },
    "1331": {
        "file_id": 171,
        "content": "This code defines a class that seems to be part of a transformer-based model. It has three methods: `tokenizationForward`, `abstractionForward`, and `deabstractionForward`. The class takes in an input tensor, pads it from one side (specified by `pad_direction`), and applies transformations based on the provided `transformerArguments`. The code uses the Einops library for array manipulations.",
        "type": "comment"
    },
    "1332": {
        "file_id": 171,
        "content": "     ), \"input logits shall be of shape (batch_size, sequence_length)\"\n        _, sequence_length = input_logits.shape\n        assert sequence_length != self.ZERO, \"zero length sequence encountered\"\n        input_pad_size = self.calculateInputPadSizeFromSequenceLength(sequence_length)\n        residual_conn = []\n        embedding = self.binary_embedding(input_logits)\n        for i in range(\n            self.ZERO, len(self.abstractionLayers), self.TWO\n        ):  # Step through every other layer\n            lookup_index = i // self.TWO\n            if input_pad_size[lookup_index] == self.ONE:  # either 1 or 0\n                embedding = self.padEmbedding(embedding, pad_direction)\n            embedding = self.sparseTransformerForward(\n                embedding, self.abstractionLayers[i], transformerArguments\n            )\n            residual_conn.append(embedding)\n            embedding = self.abstractionForward(\n                embedding, self.abstractionLayers[i + self.ONE]\n            )\n        # basicall",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:230-249"
    },
    "1333": {
        "file_id": 171,
        "content": "The code is performing hierarchical tokenization by iterating through every other layer in the abstraction layers, applying sparse transformer forward and abstraction forward operations, and storing the residual connections.",
        "type": "comment"
    },
    "1334": {
        "file_id": 171,
        "content": "y: n -> 2*n - mod\n        if input_pad_size[-1] == self.ONE:\n            embedding = self.padEmbedding(embedding, pad_direction)\n        embedding = self.sparseTransformerForward(\n            embedding, self.mainTransformer, transformerArguments\n        )\n        if input_pad_size[-1] == self.ONE:\n            embedding = self.chopEmbedding(embedding, pad_direction)\n        for i in range(\n            self.ZERO, len(self.deabstractionLayers), self.TWO\n        ):  # Step through every other layer\n            lookup_index = self.abstraction_level - i // self.TWO - self.TWO\n            embedding = self.deabstractionForward(\n                embedding, self.deabstractionLayers[i + self.ONE]\n            )\n            embedding += residual_conn[lookup_index]\n            embedding = self.sparseTransformerForward(\n                embedding, self.deabstractionLayers[i], transformerArguments\n            )\n            if input_pad_size[lookup_index] == self.ONE:\n                embedding = self.chopEmbedding(embedding",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:249-269"
    },
    "1335": {
        "file_id": 171,
        "content": "This code performs hierarchical tokenization using a sparse transformer model. It applies padding and chopping embeddings when needed, and iterates through every other layer of deabstraction layers for feature extraction.",
        "type": "comment"
    },
    "1336": {
        "file_id": 171,
        "content": ", pad_direction)\n        output_logits = self.decode_embedding(embedding)\n        return output_logits\n# myTransformer = HierachicalTokenizationTransformer()\n# myTransformer = HierachicalTokenizationTransformer(abstraction_level=5, num_layers=10)\n# myTransformer = HierachicalTokenizationTransformer(abstraction_level=100, num_layers=2)\nmyTransformer = HierachicalTokenizationTransformer(abstraction_level=20, num_layers=2)\n# input_data = torch.ones(20, 1000, dtype=torch.long)  # batch size: 20, sequence length: 30\ninput_data = torch.ones(20, 30, dtype=torch.long)  # batch size: 20, sequence length: 30\noutput_data = myTransformer.forward(\n    input_data,\n    pad_direction=\"right\",\n    transformerArguments=TransformerArguments(is_causal=True),\n)\nprint(output_data.shape)  # 20, 30, 2",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py:269-286"
    },
    "1337": {
        "file_id": 171,
        "content": "This code is initializing a HierachicalTokenizationTransformer model with specific abstraction level and number of layers. It then takes input data, applies the transformer's forward function, and prints the output shape.",
        "type": "comment"
    },
    "1338": {
        "file_id": 172,
        "content": "/rt_x_experiments/special_tokenizer_with_actions/test_simutaneous_tokenization_embedding.py",
        "type": "filepath"
    },
    "1339": {
        "file_id": 172,
        "content": "The code tackles resolving orderings in words and actions, offering various methods to combine embeddings such as elementwise addition, FFT+IFFT or separate transformer layers with linear decoders. It then passes through a transformer and decodes into two logits.",
        "type": "summary"
    },
    "1340": {
        "file_id": 172,
        "content": "# words have orderings.\n# but actions have not. usually stacked.\n# how to resolve this issue?\n# if you can decode the embedding using multiple embedding layers, what happens?\n# one embedding -> multiple meanings\n# for every embedding there is one silent token, allow to train separately.\n# fourier transform to concat these different kinds of embeddings\n# or you could train some decision embedding, to control the action type\nimport torch\nemb = torch.nn.Embedding\nemb2 = torch.nn.Embedding\n# the bot told us that we need to process different token separately.\n# you could have more choices on fft/ifft and more. you can first concat then use some dense layer to reduce its size.\n# choice 1: emb1+emb2 (elementwise addition) -> transformer -> decode by two linear layers and compare original logits\n# choice 2: fft(emb1)+fft(emb2) -> ifft -> transformer -> decode by two linear layers and compare original logits\n# choice 3: emb1 -> tranformer1 -> linear1 -> logits1; emb2 -> tranformer1 -> linear2 -> logits2\n# choi",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_simutaneous_tokenization_embedding.py:1-22"
    },
    "1341": {
        "file_id": 172,
        "content": "The code is discussing the issue of resolving orderings in words and actions, and potential methods for embedding them together. The author proposes several choices for combining embeddings from different token types, such as elementwise addition, Fast Fourier Transform (FFT) followed by Inverse FFT (IFFT), or using separate transformer layers with linear layers to decode the embeddings.",
        "type": "comment"
    },
    "1342": {
        "file_id": 172,
        "content": "ce 4: concat emb1&emb2 -> transformer -> decode -> separate into two logits",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_simutaneous_tokenization_embedding.py:22-22"
    },
    "1343": {
        "file_id": 172,
        "content": "Combines embeddings, passes through transformer, and decodes into two separate logits.",
        "type": "comment"
    },
    "1344": {
        "file_id": 173,
        "content": "/rt_x_experiments/special_tokenizer_with_actions/test_tokenmonster.py",
        "type": "filepath"
    },
    "1345": {
        "file_id": 173,
        "content": "Loading and tokenizing text using TokenMonster with a specific vocabulary.",
        "type": "summary"
    },
    "1346": {
        "file_id": 173,
        "content": "import tokenmonster\n# # Optionally set the tokenmonster directory, otherwise it will use ~/_tokenmonster\n# tokenmonster.set_local_directory(\"/path/to/preferred\")\n# Load a vocabulary by name, filepath or URL\nvocab = tokenmonster.load(\"english-24000-consistent-v1\") # cannot download.\n# Tokenize some text\ntext = \"Some text to turn into token IDs.\"\ntokens = vocab.tokenize(text)\nprint('tokens', tokens)",
        "type": "code",
        "location": "/rt_x_experiments/special_tokenizer_with_actions/test_tokenmonster.py:1-12"
    },
    "1347": {
        "file_id": 173,
        "content": "Loading and tokenizing text using TokenMonster with a specific vocabulary.",
        "type": "comment"
    },
    "1348": {
        "file_id": 174,
        "content": "/rt_x_experiments/zoom_pan_action.py",
        "type": "filepath"
    },
    "1349": {
        "file_id": 174,
        "content": "Code performs image cropping based on location and size, with support for relative movement in X and Y directions.",
        "type": "summary"
    },
    "1350": {
        "file_id": 174,
        "content": "# since we can only handle specific data in the scope, how about take transformations as actions\n# distinguish between discrete and continuous actions, like keyboard strokes, clicks and movements.\nimport numpy as np\ndef crop_at_location(img, location, size):\n    lx, ly = location\n    rx, ry = lx + size, ly + size\n    cropped_img = img[:, lx:rx, ly:ry]\n    return cropped_img\ndef relative_move_crop(img, location, size, dx, dy):\n    lx, ly = location\n    nx, ny = lx + dx, ly + dy\n    new_location = (nx, ny)\n    cropped_img = crop_at_location(img, new_location, size)\n    return cropped_img\nif __name__ == \"__main__\":\n    img = np.zeros((3, 1920, 1080))\n    cropped_img = crop_at_location(img, (10, 10), 224)\n    print(\"%s -> %s\" % (img.shape, cropped_img.shape))",
        "type": "code",
        "location": "/rt_x_experiments/zoom_pan_action.py:1-25"
    },
    "1351": {
        "file_id": 174,
        "content": "Code performs image cropping based on location and size, with support for relative movement in X and Y directions.",
        "type": "comment"
    },
    "1352": {
        "file_id": 175,
        "content": "/run_gpu_docker.sh",
        "type": "filepath"
    },
    "1353": {
        "file_id": 175,
        "content": "Testing GPU availability and NVIDIA driver version within the Docker container.",
        "type": "summary"
    },
    "1354": {
        "file_id": 175,
        "content": "docker run --rm -it --gpus 1 --runtime nvidia ubuntu:22.04 nvidia-smi # working!",
        "type": "code",
        "location": "/run_gpu_docker.sh:1-1"
    },
    "1355": {
        "file_id": 175,
        "content": "Testing GPU availability and NVIDIA driver version within the Docker container.",
        "type": "comment"
    },
    "1356": {
        "file_id": 176,
        "content": "/screenshot_test.py",
        "type": "filepath"
    },
    "1357": {
        "file_id": 176,
        "content": "This code imports the mss library to take a screenshot and timeit library for timing execution. It then defines a test_screenshot function that grabs a screenshot from monitor 0 and prints it, executing the function 10 times using timeit and printing the output (0.03 seconds).",
        "type": "summary"
    },
    "1358": {
        "file_id": 176,
        "content": "import mss\nimport timeit\ndef test_screenshot():\n    with mss.mss() as m:\n        img = m.grab(m.monitors[0])\n        print(img)\noutput = timeit.timeit(test_screenshot, number=10)\nprint(output)  # 0.03 seconds",
        "type": "code",
        "location": "/screenshot_test.py:1-12"
    },
    "1359": {
        "file_id": 176,
        "content": "This code imports the mss library to take a screenshot and timeit library for timing execution. It then defines a test_screenshot function that grabs a screenshot from monitor 0 and prints it, executing the function 10 times using timeit and printing the output (0.03 seconds).",
        "type": "comment"
    },
    "1360": {
        "file_id": 177,
        "content": "/software_capture_hid_control/Makefile",
        "type": "filepath"
    },
    "1361": {
        "file_id": 177,
        "content": "This Makefile is responsible for building and executing the Python script \"test_control.py\". It first compiles the script using a template, then runs the compiled version with Python interpreter.",
        "type": "summary"
    },
    "1362": {
        "file_id": 177,
        "content": "main: test_control.py\n\t${PYTHON} test_control.py\ntest_control.py: test_control.py.j2 $(addprefix ../, ${RENDERED_CODE})\n\t${PYTHON} ../render_python_code.py test_control.py",
        "type": "code",
        "location": "/hardware_capture_hid_power_control/Makefile:1-5"
    },
    "1363": {
        "file_id": 177,
        "content": "This Makefile is responsible for building and executing the Python script \"test_control.py\". It first compiles the script using a template, then runs the compiled version with Python interpreter.",
        "type": "comment"
    },
    "1364": {
        "file_id": 178,
        "content": "/software_capture_hid_control/abstract_class_test.py",
        "type": "filepath"
    },
    "1365": {
        "file_id": 178,
        "content": "Defining an abstract class Example with two abstract methods a and b.",
        "type": "summary"
    },
    "1366": {
        "file_id": 178,
        "content": "from abc import ABC, abstractmethod, ABCMeta\n# class Example(ABC):\nclass Example(ABC):\n    @abstractmethod\n    def a(self):\n        print('a')\n    @abstractmethod\n    def b(self):\n        print('a')\ne = Example()",
        "type": "code",
        "location": "/software_capture_hid_control/abstract_class_test.py:1-13"
    },
    "1367": {
        "file_id": 178,
        "content": "Defining an abstract class Example with two abstract methods a and b.",
        "type": "comment"
    },
    "1368": {
        "file_id": 179,
        "content": "/software_capture_hid_control/test_control.py",
        "type": "filepath"
    },
    "1369": {
        "file_id": 179,
        "content": "The code utilizes multiple control methods, libxdoHID for keyboard & mouse events, and X11 protocol for live streaming, while struggling to save monitor images using mss and xvfb backend.",
        "type": "summary"
    },
    "1370": {
        "file_id": 179,
        "content": "# TODO: more control methods (non-hardware) under way\n# vnc/rdp (rdpy3(py2), rdpy3, python3-aardwolf, rdesktop (rdp)) (docker-vnc image: dorowu/ubuntu-desktop-lxde-vnc:focal ; docker rdp image: scottyhardy/docker-remote-desktop)\n# ssh (terminal interface)\n# spice (remmina, remote-viewer (RHEL))\n# xvfb (with pyautogui?) (use vglrun (GPU)) (what alternatives to xvfb are for macOS and Windows?)\n# -----------[use remote control methods as self control methods]-----------\n# self control (pyautogui, pynput, (win)tty, tmux, subprocess, ttyd with electron/xvfb based browser client)\n# qtpy: PyQt5/5/6 abstraction layer\n# https://github.com/spyder-ide/qtpy\n# docker-wine image (in case running windows app on linux): scottyhardy/docker-wine\n# MineRL GPU rendering: https://minerl.readthedocs.io/en/latest/notes/performance-tips.html\n# rdpy3: https://github.com/massimiliano-dalcero/rdpy\n# ref: https://github.com/citronneur/rdpy/issues/91\n# shall you look over our previous project lazero/metalazero\n# unittest for xr",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:1-21"
    },
    "1371": {
        "file_id": 179,
        "content": "This code snippet appears to be a TODO list for adding more control methods to a software system. These methods include VNC/RDP, SSH, Spice, Xvfb with pyautogui and alternatives on macOS and Windows, remote control methods as self control methods using PyQt5, Docker Wine image, MineRL GPU rendering, rdpy3 library, and unittesting for xr.",
        "type": "comment"
    },
    "1372": {
        "file_id": 179,
        "content": "dp:\n# 1. run docker in fullscreen mode, run background keylogger first, then accept inputs through rdp.\n# 2. run some full screen app on windows (virtualbox), along with keylogger.\nimport sys\nsys.path.append(\"../\")\nfrom beartype import beartype\nfrom conscious_struct import HIDActionTypes\nfrom typing import List, Tuple, Union, TYPE_CHECKING\nimport time\nif TYPE_CHECKING:\n    from ..hid_utils import *\nelse:\n    from hid_utils import *\nif sys.version_info >= (3, 11):\n    from enum import StrEnum\nelse:\n    from strenum import StrEnum\nfrom enum import auto\n# TODO: test this under py3.9/3.10\nclass ControlMethod(StrEnum):\n    xvfb = auto()\n# breakpoint()\nclass Xdotool(StrEnum):\n    libxdo = auto()\n    xdoctl = auto()\n    pyxdotool = auto()\n    xdotool_jordan = auto()\n    xdotool_tlaloc = auto()\n    xdotool_cli = auto()  # no external library, work by hand.\ncontrolMethod = ControlMethod.xvfb\nxdt = Xdotool.libxdo\nif controlMethod == ControlMethod.xvfb:\n    # instead use:\n    # [xdotool](https://github.com/jordansissel/xdotool)\n ",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:21-70"
    },
    "1373": {
        "file_id": 179,
        "content": "This code is setting up variables and enums for controlling a fullscreen application through xdotool or another control method. The chosen control method (Xvfb) will be used to interact with the fullscreen app, and the selected xdotool implementation (libxdo) will perform the actions.",
        "type": "comment"
    },
    "1374": {
        "file_id": 179,
        "content": "   # [python-libxdo](https://pypi.org/project/python-libxdo/)\n    # [xdotool python wrapper](https://github.com/Tlaloc-Es/xdotool)\n    # [python-xdoctl](https://pypi.org/project/python-xdoctl/)\n    # [pyxdotool](https://github.com/cphyc/pyxdotool)\n    # -------------[AND NOW FOR SOMETHING COMPLETELY DIFFERENT]---------------\n    # [bezmouse](https://github.com/vincentbavitz/bezmouse) might help you evade bot checks, but it is up to you to **compress** user mouse coordinates. maybe just average out tracks per action frame? you name it.\n    # also compress key events?\n    # another story please...\n    # think of some abstract class, which all implementations follow.\n    # think of \"HIDBase\" instead of your imagination. just follow existing guidelines.\n    if xdt == Xdotool.libxdo:\n        import xdo\n        def xdo_del(self):\n            try:\n                xdo._libxdo.xdo_free(self._xdo)\n            except:  # python shutting down. just ignore this.\n                print(\"Unable to free xdo object. Li",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:70-89"
    },
    "1375": {
        "file_id": 179,
        "content": "The code is importing the libxdo library and creating a function to delete xdo objects when they are no longer needed. It also includes references to other related libraries and tools for interacting with mouse and keyboard events.",
        "type": "comment"
    },
    "1376": {
        "file_id": 179,
        "content": "kely Python is shutting down.\")\n        xdo.Xdo.__del__ = xdo_del\n        @beartype\n        class LibxdoHID(HIDInterface):\n            def __init__(\n                self,\n            ):\n                self.xdo = xdo.Xdo()\n            def getButtonIdFromButtonLiteral(\n                self, button_literal: HIDActionTypes.mouse_buttons\n            ):\n                # Generally, 1 is left, 2 is middle, 3 is right, 4 is wheel up, 5 is wheel down.\n                translation_map = {\n                    \"Button.left\": 1,\n                    \"Button.middle\": 2,\n                    \"Button.right\": 3,\n                }\n                button_id = translation_map[button_literal]\n                return button_id\n            def getKeySequenceFromKeyLiteral(\n                self, key_literal: HIDActionTypes.keys\n            ) -> Union[None, str]:\n                keysequence = key_literal_to_xk_keysym(key_literal)\n                return keysequence  # not joined by \"+\"\n            def _key_release(self, key_literal: HIDA",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:89-118"
    },
    "1377": {
        "file_id": 179,
        "content": "This class, LibxdoHID, is a HID interface for interacting with the XDo library. It initializes an instance of Xdo and provides methods to get button IDs from mouse buttons and key sequences from key literals.",
        "type": "comment"
    },
    "1378": {
        "file_id": 179,
        "content": "ctionTypes.keys):\n                keysequence = self.getKeySequenceFromKeyLiteral(key_literal)\n                if keysequence:\n                    self.xdo.send_keysequence_window_up(\n                        xdo.CURRENTWINDOW, keysequence.encode(\"utf8\")\n                    )\n            def _key_press(self, key_literal: HIDActionTypes.keys):\n                keysequence = self.getKeySequenceFromKeyLiteral(key_literal)\n                if keysequence:\n                    self.xdo.send_keysequence_window_down(\n                        xdo.CURRENTWINDOW, keysequence.encode(\"utf8\")\n                    )\n            def _mouse_move(self, x: Union[int, float], y: Union[int, float]):\n                self.xdo.move_mouse(x, y, screen=0)\n            def _mouse_click(\n                self,\n                x: Union[int, float],\n                y: Union[int, float],\n                button_literal: HIDActionTypes.mouse_buttons,\n                pressed: bool,\n            ):\n                self.mouse_move(x, y)\n             ",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:118-143"
    },
    "1379": {
        "file_id": 179,
        "content": "The code defines several functions for controlling the mouse and keyboard through X11 protocol. It includes _key_press, _key_release, and _mouse_click functions for sending key presses, releases, and mouse clicks respectively. The _mouse_move function is used to move the mouse pointer.",
        "type": "comment"
    },
    "1380": {
        "file_id": 179,
        "content": "   button_id = self.getButtonIdFromButtonLiteral(button_literal)\n                if pressed:\n                    self.xdo.mouse_down(xdo.CURRENTWINDOW, button_id)\n                else:\n                    self.xdo.mouse_up(xdo.CURRENTWINDOW, button_id)\n            def _mouse_scroll(\n                self,\n                x: Union[int, float],\n                y: Union[int, float],\n                dx: Union[int, float],\n                dy: Union[int, float],\n            ):\n                self.mouse_move(x, y)\n                # send up/down/left/right keys instead.\n                if dx < 0:\n                    self.xdo.send_keysequence_window(xdo.CURRENTWINDOW, \"Left\")\n                else:\n                    self.xdo.send_keysequence_window(xdo.CURRENTWINDOW, \"Right\")\n                if dy < 0:\n                    self.xdo.send_keysequence_window(xdo.CURRENTWINDOW, \"Up\")\n                else:\n                    self.xdo.send_keysequence_window(xdo.CURRENTWINDOW, \"Down\")\n    # from pyvirtualdisplay import D",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:143-167"
    },
    "1381": {
        "file_id": 179,
        "content": "The code defines functions to simulate mouse events and scrolling. It uses the xdo module to send mouse clicks, moves, and key sequences to the current window.",
        "type": "comment"
    },
    "1382": {
        "file_id": 179,
        "content": "isplay\n    from pyvirtualdisplay.smartdisplay import SmartDisplay\n    import easyprocess  # no support for stdin!\n    # import time\n    import os\n    import subprocess\n    def type_string(string: str):\n        input_bytes = string.encode()\n        p = subprocess.Popen(\"xdotool type --file -\".split(), stdin=subprocess.PIPE)\n        stdout_data = p.communicate(input=input_bytes)[0]\n        return stdout_data\n    os.system(\"rm *.png\")\n    # keyboard = Controller()\n    # virtual_display = \":3\"\n    # backend = 'xvnc'\n    backend = \"xephyr\"  # like visible xvfb, useful for live streaming (no need for ffmpeg hacks with xvfb)\n    # backend = 'xvfb'\n    # with Display(backend=backend) as disp:\n    # proc_cmd = [\"xterm\"]\n    proc_cmd = [\"leafpad\"]\n    # proc_cmd = [\"alacritty\"]\n    with SmartDisplay(\n        backend=backend, size=(1920, 1080), extra_args=[\"-fullscreen\", \"-softCursor\"]\n    ) as disp:\n        # with SmartDisplay(backend=backend, size=(1920, 1080)) as disp:\n        # with SmartDisplay(backend=backend, size",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:167-195"
    },
    "1383": {
        "file_id": 179,
        "content": "Creates a fullscreen display with SmartDisplay and sets the backend to \"xephyr\" for live streaming capability. It also defines a function to type out strings using xdotool, clears existing PNG files, and starts a leafpad application in fullscreen mode.",
        "type": "comment"
    },
    "1384": {
        "file_id": 179,
        "content": "=(1920, 1080), extra_args=['-fullscreen']) as disp: # for unit testing purpose. maybe we should log events on that display.\n        # with SmartDisplay(backend=backend, extra_args=['-title', 'xephyr_test']) as disp: # get window location by title first, then limit all events to that window.\n        # with SmartDisplay(backend='xvfb') as disp:\n        # with Display(backend='xvfb') as disp:\n        # with Display(visible=False) as disp:\n        # not working in fullscreen mode!\n        import pyautogui\n        print(\"NEW DISPLAY AT\", disp.display)  # 0, INT\n        print(\"ENV DISPLAY?\", os.environ[\"DISPLAY\"])  # :0\n        # pynput controller not working.\n        # from pynput.keyboard import Controller\n        # from pynput.keyboard import Listener\n        # keyboardListener = Listener()\n        # keyboardController = Controller()\n        # with Display(backend='xvfb') as disp2:\n        #     print(\"NEW DISPLAY AT\", disp2.display) # 2\n        # working! do not use gnome-terminal.\n        # proc = easyp",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:195-216"
    },
    "1385": {
        "file_id": 179,
        "content": "This code is trying to set up a display for unit testing purposes using different backends. It is printing the display number and environment variable DISPLAY value. It also mentions issues with pynput keyboard controller.",
        "type": "comment"
    },
    "1386": {
        "file_id": 179,
        "content": "rocess.EasyProcess([\"alacritty\"])\n        # proc = easyprocess.EasyProcess(['gnome-terminal', f\"--display={disp.display}\"])\n        # proc = easyprocess.EasyProcess(['gnome-terminal', f\"--display={disp.display}\"])\n        # no need for starting/stopping\n        import mss\n        with easyprocess.EasyProcess(proc_cmd) as proc:\n            # need this to \"wake\" the terminal when fullscreen.\n            # you click before starting the program, so the program will not be affected by the activation.\n            os.system(\"xdotool mousemove 0 0\")\n            os.system(\"xdotool click 1\")\n            # proc.start()\n            # proc.start().sleep(3)\n            # proc.sleep(5)\n            proc.sleep(3)\n            # time.sleep(3)\n            # from Xlib.display import Display\n            # Display(os.environ['DISPLAY']).get_input_focus()\n            # not working.\n            # pyautogui.write(\"echo hello world pyautogui\\n\")\n            # works.\n            type_string(\n                \"echo hello world\\r\"\n      ",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:216-243"
    },
    "1387": {
        "file_id": 179,
        "content": "Starting Gnome terminal process and executing a command using mss module.",
        "type": "comment"
    },
    "1388": {
        "file_id": 179,
        "content": "      )  # return works in leafpad, but \"\\n\" does not.\n            # type_string('echo hello world\\n')\n            # p.wait()\n            # keyboardController.type(\"echo hello world pynput\\n\")\n            pyautogui.screenshot(\"terminal2.png\")  # full shot\n            # img = disp.grab()  # # partial shot, only on changes\n            # maybe we can use this as some sort of \"attention\" mechanism?\n            img = disp.grab(autocrop=False)  # full shot again.\n            if img:\n                img.save(\"terminal.png\")\n            else:\n                print(\"no image yet.\")\n            type_string(\"just some words.\")\n            # .save not working\n            # mss.mss().save(output=\"terminal4.png\")\n            # mon_shot = mss.mss().save(mon=1, output=\"terminal4.png\")\n            if backend == \"xvfb\":\n                mon_shot = mss.mss().shot(output=\"terminal4.png\")\n            # print(mon_shot)\n            # nope. no attention/diff mechanism.\n            xdo_hid = LibxdoHID()\n            xdo_hid.mouse_mo",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:243-265"
    },
    "1389": {
        "file_id": 179,
        "content": "Capturing screenshots of terminal and other applications, using different screen grabbing techniques.\n\nThe code is capturing the screenshot of a terminal window and storing it as \"terminal2.png\" in full shot mode. It then checks if an image was captured and saves it as \"terminal.png\" if available or prints \"no image yet.\" Next, it types some text on the screen. It attempts to capture another screenshot of a monitor (mon_shot) using xvfb backend but is unsure about its success due to possible lack of attention/diff mechanism. The code also mentions trying different save methods like mss().save() but they are not working. It uses LibxdoHID to control the mouse movement.",
        "type": "comment"
    },
    "1390": {
        "file_id": 179,
        "content": "ve(300, 300)\n            xdo_hid.key_press(\"'q'\")\n            time.sleep(0.2)\n            xdo_hid.key_release(\"'q'\")\n            time.sleep(0.3)\n            disp.grab().save(\"terminal5.png\")\n            time.sleep(0.3)\n            # proc.stop()",
        "type": "code",
        "location": "/software_capture_hid_control/test_control.py:265-273"
    },
    "1391": {
        "file_id": 179,
        "content": "Taking a screenshot of the terminal and pressing q to exit.",
        "type": "comment"
    },
    "1392": {
        "file_id": 180,
        "content": "/software_capture_hid_control/test_xdo.py",
        "type": "filepath"
    },
    "1393": {
        "file_id": 180,
        "content": "Sends key sequence down and up in current window.",
        "type": "summary"
    },
    "1394": {
        "file_id": 180,
        "content": "import xdo\nxdt = xdo.Xdo()\nkey = b\"k\"\nwindow = xdo.CURRENTWINDOW\nxdt.send_keysequence_window_down(window, key)\nxdt.send_keysequence_window_up(window, key)\n# it needs binary encoding.",
        "type": "code",
        "location": "/software_capture_hid_control/test_xdo.py:1-7"
    },
    "1395": {
        "file_id": 180,
        "content": "Sends key sequence down and up in current window.",
        "type": "comment"
    },
    "1396": {
        "file_id": 181,
        "content": "/streaming_utils.yaml",
        "type": "filepath"
    },
    "1397": {
        "file_id": 181,
        "content": "The code configures a set of windows, each with specific shell commands and start directories. The windows include \"dev window\", \"BongoCat\", \"virtualbox\", \"viewer_loop\", and \"viewer_final\".",
        "type": "summary"
    },
    "1398": {
        "file_id": 181,
        "content": "session_name: streaming_utils\nstart_directory: \"/media/root/Toshiba XG3/works/agi_computer_control\"\nwindows:\n  - window_name: dev window\n    layout: tiled\n    panes:\n######################[bongocats]\n      - shell_command:\n        - \"python3 cat.pyw\"\n        start_directory: \"BongoCat\"\n      - shell_command:\n        - \"python3 cat.pyw\"\n        start_directory: \"BongoCat\"\n######################[virtualbox]\n      - shell_command:\n        - \"python3 kali_vbox_main_loop.py\"\n        start_directory: \"ubuntu_qemu_utm_arm_record/kali_vbox_control\"\n######################[viewer_loop]\n      - shell_command:\n        - sleep 1\n        - \"python3 xvfb_test_loop_double_display.py\"\n        start_directory: \"ubuntu_qemu_utm_arm_record/kali_vbox_control\"\n######################[viewer_final]\n      - shell_command:\n        - sleep 3\n        - \"bash x11grab_loop_viewer.sh\"\n        start_directory: \"ubuntu_qemu_utm_arm_record/kali_vbox_control\"",
        "type": "code",
        "location": "/streaming_utils.yaml:1-31"
    },
    "1399": {
        "file_id": 181,
        "content": "The code configures a set of windows, each with specific shell commands and start directories. The windows include \"dev window\", \"BongoCat\", \"virtualbox\", \"viewer_loop\", and \"viewer_final\".",
        "type": "comment"
    }
}