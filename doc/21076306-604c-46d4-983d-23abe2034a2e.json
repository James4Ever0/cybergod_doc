{
    "summary": "The code prepares training data by synchronizing hidden and video data, reading frames for processing, and handling failed frame readings through logging and raising exceptions.",
    "details": [
        {
            "comment": "The code imports necessary libraries, defines data structure classes and functions for parsing training data from a given base path. It uses NamedTuple and TypedDict for defining data structures, and utilizes OpenCV and JSON for handling video and data files.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/recording_train_parse.py\":0-41",
            "content": "# from collections import namedtuple\ntry:\n    from typing import TypedDict\nexcept:\n    from typing_extensions import TypedDict\ntry:\n    from typing import Literal\nexcept:\n    from typing import Literal\ntry:\n    from typing import NamedTuple\nexcept:\n    from typing_extensions import NamedTuple\nimport numpy as np\nfrom typing import Union, cast, overload\n# import logging\nfrom log_utils import logger\nclass HIDStruct(TypedDict):\n    HIDEvents: list\nclass TrainingFrame(NamedTuple):\n    datatype: Literal['hid','image']\n    data: Union[HIDStruct, np.ndarray]\n# we just need the basepath.\ndef getTrainingData(basePath: str):\n    import os\n    hid_timestamp_path = os.path.join(basePath,\"hid_timestamps.json\")\n    video_timestamp_path = os.path.join(basePath,\"video_timestamps.json\")\n    video_path = os.path.join(basePath,\"video_record.mp4\")\n    hid_rec_path = os.path.join(basePath,\"hid_record.jsonl\")\n    import json\n    import cv2\n    import jsonlines\n    video_cap = cv2.VideoCapture(video_path)\n    # breakpoint()\n    # 318 frames? "
        },
        {
            "comment": "The code is loading a video's frame count, two JSON files containing timestamp information, and defining a function to synchronize frame indexes between two arrays.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/recording_train_parse.py\":41-77",
            "content": "only got 266 timestamps!\n    frame_count = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    logger.info(\"FRAME COUNT: %d\", frame_count)\n    def load_json(filename):\n        with open(filename, \"r\") as f:\n            return json.load(f)\n    hid_timestamp = load_json(hid_timestamp_path)\n    video_timestamp = load_json(video_timestamp_path)\n    from typing import List, Union\n    import numpy as np\n    def getVideoFrameIndexSynced(\n        x: Union[List[int], np.ndarray],\n        y: Union[List[int], np.ndarray],\n        EPS: float = 1e-10,\n    ) -> List[int]:\n        \"\"\"\n        Notes:\n            All input arrays and output array are positive and increasing.\n        Params:\n            x: Actual video frame indexes.\n            y: Index list to be synced against.\n        Output:\n            x_: Synced frame indexs. (len(x_) == len(y))\n        \"\"\"\n        x_ = np.linspace(x[0], x[-1] + (1 - EPS), len(y))\n        x_ = np.floor(x_).astype(int).tolist()\n        return x_\n    hidseq = np.zeros(shape=(2, len(hid_timestamp))) "
        },
        {
            "comment": "Code is creating a sequence of hidden and video data points in synchronization. It uses numpy arrays to represent the sequences, including hidden timestamp, video timestamp, and corresponding video frame indices. The code sorts the combined sequence based on the timestamps and opens a file (hid_rec_path) to read JSON lines containing the hidden data for parsing.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/recording_train_parse.py\":77-110",
            "content": "- 1\n    hidseq[0] = np.array(range(len(hid_timestamp)))\n    videoseq = np.zeros(shape=(2, len(video_timestamp))) - 1\n    # videoseq[1] = np.array(range(len(video_timestamp)))\n    index_list_to_be_synced_against = np.array(range(len(video_timestamp)))\n    actual_video_frame_indexs = np.array(range(int(frame_count)))\n    videoseq[1] = getVideoFrameIndexSynced(\n        actual_video_frame_indexs, index_list_to_be_synced_against\n    )\n    seq = np.hstack((hidseq, videoseq))\n    logger.info(\"SEQ SHAPE: %s\", seq.shape)\n    timeseq = np.array(hid_timestamp + video_timestamp)\n    sorted_indexes = np.argsort(timeseq)\n    sorted_seq = seq[:, sorted_indexes].T.astype(int)\n    # print(sorted_seq)\n    # now, attempt to parse them.\n    hid_data_list = []\n    with open(hid_rec_path, \"r\") as f:\n        jsonl_reader = jsonlines.Reader(f)\n        while True:\n            try:\n                hid_data = jsonl_reader.read()\n                hid_data_list.append(hid_data)\n            except:\n                break\n    # maybe you shou"
        },
        {
            "comment": "This code is parsing a sequence of hidden index (HID) and frame index values. It checks that at least one type of content is active, and ensures that there are no two types of active content sharing the same index. If a HID index is present, it yields a TrainingFrame with datatype 'hid' and the corresponding HIDStruct data. If a frame index is present, it reads frames from video_cap until reaching the specified frame index before continuing.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/recording_train_parse.py\":110-135",
            "content": "ld \"yield\" data through these iterators.\n    NO_CONTENT = -1\n    suc, frame = video_cap.read()\n    frame_index_cursor = 0\n    for hid_index, frame_index in sorted_seq:\n        logger.debug(\"HID INDEX: %d, FRAME INDEX: %d\", hid_index, frame_index)\n        assert not all(\n            [e == NO_CONTENT for e in [hid_index, frame_index]]\n        ), \"at least one type of content is active\"\n        assert not all(\n            [e != NO_CONTENT for e in [hid_index, frame_index]]\n        ), \"cannot have two types of active content sharing the same index\"\n        if hid_index != NO_CONTENT:\n            hid_data = hid_data_list[hid_index]\n            logger.debug(\"HID DATA: %s\", hid_data)\n            yield TrainingFrame(datatype='hid', data=cast(HIDStruct, hid_data))\n        elif frame_index != NO_CONTENT:\n            while frame_index_cursor != frame_index:\n                suc, frame = video_cap.read()\n                frame_index_cursor += 1\n            assert (\n                suc\n            ), f\"Video '{video_path}"
        },
        {
            "comment": "Reading video frames and processing them for training data. If a frame fails to read, logs the frame shape and raises an exception.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/recording_train_parse.py\":135-146",
            "content": "' failed to read frame #{frame_index} (index starting from zero)\"\n            logger.debug(\"FRAME SHAPE: %s\", frame.shape)\n            yield TrainingFrame(datatype='image', data=frame)\n            # cv2.imshow(\"win\", frame)\n            # cv2.waitKey(1)\n        else:\n            raise Exception(\"Something impossible has happened.\")\n    # breakpoint()\n    video_cap.release()\n    # success, frame = video_cap.read()\n    # print(frame.shape) # (768, 1280, 3)"
        }
    ]
}