{
    "summary": "The code introduces a function for enhancing model training efficiency, addressing robot action interpretation and DirectML incompatibility on RTX2, while also measuring GPU memory usage.",
    "details": [
        {
            "comment": "This code is importing necessary modules, attempting to handle errors, and defining a lambda_auto_wrap_policy function. It appears to be related to module wrapping based on an arbitrary user function in the context of torch distributed algorithms.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":0-34",
            "content": "import torch\nimport typing\nimport functools\n# import zeta.nn.attention\n# no you must change the code.\ntry:\n    from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n        apply_activation_checkpointing,\n    )\nexcept:\n    # let's patch the error.\n    import torch.distributed.algorithms._checkpoint.checkpoint_wrapper\n    def lambda_auto_wrap_policy(\n        module: torch.nn.Module,\n        recurse: bool,\n        unwrapped_params: int,\n        lambda_fn: typing.Callable,\n    ) -> bool:\n        \"\"\"\n        A convenient auto wrap policy to wrap submodules based on an arbitrary user\n        function. If `lambda_fn(submodule) == True``, the submodule will be wrapped as\n        a `wrapper_cls` unit.\n        Return if a module should be wrapped during auto wrapping.\n        The first three parameters are required by :func:`_recursive_wrap`.\n        Args:\n        module (nn.Module):\n            The module to be considered in this decision.\n        recurse (bool):\n            Indicate if this is called"
        },
        {
            "comment": "Function makes a decision to recurse down the module structure (lines 34-58)\n\nUnwrapped parameters determine if there are any left to wrap in this module.\n\nLambda function is used to decide whether to wrap a module individually or not.\n\nIf recursion is enabled, always recurse (line 40).\n\nElse, check lambda function for each module (line 43-45).",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":34-58",
            "content": " to make a decision on whether we\n            should recurse down a subgraph of the module structure.\n            If False, it means this function is called to make a decision\n            on whether we should wrap the said module.\n        unwrapped_params (int):\n            The number of parameters yet to be wrapped in this module.\n        lambda_fn (Callable[nn.Module] -> bool):\n            If this returns ``True``, this module will be wrapped by\n            wrapper_cls individually.\n        \"\"\"\n        if recurse:\n            # always recurse\n            return True\n        else:\n            # if not recursing, decide whether we should wrap for the leaf node or reminder\n            return lambda_fn(module)\n    def apply_activation_checkpointing_wrapper(\n        model,\n        checkpoint_wrapper_fn=torch.distributed.algorithms._checkpoint.checkpoint_wrapper.checkpoint_wrapper,\n        check_fn=lambda _: True,\n    ):\n        \"\"\"\n        Applies :func:`checkpoint_wrapper` to modules within `model` based on a"
        },
        {
            "comment": "This function applies activation checkpointing to a given model by wrapping appropriate layers with the specified wrapper function. It does not wrap the overall root module and requires the user to define a check function to determine which modules to wrap.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":58-78",
            "content": " user-defined\n        configuration. For each module within `model`, the `check_fn` is used to decide\n        whether `module` should be wrapped with :func:`checkpoint_wrapper` or not.\n        Note::\n            This function modifies `model` in place and replaces appropriate layers with\n            their checkpoint-wrapped modules.\n        Note::\n            This function will not wrap the overall root module. If this is needed, please directly use\n            :class:`CheckpointWrapper`.\n        Usage::\n            model = nn.Sequential(\n                nn.Linear(10, 10), nn.Linear(10, 10), nn.Linear(10, 10)\n            )\n            check_fn = lambda l: isinstance(l, nn.Linear)\n            apply_activation_checkpointing(model, checkpoint_wrapper_fn=checkpoint_wrapper, check_fn=check_fn)\n        Args:\n            module (nn.Module):\n                The model who's submodules (or self) should be wrapped with activation checkpointing.\n            checkpoint_wrapper_fn (Optional[Callable[nn.Module]])\n     "
        },
        {
            "comment": "The code defines a function that wraps modules in a model based on a given check function. The wrapped modules can be used for checkpointing during training to improve efficiency. It uses the _recursive_wrap function from fsdp.wrap module, and sets various parameters like auto_wrap_policy, wrapper_cls, etc. for the wrapping process.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":78-100",
            "content": "           A `Callable` which will wrap modules\n            check_fn (Optional[Callable[nn.Module, nn.Module]])\n                A lambda function which will be passed current layer and returns\n                ``True`` or ``False`` depending on whether input layer should be wrapped.\n        Returns: None (`model` is modified inplace)\n        \"\"\"\n        # TODO: Importing inside function to avoid circular import issue between FSDP and\n        # checkpoint_wrapper. This can be resolved once wrap() APIs are decoupled from FSDP code.\n        from torch.distributed.fsdp.wrap import _recursive_wrap\n        return _recursive_wrap(\n            module=model,\n            auto_wrap_policy=functools.partial(\n                lambda_auto_wrap_policy, lambda_fn=check_fn\n            ),\n            wrapper_cls=checkpoint_wrapper_fn,\n            ignored_modules=set(),\n            ignored_params=set(),\n            only_wrap_children=True,\n        )\n    setattr(\n        torch.distributed.algorithms._checkpoint.checkpoint_wrap"
        },
        {
            "comment": "The code is detecting the available device for execution and setting up the corresponding device name (CPU, CUDA or DirectML). It also initializes a function called \"forward_new\" that performs the forward pass of the model.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":100-136",
            "content": "per,\n        \"apply_activation_checkpointing\",\n        apply_activation_checkpointing_wrapper,\n    )\nfrom rtx import RTX2\nmajor_torch_version = int(torch.__version__.split(\".\")[0])\nflash_attention = False\nif major_torch_version >= 2:  # use flash attention\n    print(\"Using flash attention\")\n    flash_attention = True\n# when posting ad via email. be sure there is an unsubscribe button to avoid legal issues.\ndev = None\ndevice_name = \"CPU\"\nif torch.cuda.is_available():\n    print(\"Trying to use first CUDA device.\")\n    # dev = torch.cuda.device(0)  # not working on torch 1.11\n    dev = \"cuda\"\n    device_name = \"CUDA\"\nelse:\n    print(\"`torch.cuda` is not available.\")\n    try:\n        print(\"Trying DirectML.\")\n        import torch_directml\n        dev = torch_directml.device()\n        device_name = \"DirectML\"\n    except:\n        print(\"Could not find DirectML device.\")\nprint(f\"Using {device_name}.\")\ndef forward_new(self, img: torch.Tensor, text: torch.Tensor):\n    \"\"\"Forward pass of the model.\"\"\"\n    try:\n        _encoded"
        },
        {
            "comment": "Code is defining a forward method for an RTX2 class. It encodes images with an encoder, prints the shape of the encoded context, and then tries to decode text using the encoded context as input to the decoder. If any exception occurs, it prints the error message and raises the error. The batch size and input length are defined for testing purposes.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":136-166",
            "content": " = self.encoder(img, return_embeddings=True)\n        print(\"encoded context shape: {}\".format(_encoded.shape))\n        # torch.Size([2, 64, 512])\n        # b, wtf, 2*dim\n        encoded = _encoded\n        # the shape is fixed. damn.\n        # now we either need addition or some thin nn\n        # encoded = torch.zeros((2,128,512)).to(dev)\n        # encoded = torch.zeros((2,64,1024)).to(dev)\n        # can we use arbitrary input? can we?\n        return self.decoder(text, context=encoded)\n    except Exception as error:\n        print(f\"Failed in forward method: {error}\")\n        raise\nRTX2.forward = forward_new\n# uninstalling and reinstalling 'timm' 'zetascale' and 'beartype' helps.\n# is it data corruption?\n# windows is not supported\n# 'NoneType' object has no attribute 'cadam32bit_grad_fp32'\nbatch_size = 1\n# batch_size = 2\ninput_length = 1024  # output_length = input_length - 1\n# usage\n# it is trying to expand the observation space to infinity, till the model says it is end.\nimg = torch.randn(\n    batch_size, 3, 2"
        },
        {
            "comment": "The code is creating a random image dataset for training a transformer model. It uses torch.randint() to generate logits, adjusting the resolution based on the environment and considering different approaches to handle audio input with ViTs. The most important task is finding a way to connect the language model (LM) with the ViT.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":166-193",
            "content": "56, 256\n)  # the size of the image is not the same as vit.\ntext = torch.randint(\n    0, 20000, (batch_size, input_length)\n)  # one of 20000 logits, 1024 as count\n# want classification? you can either use more tokens or use special classifiers.\n# for me, just use more tokens. we will train this model in multiple \"continuous\" scenarios anyway.\n# also benefit from llms\n# adjust the resolution according to the environment\n# what is similar transformer in audio files like the ViT?\n# how do we put audio in?\n# approach 1: audio -> mel graph -> ViT -> embedding (ref: https://github.com/YuanGongND/ast)\n# approach 2: native audio transformers (ref: https://github.com/lucidrains/audiolm-pytorch)\n# how to merge?\n# approach 1: linear addition\n# approach 2: concatenation\n# approach 3: concatenation with thin linear nns\n# visual: add global & local area for the robot to inspect\n# hierarchical cascade structure? like small -> medium -> large\n# THE MOST IMPORTANT THING IS TO FIGURE OUT HOW TO CONNECT THE LM WITH VIT\nmodel"
        },
        {
            "comment": "The code is initializing a RTX2 model and moving it to the GPU if one is specified. The model takes in an image and text input, and returns logits (output1) for the image and a single loss value (output2) as well as printing the shapes of these outputs along with the device they are on. The code then explains possible implementations for interpreting the robot action sequentially or using token classification. It also mentions that this implementation does not work for DirectML and measures GPU memory usage.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":193-220",
            "content": " = RTX2(attn_flash=flash_attention)\n# let's use the gpu.\n# breakpoint()\nif dev is not None:\n    model.to(dev)\n    output1, output2 = model(img.to(dev), text.to(dev))\nelse:\n    output1, output2 = model(img, text)\n# output1: torch.Size([1, 1023, 20000]) (logits)\n# output2 is a single number (loss? how come?)\n# this is the same as text input. is it?\n# or just trying to reduce the loss against input text?\nprint(\"output logits:\", output1.shape, output2.shape)  # with gradient!\nprint(\"device:\", output1.device)\n# breakpoint()\n# output logits: torch.Size([2, 1023, 20000]) torch.Size([])\n# device: privateuseone:0\n# possible implementations:\n# i decide to go with the latter.\n# 1. interpret the robot action sequentially in a loop, if separated by any other token the interpretation ends, and it will start over from the beginning\n# 2. use indicator/classifier to determine what type of action the robot is taking for every token (token classification)\n# not working for DirectML\n# memory_usage = torch.cuda.memory_allocated"
        },
        {
            "comment": "Computes and prints the system's memory usage in GB.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/rt_x_test_code/rtx2_example.py\":220-232",
            "content": "(device=dev) / 1024**3  # in GB\n# print(\"Memory Usage:\", memory_usage, \"GB\")\n############ CPU Usage ############\nimport psutil\nprocess = psutil.Process()\nmemory_usage = process.memory_info().rss / 1024**3  # in GB\nprint(\"Memory Usage:\", memory_usage, \"GB\")\n# ref: https://github.com/microsoft/DirectML/issues/444"
        }
    ]
}