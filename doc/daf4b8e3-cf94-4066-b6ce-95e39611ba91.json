{
    "summary": "The code defines neural network classes for creating bots capable of understanding human intentions and performing tasks, with a focus on decision-making using Monte Carlo tree search and batch training techniques.",
    "details": [
        {
            "comment": "The code defines classes for neurons and connections in a neural network. The neuron class has an output size of 1x50, which includes potential or hidden state. The connection class represents the connections between neurons and holds their weights and bias. The code mentions using Monte Carlo tree search to find optimal neural network structure, but does not explicitly define this algorithm.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/dynamic_plasticity_neural_networks/dynamic_neural_network.py\":0-29",
            "content": "import torch\n# let's use networkx to generate random graphs?\n# the method to train it? we only keep the input & output the same as the static model.\n# place some clock like neurons, just like the SNN\n# to ensure that neurons won't get far away from each other\n# if they want to have some spikes\n# now you might want to use solver for network assembly.\n# neurons that fire together, wire together\n# there are pending connections. if these connections has larger gradients, they will be welcomed.\n# calculate route to output. if there is no route to output, then no gradient.\n# it rolls out the monte carlo tree search in ppo. would you find some optimal neural network structure with that?\nclass Connection:\n    def __init__(self):\n        self.connected_index = ...\n        self.weights = ...\n        self.bias = ...\nclass Neuron:\n    def __init__(self): # you can choose like: 100x1 -> 1x50 (1 can be larger, like 2) or 10x10\n        # but 100x1, 1x50 can be perferred, since it includes the potential or hidden state\n"
        },
        {
            "comment": "This code defines a NeuralPort class with properties for index, port_type, and current_potential. It also initializes input_ports and output_ports lists to store indices. The code mentions batch training, recurrent connections, forwarding all inputs randomly, and learning connectivity and propagation rules.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/dynamic_plasticity_neural_networks/dynamic_neural_network.py\":29-57",
            "content": "        # we can also tune the decay factor inside the neuron.\n        self.index = ...\n        self.current_potential = ...\n        self.input_connection = ...\n        self.output_connection = ...\nclass NeuralPort:\n    def __init__(self):\n        self.index = ...\n        self.port_type = ...\n        self.current_potential = ...\n# so we would iterate through our connection pool, propagate\n# how to do batch training?\ninput_ports = [] # assign ten indices\noutput_ports = []\n# obviously they are not connected.\n# you can assess that by gradient. does it have gradient?\n# recurrent connection: (0, 1), (1, 2), (2, 0)\n# so we just do all the forwarding at once in random order. who cares the order? no one!\n# maybe you can take multiple random execution samples multiple times and average out to stablize the performance\n# so there are possibly two things to learn. the first is to learn the connectivity rule. the second is to learn the propagation rule.\n# does a regular/periodic propagation rule plausible in brain?\n# c"
        },
        {
            "comment": "Computing inter-neuron distance and creating sparse matrices for input/output nodes. Iterating over the network to form connections. Determining connectivity between neurons using potential methods like GCN, recommendation engine, or pre-programmed random initialization. Connection swapping to minimize unconnected neurons. Hierarchical grouping or adjusting weight matrices possible.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/dynamic_plasticity_neural_networks/dynamic_neural_network.py\":57-98",
            "content": "alculate inter_neuron distance\n# we have to use sparse matrix\n# output_node_index, input_node_index\n# if you swap, you break the connection\n# you can compute the gradient, to decide if you want to break or keep\n# (2, 3)\n# (4, 1)\n# we will start with the input neurons, iterate over the network\n# it forms a loop.\n# output: spikes, movements per neuron\n# what determines the connectivity between neurons?\n# what forms new connectivities?\n# (in0, out0)\n# (in1, out1)\n# (in2, out2)\n# i think we do not need the coordinates.\n# i think we need some gcn, or recommendation engine?\n# just represent the connectivity in sparse matrix.\n# connection swapping?\n# minimize the number of unconnected neurons\n# cannot connect to themselves\n# you may have pre-programmed randomly initialized connectivity matrix\n# hierarchical grouping?\n# would you rather do some value swapping in weight matricies\n# liquid state machine\n# dynamic rewiring of neurons by upper/lower weight limit, or the free energy principle\n# or you can also look at the a"
        },
        {
            "comment": "This code discusses the creation of computer bots that can understand human intentions and perform tasks, with a focus on browsing, coding, and searching. The author expresses their limited capacity to create an artificial life form but believes in using bots for decision making. They also mention participating in a 'cybergod' project that controls computers by themselves.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/dynamic_plasticity_neural_networks/dynamic_neural_network.py\":98-108",
            "content": "ctual energy consumption or battery consumption\n# by far multiple thesis have been stated, but yet unproven and unimplemented.\n# the primary objective of this project is to create computer operating bots that can understand human intentions and do everyday tasks on their own, including browsing, coding and searching.\n# it is not clear whether this project will create some artificial life, and it is not our objective. it might be a good research project for the bots, but i can say for sure it is not for me. i have limited capacity of knowledge and resources. i don't allocate that much memory biologically or physically. i choose to build the bot first. it does not have to be that complex that i cannot imagine or create.\n# decision matters. the executors of this 'life' project should be bots. i am the project manager. besides, i could participate in the 'cybergod' project which controls the computer by computer itself.\n# noise in, words out, recurse. that is dream."
        }
    ]
}