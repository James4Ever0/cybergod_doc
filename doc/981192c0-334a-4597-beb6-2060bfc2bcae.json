{
    "summary": "The code uses \"cl100k_base\" encoding to separate actions from content, defines a function 'l_tokens' for custom special tokens based on direction, value type, and index, and discusses optimization techniques such as allowing AI to emit multiple tokens and considering compression techniques.",
    "details": [
        {
            "comment": "Code imports the \"cl100k_base\" encoding from the tiktoken library, which allows encoding of text. The code aims to separate actions from content and compare different ways to represent them. It mentions loading arguments for specific encodings and includes built-in special tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py\":0-26",
            "content": "import tiktoken\n# are you sure you can encode everything? what about bytes?\n# why don't you just encode some bytes to the vocabulary?\n# don't worry we will handle that.\ncl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n# you can separate action from content.\n# we can compare the difference. eventually we will find out which is best and most performant.\n# like: [keyboard_input_start] [char1] [char2] [keyboard_input_end]\n# or: [keyboard_input] [char1] [keyboard_input] [char2]\n# compared to: [keyboard a] [keyboard upper a] [mouse move x 10] [mouse move y 10]\n# well these are some 'action' vocabularies worth learning.\n# how many special tokens can we add?\n# In production, load the arguments directly instead of accessing private attributes\n# See openai_public.py for examples of arguments for specific encodings\n# builtin special tokens: {'<|endoftext|>': 100257, '<|fim_prefix|>': 100258, '<|fim_middle|>': 100259, '<|fim_suffix|>': 100260, '<|endofprompt|>': 100276}\n# max token value: 100276\ndef make_specia"
        },
        {
            "comment": "This code defines a function 'l_tokens' that creates a dictionary with special tokens and their positions. It uses a lambda function, 'build_special_token', to create special token strings. The list of custom special tokens is generated from various combinations of direction, value type, and index. The encoding for an 'tiktoken' instance is then defined using this list of custom special tokens along with the existing set of special tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py\":26-55",
            "content": "l_tokens(token_name_list: list[str], start):\n    result = {}\n    for token_name in token_name_list:\n        result[token_name] = start\n        start += 1\n    return result\nbuild_special_token = lambda token_def: f\"<|{token_def}|>\"\ncustom_special_tokens = [build_special_token(\"my_new_token\")]\nfor direction in 'x', 'y':\n    for val_type in 'pos', 'neg':\n        for i in range(1024):\n            tk = build_special_token(f'{direction}_{val_type}_{i}')\n            custom_special_tokens.append(tk)\nenc = tiktoken.Encoding(\n    # If you're changing the set of special tokens, make sure to use a different name\n    # It should be clear from the name what behaviour to expect.\n    name=\"cl100k_im\",\n    pat_str=cl100k_base._pat_str,\n    mergeable_ranks=(mrs := cl100k_base._mergeable_ranks),\n    special_tokens={\n        **make_special_tokens(\n            list(cl100k_base._special_tokens.keys()) + custom_special_tokens,\n            start=max(mrs.values()) + 1,\n        )\n        # **cl100k_base._special_tokens,\n        # \"<|im"
        },
        {
            "comment": "This code is initializing an encoding object and testing the encoding of a text string containing special tokens. The `cl100k_base` represents a predefined vocabulary, which has a maximum token number of 100255. The code is trying to encode a target text that includes custom special tokens (e.g., \"<|my_new_token|>\"), and the encoding object handles converting those tokens into their respective values.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py\":55-82",
            "content": "_start|>\": 100_264,\n        # \"<|im_end|>\": 100_265,\n        # \"<|my_new_token|>\": 200_000,\n    },\n)\n# 100255 is the max token number in mergeable ranks\n# you can add new tokens.\nprint(f\"{cl100k_base.max_token_value:=}\")\nprint(f\"{cl100k_base.n_vocab:=}\")\n# cl100k_base.special_tokens_set\n# breakpoint()\nencode_bytes_target = bytes([x for x in range(255)])\n# you could use unicode regex to encode bytes\n# tweaking `_encode_single_piece`, `_encode_only_native_bpe` under tiktoken.core.Encoding\ntext_target = f\"hello world{custom_special_tokens[0]}my name is andy{build_special_token('x_pos_1000')}\"\ntokens = enc.encode(\n    text_target, disallowed_special=()\n)  # this will pass but will not be converted into special tokens.\n# can we train the model new tokens by using different encodings? this could introduce duality.\n# you can inform the model about the encoding. so the model might not misbehave.\n# tokens = enc.encode(text_target, allowed_special={custom_special_tokens[0]})\n# tokens = enc.encode(text_target, allowe"
        },
        {
            "comment": "The code is discussing the possibility of allowing the AI to emit multiple tokens at a time and how it could potentially impact performance and quality. It suggests considering compression techniques or augmenting training data as potential optimization strategies, but also mentions that changing the first embedding layer would be required to use more tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_action_and_text_tokenizer.py\":82-98",
            "content": "d_special={custom_special_tokens[0]}, disallowed_special = ())\ntokens_with_special = enc.encode(\n    text_target, allowed_special=\"all\", disallowed_special=()\n)\nprint(tokens)  # no special token!\nprint(tokens_with_special)\n# what if i allow the ai to emit multiple tokens a time?\n# i will sort the \"simutaneous\" tokens and order by priority\n# what about training? is that purely online? or shall we alter the training method?\n# like: [a,b,c,d,e,f] -> [a,c,e], [b,d,f] -> sample by priority\n# this is compression. this can speed up things. but not necessarily improve quality, unless you trade performance with quality.\n# or you could augment the training data, like input = x[:-3], target = x[3:]\n# either way, could possibly optimize the performance.\n# to use more tokens you need to change the first embedding layer"
        }
    ]
}