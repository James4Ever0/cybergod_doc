{
    "summary": "The code presents a hierarchical tokenization transformer model for text classification using Einops library. It incorporates multiple attention layers and sparse transforms for feature extraction, with a `sparseTransformerForward` function.",
    "details": [
        {
            "comment": "Code converts bytes to binary representation as a list of integers using bitwise shift, then discusses encoding and decoding concepts for hierarchical tokenization.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":0-42",
            "content": "# bytes -> char -> \"token\" -> sentences -> paragraphs\n# what to do in reverse? must recurse into bytes.\n# how to encode and decode?\nfrom torch import nn\nfrom torch import Tensor\nimport torch\nfrom typing import Literal\n# Convert bytes to binary representation as a list of integers using bitwise shift\ndef bytes_to_binary_int(byte_string):\n    binary_representation = [\n        int((byte_string[i] >> j) & 1)\n        for i in range(len(byte_string))\n        for j in range(7, -1, -1)\n    ]\n    return binary_representation\n# Example usage\nbyte_string = b\"hello world\"\nbinary_representation_int = bytes_to_binary_int(byte_string)\nprint(binary_representation_int)\n# you can pad with zeros when using bitlevel tokenizer\n# but how do you shift? char level or bit level? neither? self-determined?\n# encode:\n# nx2 * 2*d -> n*d\n# (nxn = nxd * dxn) * nxd = nxd\n# nxd -> convolution -> mxd, m = n/2\n# (mxm = mxd * dxm) * mxd = mxd\n# decode:\n# mxd -> deconvolution -> nxd\n# nxd * dx2 -> nx2\n# first, pad the output ahead of time.\n# # so for "
        },
        {
            "comment": "This code defines a `MultiheadSelfAttentionStack` class which is a module for self-attention layers. It has multiple layers of `nn.MultiheadAttention`, each with a specific number of heads and embed_dim. The `forward` method iterates over these layers to process the input tensor. The `TransformerArguments` class is used to specify optional arguments like `key_padding_mask`, `attn_mask`, etc.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":42-74",
            "content": "every bit in the byte, we have a vector of 768 dimensions\n# # try to reduce the bytes.\n# we've got four level of abstractions.\nfrom pydantic import BaseModel\nclass TransformerArguments(BaseModel, arbitrary_types_allowed=True):\n    key_padding_mask: Tensor | None = None\n    need_weights: bool = True\n    attn_mask: Tensor | None = None\n    average_attn_weights: bool = True\n    is_causal: bool = False\nclass MultiheadSelfAttentionStack(nn.Module):\n    def __init__(self, embed_dim: int, num_heads: int, num_layers: int, **kwargs):\n        super(MultiheadSelfAttentionStack, self).__init__()\n        self.layers = nn.ModuleList(\n            [\n                nn.MultiheadAttention(\n                    embed_dim=embed_dim, num_heads=num_heads, **kwargs\n                )\n                for _ in range(num_layers)\n            ]\n        )\n    def forward(self, input_tensor: Tensor, transformerArguments: TransformerArguments):\n        output = input_tensor\n        for layer in self.layers:\n            output, _ = layer(\n    "
        },
        {
            "comment": "Code is defining a `HierachicalTokenizationTransformer` class for text classification tasks using hierarchical tokenization and a transformer model architecture. The code also includes functions to create causal, inverse causal, and backtracking masks for the attention mechanism in the transformer layers.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":74-100",
            "content": "            query=output, key=output, value=output, **transformerArguments.dict()\n            )\n        return output\n# what if i use the inverse causal mask? backtracking mask?\n# import torch\n# a = torch.tril(torch.ones(10, 10))  # Create a lower triangular mask of ones\n# b = torch.tril(a.T)  # Transpose the mask and then take the lower triangular part to ensure backtracking\n# c = torch.tril(torch.flip(a, dims=[1]))  # Flip the mask along the horizontal axis and take the lower triangular part\n# d = torch.tril(torch.flip(a, dims=[0]))  # Flip the mask along the vertical axis and take the lower triangular part\nimport einops\nimport torch.nn.functional as F\n# hourglass replicate? not exactly. this is binary.\n# what about moe? lsm?\nclass HierachicalTokenizationTransformer(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=4, num_layers=1, abstraction_level=4):\n        # of course this is causal.\n        super().__init__()\n        self.TWO = 2\n        self.ONE = 1\n        self.ZERO = 0\n        self.bi"
        },
        {
            "comment": "This code initializes an abstraction-level transformer model with multiple attention and abstraction layers. The `nary_embedding` is an embedding layer, and the `abstractionLayers` and `deabstractionLayers` will contain these layers for different abstraction levels. The code also asserts that the abstraction level must be greater than zero.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":100-126",
            "content": "nary_embedding = nn.Embedding(\n            num_embeddings=self.TWO, embedding_dim=embed_dim\n        )\n        self.abstractionLayers = []\n        self.deabstractionLayers = []\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n        self.abstraction_level = abstraction_level\n        assert abstraction_level > 0, \"abstraction level must be greater than zero\"\n        self.mainTransformer = MultiheadSelfAttentionStack(\n            embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers\n        )\n        for _ in range(abstraction_level - 1):\n            # for _ in range(abstraction_level):\n            # Create the attention and abstraction layers\n            att_layer = MultiheadSelfAttentionStack(\n                embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers\n            )\n            abstract_layer = nn.Linear(self.TWO, self.ONE)\n            self.abstractionLayers.append(\n                att_layer\n            )  # Add the atte"
        },
        {
            "comment": "Appending abstraction and deabstraction layers to the list.\nCreating inverse attention and deabstraction layers, adding them to the list.\nDefining decode embedding layer and setting pad size.\nImplementing sparseTransformerForward function with parameters.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":126-152",
            "content": "ntion layer to the list\n            self.abstractionLayers.append(\n                abstract_layer\n            )  # Add the abstraction layer to the list\n            # Create the inverse attention and deabstraction layers\n            datt_layer = MultiheadSelfAttentionStack(\n                embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers\n            )\n            deabstract_layer = nn.Linear(self.ONE, self.TWO)\n            self.deabstractionLayers.append(\n                datt_layer\n            )  # Add the inverse attention layer to the list\n            self.deabstractionLayers.append(\n                deabstract_layer\n            )  # Add the deabstraction layer to the list\n        self.decode_embedding = nn.Linear(self.embed_dim, self.TWO)\n        self.pad_size = self.TWO**abstraction_level\n    def _sparseTransformerForwardImpl(\n        self,\n        embedding: Tensor,\n        transformer: MultiheadSelfAttentionStack,\n        transformerArguments: TransformerArguments,\n    ):\n        embeddin"
        },
        {
            "comment": "This code defines a function `sparseTransformerForward` that applies the transformer model to an input embedding. It also includes a helper function `_sparseTransformerForwardImpl` for internal use. If the `use_sliding` parameter is set, it applies the transformer model to non-boundary elements and averages them with boundary elements to reduce boundary effects. The code also includes another incomplete function `calculateInputPadSizeFromSequenceLength`.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":152-177",
            "content": "g = einops.rearrange(embedding, \"b (s1 g) d -> (b g) s1 d\", g=self.TWO)\n        embedding = transformer(embedding, transformerArguments=transformerArguments)\n        embedding = einops.rearrange(embedding, \"(b g) s1 d -> b (s1 g) d\", g=self.TWO)\n        return embedding\n    def sparseTransformerForward(\n        self,\n        embedding: Tensor,\n        transformer: MultiheadSelfAttentionStack,\n        transformerArguments: TransformerArguments,\n        use_sliding=True,\n    ):\n        _embedding = self._sparseTransformerForwardImpl(\n            embedding, transformer, transformerArguments\n        )\n        if use_sliding:\n            slide_embedding = self.sparseTransformerForward(\n                embedding[:, 1:-1, :],\n                transformer,\n                transformerArguments,\n                use_sliding=False,\n            )\n            _embedding[:, 1:-1, :] = (_embedding[:, 1:-1, :] + slide_embedding) / 2\n        return _embedding\n    def calculateInputPadSizeFromSequenceLength(self, sequence_lengt"
        },
        {
            "comment": "This code is part of a hierarchical tokenization model. It calculates the input padding size for each abstraction level, pads and chops embeddings based on the padding direction specified.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":177-205",
            "content": "h: int):\n        input_pad_size = []\n        msequence_length = int(sequence_length)\n        for _ in range(self.abstraction_level):\n            div, mod = divmod(msequence_length, self.TWO)\n            if mod == self.ONE:\n                input_pad_size.append(self.ONE)\n                div += self.ONE\n            else:\n                input_pad_size.append(self.ZERO)\n            msequence_length = div\n        return input_pad_size\n    def padEmbedding(self, embedding: Tensor, pad_direction: Literal[\"left\", \"right\"]):\n        embedding = F.pad(\n            embedding,\n            (self.ZERO, self.ZERO, self.ZERO, self.ONE)\n            if pad_direction == \"right\"\n            else (self.ZERO, self.ZERO, self.ONE, self.ZERO),\n            \"constant\",\n            self.ZERO,\n        )\n        return embedding\n    def chopEmbedding(self, embedding: Tensor, pad_direction: Literal[\"left\", \"right\"]):\n        embedding = (\n            embedding[:, : -self.ONE, :]\n            if pad_direction == \"right\"\n            else embe"
        },
        {
            "comment": "This code defines a class that seems to be part of a transformer-based model. It has three methods: `tokenizationForward`, `abstractionForward`, and `deabstractionForward`. The class takes in an input tensor, pads it from one side (specified by `pad_direction`), and applies transformations based on the provided `transformerArguments`. The code uses the Einops library for array manipulations.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":205-229",
            "content": "dding[:, self.ONE :, :]\n        )\n        return embedding\n    def abstractionForward(self, embedding: Tensor, abstractionLayer: nn.Linear):\n        embedding = einops.rearrange(embedding, \"b (s1 g) d -> b s1 d g\", g=self.TWO)\n        embedding = abstractionLayer(embedding)  # Apply attention and abstraction\n        embedding = einops.rearrange(embedding, f\"b s d {self.ONE} -> b s d\")\n        return embedding\n    def deabstractionForward(self, embedding: Tensor, deabstractionLayer: nn.Linear):\n        embedding = einops.rearrange(embedding, f\"b s d -> b s d {self.ONE}\")\n        embedding = deabstractionLayer(embedding)\n        embedding = einops.rearrange(embedding, \"b s1 d g -> b (s1 g) d\", g=self.TWO)\n        return embedding\n    def forward(\n        self,\n        input_logits: Tensor,\n        pad_direction: Literal[\"left\", \"right\"],\n        transformerArguments: TransformerArguments,\n    ):  # it is trimmed from one side. is it causal?\n        assert (\n            len(input_logits.shape) == self.TWO\n   "
        },
        {
            "comment": "The code is performing hierarchical tokenization by iterating through every other layer in the abstraction layers, applying sparse transformer forward and abstraction forward operations, and storing the residual connections.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":229-248",
            "content": "     ), \"input logits shall be of shape (batch_size, sequence_length)\"\n        _, sequence_length = input_logits.shape\n        assert sequence_length != self.ZERO, \"zero length sequence encountered\"\n        input_pad_size = self.calculateInputPadSizeFromSequenceLength(sequence_length)\n        residual_conn = []\n        embedding = self.binary_embedding(input_logits)\n        for i in range(\n            self.ZERO, len(self.abstractionLayers), self.TWO\n        ):  # Step through every other layer\n            lookup_index = i // self.TWO\n            if input_pad_size[lookup_index] == self.ONE:  # either 1 or 0\n                embedding = self.padEmbedding(embedding, pad_direction)\n            embedding = self.sparseTransformerForward(\n                embedding, self.abstractionLayers[i], transformerArguments\n            )\n            residual_conn.append(embedding)\n            embedding = self.abstractionForward(\n                embedding, self.abstractionLayers[i + self.ONE]\n            )\n        # basicall"
        },
        {
            "comment": "This code performs hierarchical tokenization using a sparse transformer model. It applies padding and chopping embeddings when needed, and iterates through every other layer of deabstraction layers for feature extraction.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":248-268",
            "content": "y: n -> 2*n - mod\n        if input_pad_size[-1] == self.ONE:\n            embedding = self.padEmbedding(embedding, pad_direction)\n        embedding = self.sparseTransformerForward(\n            embedding, self.mainTransformer, transformerArguments\n        )\n        if input_pad_size[-1] == self.ONE:\n            embedding = self.chopEmbedding(embedding, pad_direction)\n        for i in range(\n            self.ZERO, len(self.deabstractionLayers), self.TWO\n        ):  # Step through every other layer\n            lookup_index = self.abstraction_level - i // self.TWO - self.TWO\n            embedding = self.deabstractionForward(\n                embedding, self.deabstractionLayers[i + self.ONE]\n            )\n            embedding += residual_conn[lookup_index]\n            embedding = self.sparseTransformerForward(\n                embedding, self.deabstractionLayers[i], transformerArguments\n            )\n            if input_pad_size[lookup_index] == self.ONE:\n                embedding = self.chopEmbedding(embedding"
        },
        {
            "comment": "This code is initializing a HierachicalTokenizationTransformer model with specific abstraction level and number of layers. It then takes input data, applies the transformer's forward function, and prints the output shape.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/special_tokenizer_with_actions/test_hierachical_tokenization.py\":268-285",
            "content": ", pad_direction)\n        output_logits = self.decode_embedding(embedding)\n        return output_logits\n# myTransformer = HierachicalTokenizationTransformer()\n# myTransformer = HierachicalTokenizationTransformer(abstraction_level=5, num_layers=10)\n# myTransformer = HierachicalTokenizationTransformer(abstraction_level=100, num_layers=2)\nmyTransformer = HierachicalTokenizationTransformer(abstraction_level=20, num_layers=2)\n# input_data = torch.ones(20, 1000, dtype=torch.long)  # batch size: 20, sequence length: 30\ninput_data = torch.ones(20, 30, dtype=torch.long)  # batch size: 20, sequence length: 30\noutput_data = myTransformer.forward(\n    input_data,\n    pad_direction=\"right\",\n    transformerArguments=TransformerArguments(is_causal=True),\n)\nprint(output_data.shape)  # 20, 30, 2"
        }
    ]
}