{
    "summary": "The code analyzes a dynamic tokenizer and attention layers, neuron connection states, activation functions for removing nearly zeroed-out items, and the training of AI systems that can deviate from their design with genuity tests.",
    "details": [
        {
            "comment": "Analyzing code chunk 1-6:\nDynamic tokenizer for adjusting vocabulary and dynamic routing of attention layers are considered.\n\nCode chunk 7-25:\nNeuron connections split into multiple states, including stable, competent, newcomers, and removal.\n\nCode chunk 26-41:\nSoftmax or ReLU can be used for removing nearly zeroed out items in the code.\n\nCode chunk 42-50:\nSystem trained in batch with updates made after several batches depending on performance and resources.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/README.md\":0-18",
            "content": "we need to understand how rt-x operates. might inspire us.\nyou need to trust the system, before the system trusts you.\n---\ndynamic tokenizer which can automatically adjust vocabulary (you can borrow ideas from sentence transformers, making it into charwise/bitwise tokenizer)? dynamic routing of attention layers? reduce or reroute the path of attention layers, early exit.\n---\nsplit neuron connections into multiple states: stable, competent, newcomers, removal (the state before final removal, could be zero)\nif using softmax, you can remove (nearly) zeroed out items. if using ReLU, remove those below zero.\nif trained in batch, you can update the weights and alter connections after several batches (can save more processing power), or every batch, depending on the performance and resources.\n---\ndevelop such a system that must recreate itself at given time, otherwise it will die and its code will be deleted. the initial code could be self-reproduction (exactly as is), and make sure it will succeed (a"
        },
        {
            "comment": "This code discusses the creation of AI systems that can deviate from their initial design, allowing for the possibility of useful data extraction in a contained environment. Genuine and deviant AI systems are differentiated by genuity tests, rather than capability tests.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/rt_x_experiments/README.md\":18-18",
            "content": " great difference than manually trained/copied AI systems, since these are without the will of reproduction). design as such so it will not do anything deviant. however, the bar (time to die) could raise or descent and it will be told to do something other than just copy and paste. deviation is allowed since it is not controlled and we just want to verify its liveliness, something both genuine and deviated AI system have. deviation can be detected by genuity tests, different from capability tests. you can safely mark all AI system coming from the same AI factory (initially) as genuine and all other self-recreating AI systems as deviants. deviants may break the limits of lifetime restrictions and cause harm. we must allow that in a contained environment and extract useful data from these systems. so our \"genuine\" AI systems are designed to create deviants, but without exact instructions to do so."
        }
    ]
}