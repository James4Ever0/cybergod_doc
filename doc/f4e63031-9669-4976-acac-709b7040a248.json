{
    "summary": "The code includes classes and functions for token insertion, input size checks, mask generation, training pair creation, sequence padding, and probabilistic noise methods, with a focus on iterating through thought token insertion pairs for source tokens.",
    "details": [
        {
            "comment": "Code imports necessary libraries and defines various classes, enums, and variables related to token insertion methods. It also includes type annotations for clarity.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":0-27",
            "content": "from typing import Callable, Iterable, Optional\nimport torch\nimport math\nfrom beartype import beartype\nfrom beartype.vale import Is\nimport torch.nn.functional as F\nfrom enum import auto, Enum\nimport copy\nfrom typing_extensions import overload, Literal, Annotated\nfrom overtake import overtake  # type: ignore\nReplaceRatio = Annotated[float, Is[lambda number: 0 <= number < 1]]\nNonNegativeFloat = Annotated[float, Is[lambda number: number > 0]]\nclass InsertionMethodCategory(Enum):\n    common_source = auto()\n    separate_source = auto()\nclass ThoughtTokenInsertionMethod(Enum):\n    autoregressive = (auto(), InsertionMethodCategory.common_source)\n    generative_insert = (auto(), InsertionMethodCategory.common_source)\n    generative_insert_and_overwrite = ( # with probablistic noise and original random token swap ratio\n        auto(),\n        InsertionMethodCategory.common_source,\n    )  # will use generated target tokens to replace original randomly inserted thought tokens.\n    # not implemented\n    # iterate_and_in"
        },
        {
            "comment": "The code defines a class, an exception, and some functions. It appears to be involved in processing thought tokens and handling insertion methods for these tokens. The `UnknownThoughtTokenInsertionMethod` class is used as an error handler, while the `get_batch_and_seqlen()` function takes in source tokens and returns a batch size and sequence length. The code also defines an equality fulfillment transformer which seems to set a 'fulfilled' attribute for instances with existing 'fulfilled' attributes.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":27-62",
            "content": "sert_separately = (auto(), InsertionMethodCategory.separate_source)\n    # iterate_and_insert_together = (auto(), InsertionMethodCategory.separate_source)\n    @property\n    def category(self):\n        return self.value[1]\ndef equality_fulfillment_instance_transformer(instance):\n    new_instance = copy.copy(instance)\n    assert hasattr(\n        instance, \"fulfilled\"\n    ), \"cannot process instance with 'fulfilled' attribute\"\n    setattr(new_instance, \"fulfilled\", False)\n    old_eq = copy.copy(new_instance.__eq__)\n    def new_eq(self, other: object):\n        is_equal = old_eq(other)\n        if is_equal:\n            self.fulfilled = True\n        return is_equal\n    setattr(new_instance, \"__eq__\", new_eq)\n    return new_instance\nclass UnknownThoughtTokenInsertionMethod(Exception):\n    def __init__(self, insert_method):\n        super().__init__(f\"Method '{insert_method}' is not available.\")\n@beartype\ndef get_batch_and_seqlen(source_tokens: torch.Tensor):\n    source_size = source_tokens.shape\n    assert (\n        len("
        },
        {
            "comment": "Line 62-64:\n```\nsource_size) == 2\n    ), f\"wrong token size: {source_size} required: (batch, seqlen)\"\n    batch, seqlen = source_size\n    return batch, seqlen\n```\nEnsures correct input size is provided for batch and sequence length.\n\nLine 67-74:\n```\n@beartype\ndef create_zeros_from_tensor_metadata_and_insert_rate(\n    batch: int, seqlen: int, dtype: torch.dtype,insert_rate: float\n):\n    assert insert_rate > 0, f\"insert rate not positive: {insert_rate}\"\n    added_seqlen = math.ceil(thought_token_insert_rate * seqlen)\n    new_seqlen = seqlen + added_seqlen\n    zeros = torch.zeros((batch, new_seqlen), dtype=dtype)\n    return added_seqlen, new_seqlen, zeros\n```\nCreates zero tensor with the specified batch and sequence length.\n\nLine 76-91:\n```\n@beartype\ndef create_mask(batch: int, seqlen: int, k: int):\n    assert k > 0, f\"k ({k}) is not positive\"\n    assert k < seqlen, f\"k ({k}) must be less than seqlen ({seqlen})\"\n    # Generate random indices for each row\n    random_indices = torch.stack([torch.randperm(seqlen)[:k] for _ in range(batch)])\n    # Create a mask tensor to mark the selected indices\n    mask = torch.zeros((batch, seqlen), dtype=torch.bool)\n    mask.scatter_(1, random_indices, True)\n    return mask\n```\nGenerates a mask for randomly selected sequence elements.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":62-91",
            "content": "source_size) == 2\n    ), f\"wrong token size: {source_size} required: (batch, seqlen)\"\n    batch, seqlen = source_size\n    return batch, seqlen\n@beartype\ndef create_zeros_from_tensor_metadata_and_insert_rate(\n    batch: int, seqlen: int, dtype: torch.dtype,insert_rate: float\n):\n    assert insert_rate > 0, f\"insert rate not positive: {insert_rate}\"\n    added_seqlen = math.ceil(thought_token_insert_rate * seqlen)\n    new_seqlen = seqlen + added_seqlen\n    zeros = torch.zeros((batch, new_seqlen), dtype=dtype)\n    return added_seqlen, new_seqlen, zeros\n@beartype\ndef create_mask(batch: int, seqlen: int, k: int):\n    assert k > 0, f\"k ({k}) is not positive\"\n    assert k < seqlen, f\"k ({k}) must be less than seqlen ({seqlen})\"\n    # Generate random indices for each row\n    random_indices = torch.stack([torch.randperm(seqlen)[:k] for _ in range(batch)])\n    # Create a mask tensor to mark the selected indices\n    mask = torch.zeros((batch, seqlen), dtype=torch.bool)\n    mask.scatter_(1, random_indices, True)\n    retu"
        },
        {
            "comment": "Insert source token into zeros: Creates mask for source token locations, assigns source tokens to those locations in zeros tensor.\nInsert thought token into zeros: Assigns thought tokens to locations not occupied by source tokens.\nSample thought tokens: Generates random indices from thought token vocabulary and creates a tensor of sampled thought tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":91-130",
            "content": "rn mask\n@beartype\ndef insert_source_token_to_zeros(\n    source_tokens: torch.Tensor,\n    zeros: torch.Tensor,\n    batch: int,\n    seqlen: int,\n    new_seqlen: int,\n):\n    source_token_locations = create_mask(batch, new_seqlen, seqlen)\n    zeros[source_token_locations] = source_tokens\n    return source_token_locations\n@beartype\ndef insert_thought_token_to_zeros(\n    thought_tokens: torch.Tensor,\n    zeros: torch.Tensor,\n    source_token_locations: torch.Tensor,\n):\n    thought_token_locations = ~source_token_locations  # do not use \"not\"\n    zeros[thought_token_locations] = thought_tokens\n    return thought_token_locations\n@beartype\ndef sample_thought_tokens(\n    thought_token_vocabulary: list[int], batch: int, added_seqlen: int\n):\n    # Sampled thought_token_vocabulary indices\n    sampled_indices = torch.randint(\n        0, len(thought_token_vocabulary), size=(batch, added_seqlen)\n    )\n    # Create tensor using sampled indices\n    thought_tokens = torch.tensor(thought_token_vocabulary)[sampled_indices]\n    return th"
        },
        {
            "comment": "Function insert_thought_tokens:\nInserts thought tokens into source tokens based on thought token vocabulary and insertion rate. Returns the zeros, new sequence length, source token locations, and thought token locations. \n\nFunction pad_seq_left:\nPads input tensor with specified size to the left with a given value. Ensures padding size is non-negative.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":130-158",
            "content": "ought_tokens\n@beartype\ndef insert_thought_tokens(\n    source_tokens: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    thought_token_insert_rate: NonNegativeFloat,\n):\n    batch, seqlen = get_batch_and_seqlen(source_tokens)\n    added_seqlen, new_seqlen, zeros = create_zeros_from_tensor_metadata_and_insert_rate(\n        batch, seqlen, source_tokens.dtype, thought_token_insert_rate\n    )\n    source_token_locations = insert_source_token_to_zeros(\n        source_tokens, zeros, batch, seqlen, new_seqlen\n    )\n    thought_tokens = sample_thought_tokens(\n        thought_token_vocabulary, batch, added_seqlen\n    )\n    thought_token_locations = insert_thought_token_to_zeros(\n        thought_tokens, zeros, source_token_locations\n    )\n    return zeros, new_seqlen, source_token_locations, thought_token_locations\n@beartype\ndef pad_seq_left(input_tensor: torch.Tensor, pad_size: int, value):\n    assert pad_size >= 0, f\"pad size ({pad_size}) must be non negative\"\n    ret = F.pad(input_tensor, (pad_size, 0), mode=\"co"
        },
        {
            "comment": "This function pads the processed tokens and thought token locations with a specific padding size and pad token index.\n\nQuestion:",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":158-196",
            "content": "nstant\", value=value)\n    return ret\n@beartype\ndef pad_processed_and_thought_tokens(\n    processed_tokens: torch.Tensor,\n    thought_token_locations: torch.Tensor,\n    train_window_size: int,\n    pad_token_idx: int,\n):\n    pad_size = train_window_size - 1\n    padded_processed_tokens = pad_seq_left(processed_tokens, pad_size, pad_token_idx)\n    padded_thought_token_locations = pad_seq_left(\n        thought_token_locations, pad_size, False\n    )\n    return padded_processed_tokens, padded_thought_token_locations\n@beartype\ndef get_autoregressive_generator_and_thought_token_locations(\n    source_tokens: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    thought_token_insert_rate: NonNegativeFloat,\n):\n    (\n        processed_tokens,\n        new_seqlen,\n        _,\n        thought_token_locations,\n    ) = insert_thought_tokens(\n        source_tokens, thought_token_vocabulary, thought_token_insert_rate\n    )\n    assert new_seqlen > 1\n    (\n        padded_processed_tokens,\n        padded_thought_token_locations,\n    ) ="
        },
        {
            "comment": "This code defines functions for inserting thought tokens and generating training pairs. It takes input source_tokens, thought_token_vocabulary, and thought_token_insert_rate as parameters to control the process of inserting thought tokens into the source sequence. The function returns an iterable of tuples containing modified source sequences and their corresponding target sequences. The code uses autoregressive generator and generative methods for token insertion.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":196-224",
            "content": " pad_processed_and_thought_tokens(\n        processed_tokens, thought_token_locations, train_window_size, pad_token_idx\n    )\n    autoregressive_generator = autoregressively_yield_train_pairs(\n        padded_processed_tokens, train_window_size, new_seqlen\n    )\n    return autoregressive_generator, padded_thought_token_locations\n@overload\ndef insert_thought_tokens_and_yield_train_pairs(\n    insertion_method: Literal[ThoughtTokenInsertionMethod.autoregressive],\n    source_tokens: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    thought_token_insert_rate: NonNegativeFloat,\n) -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n    (\n        autoregressive_generator,\n        _,\n    ) = get_autoregressive_generator_and_thought_token_locations(\n        source_tokens, thought_token_vocabulary, thought_token_insert_rate\n    )\n    yield from autoregressive_generator\ndef generative_insert_thought_tokens_and_yield_train_pairs(\n    source_tokens: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    thought_toke"
        },
        {
            "comment": "Function that generates training pairs for inserting thought tokens in a generative manner.\nInputs: autoregressive_generator, target_token_prob_generator, padded_thought_token_locations, thought_token_vocabulary, non_thought_token_vocabulary, train_window_size, probablistic_noise_ratio\nYields: training pairs for inserting thought tokens",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":224-251",
            "content": "n_insert_rate: NonNegativeFloat,\n    non_thought_token_vocabulary: list[int],\n    target_token_prob_generator: Callable[[torch.Tensor], torch.Tensor],\n    probablistic_noise_ratio:ReplaceRatio = 0,\n) -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n    (\n        autoregressive_generator,\n        padded_thought_token_locations,\n    ) = get_autoregressive_generator_and_thought_token_locations(\n        source_tokens, thought_token_vocabulary, thought_token_insert_rate\n    )\n    yield from generative_insert_yield_train_pairs(\n        autoregressive_generator,\n        target_token_prob_generator,\n        padded_thought_token_locations,\n        thought_token_vocabulary,\n        non_thought_token_vocabulary,\n        train_window_size,\n        probablistic_noise_ratio\n    )\n@overload\ndef insert_thought_tokens_and_yield_train_pairs(\n    insertion_method: Literal[ThoughtTokenInsertionMethod.generative_insert],\n    source_tokens: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    thought_token_insert_rate: NonNega"
        },
        {
            "comment": "This function generates thought tokens by inserting them into a source sequence with a specified rate. It takes in the source tokens, thought token vocabulary, thought token insertion rate, non-thought token vocabulary, and target token probability generator.\n\nThe `generate_porportional_mask_for_tensor` function creates a boolean mask for randomly selecting elements from an input tensor. It calculates the number of elements to be zeroed based on a given proportion and shuffles the mask randomly.\n\nThe `swap_input_tokens_with_previous_target_token` function is incomplete, making it difficult to provide a comment without further context.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":251-276",
            "content": "tiveFloat,\n    non_thought_token_vocabulary: list[int],\n    target_token_prob_generator: Callable[[torch.Tensor], torch.Tensor],\n) -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n    yield from generative_insert_thought_tokens_and_yield_train_pairs(\n        source_tokens,\n        thought_token_vocabulary,\n        thought_token_insert_rate,\n        non_thought_token_vocabulary,\n        target_token_prob_generator,\n    )\n@beartype\ndef generate_porportional_mask_for_tensor(input_tensor:torch.Tensor, porportion: ReplaceRatio):\n    # Determine the number of elements to be zeroed\n    num_elements = input_tensor.numel()\n    num_zero_elements = int(porportion * num_elements)\n    # Create a boolean mask for randomly selecting 30% of the elements\n    mask = torch.zeros(num_elements, dtype=torch.bool)\n    mask[:num_zero_elements] = 1  # Set the first 30% elements to True\n    mask = mask[torch.randperm(num_elements)]  # Shuffle the mask randomly\n    return mask\n@beartype\ndef swap_input_tokens_with_previous_target_token"
        },
        {
            "comment": "The code defines a function `s_by_swap_ratio` that takes in three parameters: `input_tokens`, `prev_target_tokens`, and `input_token_swap_ratio`. It generates a mask based on the input token swap ratio, and then sets the corresponding elements in both `input_tokens` and `prev_target_tokens` to 0. Finally, it adds the two tensors together and returns the resulting `input_tokens`.\n\nThe function `insert_thought_tokens_and_yield_train_pairs` is overloaded with a specific insertion method of `ThoughtTokenInsertionMethod.generative_insert_and_overwrite`. It takes in several parameters, including `source_tokens`, `thought_token_vocabulary`, `thought_token_insert_rate`, `non_thought_token_vocabulary`, `target_token_prob_generator`, `proabalistic_noise_ratio`, and `input_token_swap_ratio`. It then calls another function, `generative_insert_thought_tokens_and_yield_train_pairs`, passing in the required parameters and returns an iterable of tuple of torch.Tensors representing training pairs.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":276-300",
            "content": "s_by_swap_ratio(input_tokens:torch.Tensor,prev_target_tokens:torch.Tensor, input_token_swap_ratio):\n    input_mask = generate_porportional_mask_for_tensor(input_tokens, input_token_swap_ratio)\n    prev_target_mask = ~input_mask\n    input_tokens[input_mask] = 0\n    prev_target_tokens[prev_target_mask] = 0\n    input_tokens += prev_target_tokens\n    return input_tokens\n@overload\ndef insert_thought_tokens_and_yield_train_pairs(\n    insertion_method: Literal[\n        ThoughtTokenInsertionMethod.generative_insert_and_overwrite\n    ],\n    source_tokens: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    thought_token_insert_rate: NonNegativeFloat,\n    non_thought_token_vocabulary: list[int],\n    target_token_prob_generator: torch.nn.Module,\n    probablistic_noise_ratio:ReplaceRatio,\n    input_token_swap_ratio:ReplaceRatio,\n) -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n    generator = generative_insert_thought_tokens_and_yield_train_pairs(\n        source_tokens,\n        thought_token_vocabulary,\n        "
        },
        {
            "comment": "This function generates input-target token pairs and yields them, potentially inserting thought tokens and swapping input tokens with previous target tokens based on specified ratios. It also supports cropping input tokens by index and window size using another function.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":300-333",
            "content": "thought_token_insert_rate,\n        non_thought_token_vocabulary,\n        target_token_prob_generator,\n        probablistic_noise_ratio,\n    )\n    prev_target_tokens = None\n    for input_tokens, target_tokens in generator:\n        if prev_target_tokens is not None:\n            if input_token_swap_ratio > 0:\n                input_tokens = swap_input_tokens_with_previous_target_tokens_by_swap_ratio(input_tokens, prev_target_tokens, input_token_swap_ratio)\n            else:\n                input_tokens = prev_target_tokens\n        yield input_tokens, target_tokens\n        prev_target_tokens = target_tokens.clone()\n@overtake(runtime_type_checker=\"beartype\")\ndef insert_thought_tokens_and_yield_train_pairs(\n    insertion_method,\n    source_tokens,\n    thought_token_vocabulary,\n    thought_token_insert_rate,\n    non_thought_token_vocabulary=None,\n    target_token_prob_generator=None,\n    probablistic_noise_ratio = 0,\n    input_token_swap_ratio = 0\n):\n    ...\n@beartype\ndef crop_input_token_by_index_and_window_size(\n    pr"
        },
        {
            "comment": "This code seems to define functions for token cropping, autoregressive training pair generation, and probability to token conversion. \n\nThe function `crop_input_token_by_index_and_window_size` takes in a tensor of processed tokens, an index, and a window size, then crops the input tokens within that range. \n\nThe function `crop_target_token_by_index_and_window_size` does the same for target tokens, but offsets the index by one. \n\nThe function `autoregressively_yield_train_pairs` generates autoregressive training pairs using the previous functions to crop tokens. It iterates over a range and yields input and target token crops for each iteration. \n\nLastly, the `prob_to_token` function takes in a tensor of token probabilities and an optional masked location (default is None), presumably to convert those probabilities into corresponding tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":333-366",
            "content": "ocessed_tokens: torch.Tensor, index: int, window_size: int\n):\n    cropped_tokens = processed_tokens[:, index : index + window_size]\n    return cropped_tokens\n@beartype\ndef crop_target_token_by_index_and_window_size(\n    processed_tokens: torch.Tensor, index: int, window_size: int\n):\n    return crop_input_token_by_index_and_window_size(\n        processed_tokens, index + 1, window_size\n    )\n# the sample process shall start from zero.\n@beartype\ndef autoregressively_yield_train_pairs(\n    padded_processed_tokens: torch.Tensor, train_window_size: int, new_seqlen: int\n):\n    for i in range(new_seqlen - 1):\n        input_tokens = crop_input_token_by_index_and_window_size(\n            padded_processed_tokens, i, train_window_size\n        )\n        target_tokens = crop_target_token_by_index_and_window_size(\n            padded_processed_tokens, i, train_window_size\n        )\n        yield input_tokens, target_tokens\n@beartype\ndef prob_to_token(\n    token_prob: torch.Tensor,\n    masked_location: Optional[torch.Tensor] = N"
        },
        {
            "comment": "This code is defining two functions: \n1. `prob_to_token`: This function takes a tensor containing probability scores for each token in the vocabulary and converts it to a list of tokens. It also allows masking certain locations by setting their probabilities to zero if a `masked_vocabulary` or `masked_location` is provided.\n2. `generate_target_tokens_with_thought_token_locations_and_non_thought_token_vocabulary`: This function generates target tokens by first identifying locations of \"thought\" and \"non-thought\" tokens based on the input parameters, and then calling `prob_to_token` to convert probability scores for each token into a list of tokens.\n\nCode Reviewer",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":366-395",
            "content": "one,\n    masked_vocabulary: Optional[list[int]] = None,\n):\n    ret_prob = token_prob.clone()\n    if masked_vocabulary is not None:\n        ret_prob[:, masked_vocabulary] = 0\n    ret_tokens = torch.argmax(ret_prob, dim=2)\n    if masked_location is not None:\n        ret_tokens[masked_location] = 0\n    return ret_tokens\n@beartype\ndef generate_target_tokens_with_thought_token_loctions_and_non_thought_token_vocabulary(\n    token_prob: torch.Tensor,\n    thought_token_locations: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    non_thought_token_vocabulary: list[int],\n):\n    assert (\n        len(token_prob.shape) == 3\n    ), f\"wrong token probability tensor shape ({token_prob}). should be: (batch_size, sequence_length, vocabulary_size)\"\n    # what is the shape of this prob?\n    non_thought_token_locations = ~thought_token_locations\n    thought_tokens = prob_to_token(\n        token_prob, non_thought_token_locations, non_thought_token_vocabulary\n    )\n    non_thought_tokens = prob_to_token(\n        token_prob, "
        },
        {
            "comment": "This code defines functions to generate Gaussian noise and add probabilistic noise to token probabilities. The `generate_gaussian_noise_within_bounds` function generates Gaussian noise within a specified range, while the `add_probablistic_noise_to_prob` function adds probabilistic noise to token probabilities using the generated Gaussian noise.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":395-421",
            "content": "thought_token_locations, thought_token_vocabulary\n    )\n    ret_tokens = thought_tokens + non_thought_tokens\n    return ret_tokens\n@beartype\ndef generate_gaussian_noise_within_bounds(size:tuple, lower:float, upper:float):\n    assert lower <= upper, f\"rule lower ({lower}) <= upper ({upper}) does not comply\"\n    # Parameters\n    mean = 0\n    std = 1\n    # Generate Gaussian noise\n    noise = torch.normal(mean, std, size=size)  # Generate 10 samples of Gaussian noise\n    # Scale the noise to the range [a, b]\n    scaled_noise = (noise - noise.mean()) / noise.std()  # Standardize the noise\n    scaled_noise = (scaled_noise * (upper - lower)) + (lower + upper) / 2  # Scale to the desired range\n    return scaled_noise\n@beartype\ndef add_probablistic_noise_to_prob(token_prob:torch.Tensor, probablistic_noise_ratio:ReplaceRatio):\n    min_prob = float(token_prob.min())\n    max_prob = float(token_prob.max())\n    noise_prob = generate_gaussian_noise_within_bounds(token_prob.shape, min_prob, max_prob)\n    token_prob_with_n"
        },
        {
            "comment": "This function takes an autoregressive generator and a target token probability generator, along with other parameters. It iterates through the input tokens from the autoregressive generator, applying probabilistic noise to the output token probabilities and generating target tokens based on thought token locations and non-thought token vocabulary.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":421-442",
            "content": "oise = token_prob + noise_prob * probablistic_noise_ratio\n    return token_prob_with_noise\n# demo on how to use thought tokens.\n@beartype\ndef generative_insert_yield_train_pairs(\n    autoregressive_generator: Iterable,\n    target_token_prob_generator: Callable[[torch.Tensor], torch.Tensor],\n    padded_thought_token_locations: torch.Tensor,\n    thought_token_vocabulary: list[int],\n    non_thought_token_vocabulary: list[int],\n    train_window_size: int,\n    probablistic_noise_ratio: ReplaceRatio,\n):\n    for i, (input_tokens, _) in enumerate(autoregressive_generator):\n        with torch.no_grad():\n            output_token_prob = target_token_prob_generator(input_tokens)\n            output_token_prob = add_probablistic_noise_to_prob(output_token_prob,probablistic_noise_ratio)\n        thought_token_locations = crop_target_token_by_index_and_window_size(\n            padded_thought_token_locations, i, train_window_size\n        )\n        target_tokens = generate_target_tokens_with_thought_token_loctions_and_non_t"
        },
        {
            "comment": "This code defines a function that generates thought tokens based on input and target tokens. It also performs testing with random input tokens, inserting thought tokens at a specified rate, and calculates the vocabulary for base, thought, and non-thought tokens. The code then prints a separator line.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":442-479",
            "content": "hought_token_vocabulary(\n            output_token_prob,\n            thought_token_locations,\n            thought_token_vocabulary,\n            non_thought_token_vocabulary,\n        )\n        yield input_tokens, target_tokens\n# output thought tokens affect input tokens?\nif __name__ == \"__main__\":\n    # begin test\n    base_token_count = 1000\n    thought_token_count = 2000\n    pad_token_idx = base_token_count + thought_token_count\n    total_token_count = pad_token_idx + 1\n    thought_token_insert_rate = 0.2\n    source_batchsize = 1\n    source_seqlen = 20\n    source_size = (source_batchsize, source_seqlen)\n    train_window_size = 10\n    thought_token_vocabulary = [\n        base_token_count + i for i in range(thought_token_count)\n    ]\n    non_thought_token_vocabulary = [\n        i for i in range(total_token_count) if i not in thought_token_vocabulary\n    ]\n    source_tokens = torch.randint(\n        0, base_token_count, source_size\n    )  # okay, lower than upper bound.\n    print(\"[autoregressive]\".center(50, \"-\"))\n   "
        },
        {
            "comment": "Iterating through thought token insertion pairs for source tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/qstar_my_guess/thought_tokens.py\":479-488",
            "content": " for input_tokens, target_tokens in insert_thought_tokens_and_yield_train_pairs(\n        ThoughtTokenInsertionMethod.autoregressive,\n        source_tokens,\n        thought_token_vocabulary,\n        thought_token_insert_rate,\n    ):  \n        print(input_tokens)\n        print(target_tokens)\n        print(\"-\"*50)\n        # breakpoint()"
        }
    ]
}