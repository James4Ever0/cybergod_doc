{
    "summary": "The code processes HID actions, handles various inputs and formats, validates types, includes classes for data management and training, ensures proper shape and logging. It manages training data in an image processing program using SequentialEvalQueue class with LSTM, fixes bugs 9 and 13, optimizes data type bits, prepares input data for RNN, uses Einops library with ViTDecoder to process selected image bits, rearranges and copies data based on shared and exclusive indices, performs 1D convolution with filters, and generates output.",
    "details": [
        {
            "comment": "This code is importing necessary libraries and defining classes for handling HID (Human Interface Device) actions such as keyboard and mouse events. It also includes type hints for different action types.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":0-45",
            "content": "# TODO: design a way to \"observe\" current holding keys, current mouse location, encode that observation and feed into model input along with screen image data\n# import pynput\n# no such dependency when training.\nimport einops\nimport os\nimport numpy as np\nimport cv2\nimport ast\nfrom pydantic import BaseModel, validator\nfrom typing import Union, Mapping, List\n# import logging\nfrom log_utils import logger\nfrom pydantic_numpy import NDArray\nimport torch\ntry:\n    from typing import Literal\nexcept:\n    from typing_extensions import Literal  # this is a failsafe.\ntry:\n    from typing import TypeAlias\nexcept:\n    from typing_extensions import TypeAlias\n##############\n#  HID BASE  #\n##############\nclass HIDActionTypes:\n    keyboard_action_types: TypeAlias = Literal[\n        \"key_press\",\n        \"key_release\",\n    ]\n    mouse_action_types: TypeAlias = Literal[\n        \"mouse_move\",\n        \"mouse_click\",\n        \"mouse_scroll\",\n    ]\n    action_types: TypeAlias = Literal[\n        keyboard_action_types,\n        mouse_action_types,\n    "
        },
        {
            "comment": "This code defines two type aliases, \"mouse_buttons\" and \"keys\", which represent specific button presses and key presses respectively. The mouse_buttons alias includes the literal strings for left, middle, and right mouse buttons, while the keys alias contains a list of literal strings representing commonly used keys on a keyboard.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":45-99",
            "content": "]\n    mouse_buttons: TypeAlias = Literal[\n        \"Button.left\",\n        \"Button.middle\",\n        \"Button.right\",\n    ]\n    keys: TypeAlias = Literal[\n        \"\"\"','\"\"\",\n        \"\"\"'.'\"\"\",\n        \"\"\"'/'\"\"\",\n        \"\"\"';'\"\"\",\n        \"\"\"\\\"'\\\"\"\"\",\n        \"\"\"'['\"\"\",\n        \"\"\"']'\"\"\",\n        \"\"\"'\\\\'\"\"\",\n        \"\"\"'='\"\"\",\n        \"\"\"'-'\"\"\",\n        \"\"\"'0'\"\"\",\n        \"\"\"'9'\"\"\",\n        \"\"\"'8'\"\"\",\n        \"\"\"'7'\"\"\",\n        \"\"\"'6'\"\"\",\n        \"\"\"'5'\"\"\",\n        \"\"\"'4'\"\"\",\n        \"\"\"'3'\"\"\",\n        \"\"\"'2'\"\"\",\n        \"\"\"'1'\"\"\",\n        \"\"\"'`'\"\"\",\n        \"\"\"'a'\"\"\",\n        \"\"\"'b'\"\"\",\n        \"\"\"'c'\"\"\",\n        \"\"\"'d'\"\"\",\n        \"\"\"'e'\"\"\",\n        \"\"\"'f'\"\"\",\n        \"\"\"'g'\"\"\",\n        \"\"\"'h'\"\"\",\n        \"\"\"'i'\"\"\",\n        \"\"\"'j'\"\"\",\n        \"\"\"'k'\"\"\",\n        \"\"\"'l'\"\"\",\n        \"\"\"'m'\"\"\",\n        \"\"\"'n'\"\"\",\n        \"\"\"'o'\"\"\",\n        \"\"\"'p'\"\"\",\n        \"\"\"'q'\"\"\",\n        \"\"\"'r'\"\"\",\n        \"\"\"'s'\"\"\",\n        \"\"\"'t'\"\"\",\n        \"\"\"'u'\"\"\",\n        \"\"\"'v'\"\"\",\n        \"\"\"'w'\"\"\",\n        \"\"\"'x'\"\"\",\n        \"\"\"'y'\"\"\",\n        \"\"\"'z'\"\"\",\n       "
        },
        {
            "comment": "This code defines a list of key names for the pynput.keyboard.Key module and creates two lists for keyboard action types. The first class, HIDActionBase, has two attributes: mouse_resolution (1000) and keyboard_action_types (list of keyboard action types).",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":99-148",
            "content": " \"Key.alt\",  # check pynput.keyboard.Key\n        \"Key.alt_r\",\n        \"Key.backspace\",\n        \"Key.caps_lock\",\n        \"Key.cmd\",\n        \"Key.cmd_r\",\n        \"Key.ctrl\",\n        \"Key.ctrl_r\",\n        \"Key.delete\",\n        \"Key.down\",\n        \"Key.end\",\n        \"Key.enter\",\n        \"Key.esc\",\n        \"Key.f1\",\n        \"Key.f2\",\n        \"Key.f3\",\n        \"Key.f4\",\n        \"Key.f5\",\n        \"Key.f6\",\n        \"Key.f7\",\n        \"Key.f8\",\n        \"Key.f9\",\n        \"Key.f10\",\n        \"Key.f11\",\n        \"Key.f12\",\n        \"Key.f13\",\n        \"Key.f14\",\n        \"Key.f15\",\n        \"Key.f16\",\n        \"Key.f17\",\n        \"Key.f18\",\n        \"Key.f19\",\n        \"Key.f20\",\n        \"Key.home\",\n        \"Key.left\",\n        \"Key.page_down\",\n        \"Key.page_up\",\n        \"Key.right\",\n        \"Key.shift\",\n        \"Key.shift_r\",\n        \"Key.space\",\n        \"Key.tab\",\n        \"Key.up\",\n    ]\nclass HIDActionBase:\n    mouse_resolution: int = 1000\n    keyboard_action_types = list(HIDActionTypes.keyboard_action_types.__args__)\n    mouse_action_types = list("
        },
        {
            "comment": "Extracting argument types for action types, mouse buttons, and keys.\nCalculating total length including base type and additional mappings for a given keycode.\nStatic method to unshift keycodes based on specific conditions.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":148-189",
            "content": "HIDActionTypes.mouse_action_types.__args__)\n    action_types = list(HIDActionTypes.action_types.__args__)\n    mouse_buttons = list(HIDActionTypes.mouse_buttons.__args__)\n    keys = list(HIDActionTypes.keys.__args__)\n    length = (\n        len(action_types)\n        + len(keys)\n        + len(mouse_buttons)\n        + 1  # mouse pressed\n        + 4 * mouse_resolution\n    )  # ,\n    #             1)\n    @staticmethod\n    def unshift_keycode(keycode: str) -> Union[str, None]:\n        unshift_keycodes = {\n            \"!\": \"1\",\n            \"@\": \"2\",\n            \"#\": \"3\",\n            \"$\": \"4\",\n            \"%\": \"5\",\n            \"^\": \"6\",\n            \"&\": \"7\",\n            \"*\": \"8\",\n            \"(\": \"9\",\n            \")\": \"0\",\n            \"_\": \"-\",\n            \"+\": \"=\",\n            \"{\": \"[\",\n            \"}\": \"]\",\n            \"|\": \"\\\\\",\n            \":\": \";\",\n            '\"': \"'\",\n            \"<\": \",\",\n            \">\": \".\",\n            \"?\": \"/\",\n            \"~\": \"`\",\n        }\n        ctrl_keycodes = {\n            \"\\x01\": \"a\",\n           "
        },
        {
            "comment": "This code is mapping various characters and special keys to their corresponding alphabets and numbers.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":189-231",
            "content": " \"\\x02\": \"b\",\n            \"\\x03\": \"c\",\n            \"\\x04\": \"d\",\n            \"\\x05\": \"e\",\n            \"\\x06\": \"f\",\n            \"\\x07\": \"g\",\n            \"\\x08\": \"h\",\n            \"\\t\": \"i\",\n            \"\\n\": \"j\",\n            \"\\x0b\": \"k\",\n            \"\\x0c\": \"l\",\n            \"\\r\": \"m\",\n            \"\\x0e\": \"n\",\n            \"\\x0f\": \"o\",\n            \"\\x10\": \"p\",\n            \"\\x11\": \"q\",\n            \"\\x12\": \"r\",\n            \"\\x13\": \"s\",\n            \"\\x14\": \"t\",\n            \"\\x15\": \"u\",\n            \"\\x16\": \"v\",\n            \"\\x17\": \"w\",\n            \"\\x18\": \"x\",\n            \"\\x19\": \"y\",\n            \"\\x1a\": \"z\",\n            \"<219>\": \"[\",\n            \"<221>\": \"]\",\n            \"<189>\": \"-\",\n            \"<187>\": \"=\",\n            \"<192>\": \"`\",\n            \"<48>\": \"0\",\n            \"<49>\": \"1\",\n            \"<50>\": \"2\",\n            \"<51>\": \"3\",\n            \"<52>\": \"4\",\n            \"<53>\": \"5\",\n            \"<54>\": \"6\",\n            \"<55>\": \"7\",\n            \"<56>\": \"8\",\n            \"<57>\": \"9\",\n            \"<220>\": \"\\\\\",\n            \"<186>\": \";\",\n "
        },
        {
            "comment": "This code is handling keycode conversion for a keyboard input processing system. It maps certain special characters to their corresponding codes, and handles unconvertable keycodes by discarding them. If the keycode starts with \"<\" and ends with \">\", it's treated as a special keycode. The function then attempts to convert the keycode, and if successful, returns the converted keycode; otherwise, it returns None.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":231-256",
            "content": "           \"<222>\": \"'\",\n            \"<188>\": \",\",\n            \"<190>\": \".\",\n            \"<191>\": \"/\",\n        }\n        keycode = unshift_keycodes.get(keycode, ctrl_keycodes.get(keycode, keycode))\n        # still, this is something out of concern.\n        if keycode.startswith(\"<\") and keycode.endswith(\">\"):\n            logger.warning(\"Discarding unconvertable keycode: %s\" % keycode)\n            # keycode = pynput.keyboard.KeyCode(int(keycode[1:-1]))\n            return\n        return keycode\n    @staticmethod\n    def uncover_keycode(keycode: str) -> Union[str, None]:\n        if not keycode.startswith(\"Key.\"):\n            keycode_converted = HIDActionBase.unshift_keycode(\n                keycode\n                if keycode.startswith(\"<\") and keycode.endswith(\">\")\n                else ast.literal_eval(keycode)\n            )\n            return keycode_converted\n            # this could be None.\n            # when this is None, simply skip this code. do not end the conversion. skip it.\n        else:\n            "
        },
        {
            "comment": "This class represents a HID action that can be a key press, key release, mouse move, mouse click, or mouse scroll. It has properties for the action type, key, mouse button, and coordinates. The to_ndarray method converts the instance to a numpy array, while the to_action method returns an action string in the format [\"<type>\", \"<key or coordinates>\"].",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":256-285",
            "content": "return keycode\nclass HIDAction(BaseModel, HIDActionBase):\n    # static method: from_action\n    # static method: from_ndarray\n    # instance method: to_ndarray\n    # instance method: to_action\n    max_x: int\n    max_y: int\n    action_type: Literal[\n        \"key_press\",  # [\"key_press\", \"'w'\"]\n        \"key_release\",  # [\"key_release\", \"'r'\"]\n        \"mouse_move\",  # [\"mouse_move\", [176.7734375, 580.40625]], \"timeStamp\": 1680247557.125498}\n        \"mouse_click\",  # [\"mouse_click\", [176.7734375, 580.40625, \"Button.left\", true]]\n        \"mouse_scroll\",  # [\"mouse_scroll\", [938.76171875, 318.75, 0, 0]]\n        #         None,  # end_of_action\n    ]  # you need to specify this.\n    key: Union[\n        HIDActionTypes.keys,\n        None,\n    ] = None\n    mouse_button: Union[HIDActionTypes.mouse_buttons, None] = None\n    mouse_pressed: Union[bool, None] = None\n    x: Union[float, None] = None\n    y: Union[float, None] = None\n    dx: Union[float, None] = None\n    dy: Union[float, None] = None\n    @validator(\"max_x\", \"max_"
        },
        {
            "comment": "This code defines several validator functions and a static method for a class. The validator functions check if the input values are of the correct type and within specific ranges or sets, returning the input if it passes the checks, otherwise raising an exception. The static method, \"from_action_json,\" constructs objects from a JSON list representation, handling some potential issues with single char keys.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":285-323",
            "content": "y\")\n    def greater_than_zero(cls, v):\n        assert type(v) == int\n        assert v > 0\n        return v\n    @validator(\"action_type\")\n    def action_type_within_action_types(cls, v):\n        if v:\n            assert v in HIDActionBase.action_types\n        return v\n    @validator(\"key\")\n    def key_within_keys(cls, v):\n        if v:\n            assert v in HIDActionBase.keys\n        return v\n    @validator(\"mouse_button\")\n    def mouse_button_within_mouse_buttons(cls, v):\n        if v:\n            assert v in HIDActionBase.mouse_buttons\n        return v\n    @validator(\"mouse_pressed\")\n    def mouse_pressed_type_check(cls, v):\n        if v:\n            assert type(v) == bool\n        return v\n    @staticmethod\n    def from_action_json(action_json: list, max_x: int, max_y: int):\n        action_type = action_json[0]\n        action_args = action_json[1]\n        construct_args = dict(max_x=max_x, max_y=max_y, action_type=action_type)\n        # BUG: convert single char keys to quoted format.\n        # TODO: make sure ' '"
        },
        {
            "comment": "Checks if the action type is a key press or release, mouse move, or mouse click. If it's a key action, converts it into Key.Space format. For mouse actions, checks that x and y coordinates are within valid ranges. Updates construct_args accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":323-347",
            "content": " is converted into Key.Space\n        if action_type.startswith(\"key\"):\n            if len(action_args) == 1:\n                if action_args != \"'\":\n                    action_args = f\"'{action_args}'\"\n                else:\n                    action_args = f'\"{action_args}\"'\n            if action_args == repr(\" \"):\n                action_args = \"Key.space\"\n        if action_type == \"key_press\":\n            assert action_args in HIDActionBase.keys\n            construct_args.update(dict(key=action_args))\n        elif action_type == \"key_release\":\n            assert action_args in HIDActionBase.keys\n            construct_args.update(dict(key=action_args))\n        elif action_type == \"mouse_move\":\n            assert action_args[0] >= 0 and action_args[0] <= max_x\n            assert action_args[1] >= 0 and action_args[1] <= max_y\n            construct_args.update(dict(x=action_args[0], y=action_args[1]))\n        elif action_type == \"mouse_click\":\n            assert action_args[0] >= 0 and action_args[0] <= max"
        },
        {
            "comment": "The code checks the type and validity of input arguments for constructing different types of actions. It updates a dictionary with the action's parameters based on the action type (mouse click or scroll).",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":347-375",
            "content": "_x\n            assert action_args[1] >= 0 and action_args[1] <= max_y\n            assert action_args[2] in HIDActionBase.mouse_buttons\n            assert type(action_args[3]) == bool\n            construct_args.update(\n                dict(\n                    x=action_args[0],\n                    y=action_args[1],\n                    mouse_button=action_args[2],\n                    mouse_pressed=action_args[3],\n                )\n            )\n        elif action_type == \"mouse_scroll\":\n            assert action_args[0] >= 0 and action_args[0] <= max_x\n            assert action_args[1] >= 0 and action_args[1] <= max_y\n            assert action_args[2] >= -max_x and action_args[2] <= max_x\n            assert action_args[3] >= -max_y and action_args[3] <= max_y\n            construct_args.update(\n                dict(\n                    x=action_args[0],\n                    y=action_args[1],\n                    dx=action_args[2],\n                    dy=action_args[3],\n                )\n            )\n        else:\n"
        },
        {
            "comment": "Raises an exception if the action type is unknown.\nCreates a HIDAction object using given arguments.\nStatic method to create HIDAction from numpy array.\nAsserts that the shape of ndarray is (HIDActionBase.length,).\nGets the action type index from ndarray.\nConstructs construct_args with max_x, max_y, and action_type.\nIf action type exists, gets key index from ndarray and retrieves key.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":375-401",
            "content": "            raise Exception(\n                \"Unknown action type: %s\\naction args: %s\" % (action_type, action_args)\n            )\n        mHIDAction = HIDAction(**construct_args)\n        return mHIDAction\n    @staticmethod\n    def from_ndarray(ndarray: np.ndarray, max_x: int, max_y: int):\n        assert ndarray.shape == (HIDActionBase.length,)\n        cursor = 0\n        action_type_ndarray = ndarray[cursor : cursor + len(HIDActionBase.action_types)]\n        cursor += len(HIDActionBase.action_types)\n        action_type_index = np.argmax(action_type_ndarray)\n        action_type = HIDActionBase.action_types[action_type_index]\n        del action_type_ndarray\n        del action_type_index\n        construct_args = dict(max_x=max_x, max_y=max_y, action_type=action_type)\n        if action_type:\n            key_ndarray = ndarray[cursor : cursor + len(HIDActionBase.keys)]\n            cursor += len(HIDActionBase.keys)\n            key_index = np.argmax(key_ndarray)\n            key = HIDActionBase.keys[key_index]\n    "
        },
        {
            "comment": "Deleting variables, processing mouse button and position data, updating cursor position.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":401-426",
            "content": "        del key_ndarray\n            del key_index\n            mouse_button_ndarray = ndarray[\n                cursor : cursor + len(HIDActionBase.mouse_buttons)\n            ]\n            cursor += len(HIDActionBase.mouse_buttons)\n            mouse_button_index = np.argmax(mouse_button_ndarray)\n            mouse_button = HIDActionBase.mouse_buttons[mouse_button_index]\n            del mouse_button_ndarray\n            del mouse_button_index\n            mouse_pressed_ndarray = ndarray[cursor : cursor + 1]\n            cursor += 1\n            mouse_pressed = bool(mouse_pressed_ndarray[0][0])\n            del mouse_pressed_ndarray\n            x_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            cursor += HIDActionBase.mouse_resolution\n            x_index = np.argmax(x_ndarray)\n            x = (x_index / HIDActionBase.mouse_resolution) * max_x\n            del x_ndarray\n            del x_index\n            y_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            c"
        },
        {
            "comment": "Calculating mouse and key inputs based on ndarray max index, updating construct_args accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":426-449",
            "content": "ursor += HIDActionBase.mouse_resolution\n            y_index = np.argmax(y_ndarray)\n            y = (y_index / HIDActionBase.mouse_resolution) * max_y\n            del y_ndarray\n            del y_index\n            dx_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            cursor += HIDActionBase.mouse_resolution\n            dx_index = np.argmax(dx_ndarray)\n            dx = (dx_index / HIDActionBase.mouse_resolution) * 2 * max_x - max_x\n            del dx_ndarray\n            del dx_index\n            dy_ndarray = ndarray[cursor : cursor + HIDActionBase.mouse_resolution]\n            cursor += HIDActionBase.mouse_resolution\n            dy_index = np.argmax(dy_ndarray)\n            dy = (dy_index / HIDActionBase.mouse_resolution) * 2 * max_y - max_y\n            del dy_ndarray\n            del dy_index\n            if action_type == \"key_press\":\n                construct_args.update(dict(key=key))\n            elif action_type == \"key_release\":\n                construct_args.update(dict(key=ke"
        },
        {
            "comment": "This code is handling different types of mouse actions. It updates construct_args based on the action type and creates a HIDAction object using those arguments. If the number is greater than mouse_resolution, it logs a warning.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":449-476",
            "content": "y))\n            elif action_type == \"mouse_move\":\n                construct_args.update(dict(x=x, y=y))\n            elif action_type == \"mouse_click\":\n                construct_args.update(\n                    dict(\n                        x=x, y=y, mouse_button=mouse_button, mouse_pressed=mouse_pressed\n                    )\n                )\n            elif action_type == \"mouse_scroll\":\n                construct_args.update(dict(x=x, y=y, dx=dx, dy=dy))\n        else:\n            pass\n        del cursor\n        mHIDAction = HIDAction(**construct_args)\n        return mHIDAction\n    def round_within(self, number: Union[int, float], number_name: str) -> int:\n        result = round(number)\n        if result > self.mouse_resolution:\n            logger.warning(f\"Warning: {number_name} overflow\")\n            logger.warning(f\"Value {result} greater than {self.mouse_resolution}\")\n            return self.mouse_resolution\n        elif result < 0:\n            logger.warning(f\"Warning: {number_name} overflow\")\n         "
        },
        {
            "comment": "Creates a numpy array for each action type, key, mouse button, and coordinates, with 1's at their respective indices if they are not None.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":476-507",
            "content": "   logger.warning(f\"Value {result} smaller than 0\")\n            return 0\n        return result\n    def to_ndarray(\n        self,\n    ) -> np.ndarray:\n        action_type_ndarray = np.zeros((len(self.action_types), 1))\n        action_type_ndarray[self.action_types.index(self.action_type)] = 1\n        key_ndarray = np.zeros((len(self.keys), 1))\n        if self.key:\n            key_ndarray[self.keys.index(self.key)] = 1\n        mouse_button_ndarray = np.zeros((len(self.mouse_buttons), 1))\n        if self.mouse_button:\n            mouse_button_ndarray[self.mouse_buttons.index(self.mouse_button)] = 1\n        mouse_pressed_array = np.zeros((1, 1))\n        if self.mouse_pressed:\n            mouse_pressed_array[0] = 1\n        x_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.x:\n            x_ndarray[\n                self.round_within(self.mouse_resolution * self.x / self.max_x, \"X\")\n            ] = 1\n        y_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.y:\n            y_ndarray[\n     "
        },
        {
            "comment": "The code initializes and concatenates multiple numpy arrays representing various actions and parameters (action type, key pressed, mouse button, mouse position, etc.). It then returns the concatenated array.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":507-542",
            "content": "           self.round_within(self.mouse_resolution * self.y / self.max_y, \"Y\")\n            ] = 1\n        dx_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.dx:\n            dx_ndarray[\n                self.round_within(\n                    self.mouse_resolution * (self.dx + self.max_x) / (2 * self.max_x),\n                    \"DX\",\n                )\n            ] = 1\n        dy_ndarray = np.zeros((self.mouse_resolution, 1))\n        if self.dy:\n            dy_ndarray[\n                self.round_within(\n                    self.mouse_resolution * (self.dy + self.max_y) / (2 * self.max_y),\n                    \"DY\",\n                )\n            ] = 1\n        ndarray = np.concatenate(\n            [\n                action_type_ndarray,\n                key_ndarray,\n                mouse_button_ndarray,\n                mouse_pressed_array,\n                x_ndarray,\n                y_ndarray,\n                dx_ndarray,\n                dy_ndarray,\n            ]\n        )\n        return ndarray\n    def to_actio"
        },
        {
            "comment": "This function checks the action type and constructs appropriate arguments for each type of action (key press/release, mouse move/click/scroll). It asserts that all required conditions are met before constructing the action_args.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":542-568",
            "content": "n_json(\n        self,\n    ) -> Union[list, None]:\n        action_type = self.action_type\n        if action_type:\n            if action_type == \"key_press\":\n                assert self.key in self.keys\n                action_args = self.key\n            elif action_type == \"key_release\":\n                assert self.key in self.keys\n                action_args = self.key\n            elif action_type == \"mouse_move\":\n                assert self.x >= 0 and self.x <= self.max_x\n                assert self.y >= 0 and self.y <= self.max_y\n                action_args = [self.x, self.y]\n            elif action_type == \"mouse_click\":\n                assert self.x >= 0 and self.x <= self.max_x\n                assert self.y >= 0 and self.y <= self.max_y\n                assert self.mouse_button in self.mouse_buttons\n                assert type(self.mouse_pressed) == bool\n                action_args = [self.x, self.y, self.mouse_button, self.mouse_pressed]\n            elif action_type == \"mouse_scroll\":\n                as"
        },
        {
            "comment": "Code is validating input values and converting them into JSON format for use in the application. It also handles exceptions for unknown action types and ensures proper data structure.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":568-604",
            "content": "sert self.x >= 0 and self.x <= self.max_x\n                assert self.y >= 0 and self.y <= self.max_y\n                assert self.dx >= -self.max_x and self.dx <= self.max_x\n                assert self.dy >= -self.max_y and self.dy <= self.max_y\n                action_args = [self.x, self.y, self.dx, self.dy]\n            else:\n                raise Exception(\"Unknown action_type: %s\" % action_type)\n            action_json = [action_type, action_args]\n        else:\n            action_json = None\n        return action_json\n#########################\n#  HID DATA VALIDATION  #\n#########################\nfrom pydantic import confloat\nclass KeyPress(BaseModel):\n    _action_type = \"key_press\"\n    key: HIDActionTypes.keys\n    def to_list(self) -> List:\n        return [self._action_type, self.key]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = [lst[1]]\n        assert len(action_args) == 1\n        assert action_type == cls._action_type\n        assert isinstance(action"
        },
        {
            "comment": "This code defines two classes, KeyRelease and MouseClick, that inherit from BaseModel. Both classes have methods to convert objects to a list and vice versa. The to_list method returns a list representing the action type and relevant parameters (key for KeyRelease, coordinates, button, and press state for MouseClick). The from_list classmethod parses a list and creates an instance of the respective class with the correct values.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":604-647",
            "content": "_args[0], HIDActionTypes.keys)\n        return cls(key=action_args[0])\nclass KeyRelease(BaseModel):\n    _action_type = \"key_release\"\n    key: HIDActionTypes.keys\n    def to_list(self) -> List:\n        return [self._action_type, self.key]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = [lst[1]]\n        assert len(action_args) == 1\n        assert action_type == cls._action_type\n        assert isinstance(action_args[0], HIDActionTypes.keys)\n        return cls(key=action_args[0])\nclass MouseClick(BaseModel):\n    _action_type = \"mouse_click\"\n    x: confloat(ge=0)\n    y: confloat(ge=0)\n    button: HIDActionTypes.mouse_buttons\n    pressed: bool\n    def to_list(self) -> List:\n        return [self._action_type, [self.x, self.y, self.button, self.pressed]]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = lst[1]\n        assert len(action_args) == 4\n        assert action_type == cls._action_type\n        assert isinstanc"
        },
        {
            "comment": "This code defines two classes, `MouseMove` and `MouseScroll`, which inherit from the `BaseModel` class. Both classes represent different types of mouse actions. The constructor for each class takes arguments representing the x and y coordinates (both expected to be non-negative floats) and a boolean indicating whether a button is pressed or not. The `to_list()` method converts an instance of these classes into a list, while the `from_list()` classmethod recreates an instance from a list representation.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":647-686",
            "content": "e(action_args[0], confloat(ge=0))\n        assert isinstance(action_args[1], confloat(ge=0))\n        assert isinstance(action_args[2], HIDActionTypes.mouse_buttons)\n        assert isinstance(action_args[3], bool)\n        return cls(\n            x=action_args[0],\n            y=action_args[1],\n            button=action_args[2],\n            pressed=action_args[3],\n        )\nclass MouseMove(BaseModel):\n    _action_type = \"mouse_move\"\n    x: confloat(ge=0)\n    y: confloat(ge=0)\n    def to_list(self) -> List:\n        return [self._action_type, [self.x, self.y]]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = lst[1]\n        assert len(action_args) == 2\n        assert action_type == cls._action_type\n        assert isinstance(action_args[0], confloat(ge=0))\n        assert isinstance(action_args[1], confloat(ge=0))\n        return cls(x=action_args[0], y=action_args[1])\nclass MouseScroll(BaseModel):\n    _action_type = \"mouse_scroll\"\n    x: confloat(ge=0)\n    y: confloat("
        },
        {
            "comment": "This code defines a class for storing x and y coordinates, along with dx and dy values. It has methods to convert the object into a list of its attributes and vice versa. Additionally, it includes assertions to ensure that the input list conforms to certain data types and conditions. The code also includes the definition of a VideoCaptureContextManager class for working with video capture contexts using OpenCV's VideoCapture API.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":686-725",
            "content": "ge=0)\n    dx: float\n    dy: float\n    def to_list(self) -> List:\n        return [self._action_type, [self.x, self.y, self.dx, self.dy]]\n    @classmethod\n    def from_list(cls, lst: List):\n        action_type = lst[0]\n        action_args = lst[1]\n        assert len(action_args) == 4\n        assert action_type == cls._action_type\n        assert isinstance(action_args[0], confloat(ge=0))\n        assert isinstance(action_args[1], confloat(ge=0))\n        assert isinstance(action_args[2], float)\n        assert isinstance(action_args[3], float)\n        return cls(\n            x=action_args[0], y=action_args[1], dx=action_args[2], dy=action_args[3]\n        )\n#################\n# VIDEO CONTEXT #\n#################\nclass VideoCaptureContextManager:\n    def __init__(self, videoPath):\n        self.videoPath = videoPath\n    def __enter__(self):\n        logger.info(\"Entering the context...\")\n        self.cap = cv2.VideoCapture(self.videoPath)\n        return self.cap\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        try"
        },
        {
            "comment": "This code defines a class called ConsciousBase, which has variables related to data types, special tokens, and image dimensions for image processing. It also calculates the length of the various components in the consciousness structure.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":725-762",
            "content": ":\n            self.cap.release()\n        finally:\n            import gc\n            gc.collect()\n            logger.info(\"Leaving the context...\")\n        #  print(exc_type, exc_value, exc_tb, sep=\"\\n\")\n##################\n# CONSCIOUS BASE #\n##################\nclass ConsciousBase:\n    data_types = [\"image\", \"HIDAction\"]\n    special_tokens = [\"image_newline\", \"image_end\", \"action_end\", None]\n    #     vector_size = 1+2+1000+4110 # visual things are pre-encoded. no raw image here!\n    # vector size is \"length\" now\n    image_dim = 224\n    image_channels = 3\n    data_type_length = len(data_types)\n    special_token_length = len(special_tokens)\n    image_length = image_dim * image_dim * image_channels  # obviously flattened.\n    # FIX 1: plus to colon.\n    split_sizes = [\n        len(data_types),\n        len(special_tokens),\n        image_length,  # FIX 9: change to flattened image bits count\n        HIDActionBase.length,  # 4110?\n    ]\n    length = sum(split_sizes)\n    # you cannot easily revert this compression by arg"
        },
        {
            "comment": "This code defines a class called \"ConsciousBlock\" which is derived from \"BaseModel\" and \"ConsciousBase\". It has attributes for data_type, special_token, image_data, and action_data. The class also includes methods to convert the object to JSON format and create an instance of the class from a dictionary representation of its fields.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":762-797",
            "content": "max or something else.\n    # so you need the decoder.\n# can it be consciousnessless?\nclass ConsciousBlock(BaseModel, ConsciousBase):\n    data_type: Literal[\"image\", \"HIDAction\"]  # 2 bits, required\n    special_token: Union[\n        Literal[\n            \"image_newline\",\n            \"image_end\",\n            \"action_end\",  # change some of these bits into -torch.inf, so you won't have paradox like results.\n        ],\n        None,\n    ] = None  # 4 bits\n    image_data: Union[\n        None, NDArray\n    ] = None  # what is the shape of this image data? assume to be [3,224,224] (c h w) flattened\n    action_data: Union[None, NDArray] = None  # assume to be: (4110, )\n    # [1,1000] -> [3,1000,1000] -> [3,224,224]\n    #    einsum.repeat       conv2d\n    # so, maybe you still need some ViT decode layer.\n    @staticmethod\n    def from_json(data: Mapping):\n        mConsciousBlock = ConsciousBlock(**data)\n        return mConsciousBlock\n    def to_json(self) -> Mapping:\n        mJson = self.dict()\n        return mJson\n    @st"
        },
        {
            "comment": "Converts a tensor into a ConsciousBlock instance by splitting based on provided sizes and assigning appropriate data types.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":797-820",
            "content": "aticmethod\n    def from_tensor(tensor: torch.Tensor):\n        # check its shape.\n        assert tensor.shape == (ConsciousBlock.length,)\n        split_sizes = ConsciousBase.split_sizes\n        data_bits, special_bits, image_data, action_data = einops.unpack(\n            tensor, [[s] for s in split_sizes], \"*\"\n        )\n        #         data_bits, special_bits, image_data, action_data = size_splits(tensor, split_sizes)\n        data_type = ConsciousBase.data_types[int(torch.argmax(data_bits))]\n        if data_type == \"image\":\n            special_bits[2] = -torch.inf\n            special_token = ConsciousBase.special_tokens[\n                int(torch.argmax(special_bits))\n            ]\n            mConsciousBlock = ConsciousBlock(\n                data_type=data_type, special_token=special_token, image_data=image_data\n            )\n        elif data_type == \"HIDAction\":\n            special_bits[0:2] = -torch.inf\n            special_token = ConsciousBase.special_tokens[\n                int(torch.argmax(special_"
        },
        {
            "comment": "The code defines a class called ConsciousBlock which has a constructor and a method to convert the object into a Tensor. The constructor takes in data_type, special_token, and action_data as parameters. If an unknown data_type is provided, it raises an exception. The to_tensor() method converts the ConsciousBlock object into a tensor using Einops library's unpack function, then sets the bit corresponding to the data_type of the ConsciousBlock to 1. There are two bugs in this code: BUG 1 - not enough values to unpack; BUG 2 - comparing ndarray to None.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":820-844",
            "content": "bits))\n            ]\n            mConsciousBlock = ConsciousBlock(\n                data_type=data_type,\n                special_token=special_token,\n                action_data=action_data,\n            )\n        else:\n            raise Exception(\"Unknown data_type:\", data_type)\n        return mConsciousBlock\n    def to_tensor(self) -> torch.Tensor:\n        mTensorBase = torch.zeros((ConsciousBase.length))\n        # BUG 1: not enough values to unpack\n        #         print(ConsciousBase.length)\n        #         print(ConsciousBase.split_sizes)\n        data_bits, special_bits, image_data, action_data = einops.unpack(\n            mTensorBase, [[s] for s in ConsciousBase.split_sizes], \"*\"\n        )\n        #         data_bits, special_bits, image_data, action_data = size_splits(mTensorBase, ConsciousBase.split_sizes)\n        data_bits[ConsciousBase.data_types.index(self.data_type)] = 1\n        if self.data_type == \"image\":\n            # BUG 2: comparing ndarray to None\n            # FIX 2: change \"!=\" into \"i"
        },
        {
            "comment": "Ensures image data and action data are not None, checks shapes, and converts to torch.Tensor.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":844-866",
            "content": "s not\"\n            assert self.image_data is not None\n            logger.debug(\"Image data shape: %s\", self.image_data.shape)\n            logger.debug(\n                \"Expected data shape: %s\",\n                expected_data_shape := (ConsciousBase.image_length,),\n            )\n            assert self.image_data.shape == expected_data_shape\n            assert self.special_token != \"action_end\"\n            image_data = torch.Tensor(self.image_data)\n            special_bits[ConsciousBase.special_tokens.index(self.special_token)] = 1\n        elif self.data_type == \"HIDAction\":\n            assert self.action_data is not None\n            if len(self.action_data.shape) > 1:\n                self.action_data = self.action_data.reshape((-1,))\n            logger.debug(\"Action data shape: %s\", self.action_data.shape)\n            # BUG: actual: (4110, 1)\n            # shall we reshape this.\n            logger.debug(\n                \"Expected data shape: %s\",\n                expected_data_shape := (HIDActionBase.lengt"
        },
        {
            "comment": "Creates a tensor from various data types, checking their shapes and handling special tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":866-892",
            "content": "h,),\n            )  # TODO: expected shape is (4110, )? how to make this typed?\n            assert self.action_data.shape == expected_data_shape\n            assert self.special_token not in [\"image_newline\", \"image_end\"]\n            action_data = torch.Tensor(self.action_data)\n            special_bits[ConsciousBase.special_tokens.index(self.special_token)] = 1\n        else:\n            # FIX: found by pyright (UndefinedVariable)\n            raise Exception(\"Unknown data_type:\", self.data_type)\n        mTensor, _ = einops.pack(\n            (data_bits, special_bits, image_data, action_data), \"*\"\n        )\n        #         mTensor = torch.concat((data_bits, special_bits, image_data, action_data))\n        del mTensorBase\n        return mTensor\nclass ConsciousFlow(BaseModel, ConsciousBase):\n    consciousBlocks: List[ConsciousBlock]\n    @staticmethod\n    def from_json(data: List[Mapping]):\n        mList = [ConsciousBlock.from_json(j) for j in data]\n        mConsciousFlow = ConsciousFlow(consciousBlocks=mList)\n "
        },
        {
            "comment": "This code defines classes for representing and manipulating conscious data in the form of flows and streams. The ConsciousFlow class has methods to convert between JSON and tensors, while the ConsciousStream class represents a list of such flows with methods to convert from JSON as well.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":892-920",
            "content": "       return mConsciousFlow\n    def to_json(self) -> List[Mapping]:\n        mJson = [c.to_json() for c in self.consciousBlocks]\n        return mJson\n    @staticmethod\n    def from_tensor(tensor: torch.Tensor):\n        consciousBlockCount, vector_length = tensor.shape\n        assert vector_length == ConsciousBase.length\n        mConsciousBlocks = []\n        for i in range(consciousBlockCount):\n            arr = tensor[i, :]  # dimension reduction.\n            mConsciousBlock = ConsciousBlock.from_tensor(arr)\n            mConsciousBlocks.append(mConsciousBlock)\n        mConsciousFlow = ConsciousFlow(consciousBlocks=mConsciousBlocks)\n        return mConsciousFlow\n    def to_tensor(self) -> torch.Tensor:\n        mTensor, _ = einops.pack([c.to_tensor() for c in self.consciousBlocks], \"* d\")\n        return mTensor\nclass ConsciousStream(BaseModel, ConsciousBase):\n    consciousFlows: List[ConsciousFlow]\n    @staticmethod\n    def from_json(data: List[Mapping]):\n        mList = [ConsciousFlow.from_json(j) for j in d"
        },
        {
            "comment": "This code defines a class called ConsciousStream which represents a stream of conscious flows. It has methods for creating an instance from a list of conscious flows, converting the stream to JSON format, converting it to a tensor, and converting individual conscious flows to tensors. The class also mentions Trainer and Dataset but they are not defined in this code snippet.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":920-951",
            "content": "ata]\n        mConsciousStream = ConsciousStream(consciousFlows=mList)\n        return mConsciousStream\n    def to_json(self) -> List[Mapping]:\n        mJson = [c.to_json() for c in self.consciousFlows]\n        return mJson\n    @staticmethod\n    def from_tensor(tensor: torch.Tensor):\n        consciousFlowCount, _, vector_length = tensor.shape\n        assert vector_length == ConsciousBase.length\n        mConsciousFlows = []\n        for i in range(consciousFlowCount):\n            arr = tensor[i, :, :]  # dimension reduction.\n            mConsciousFlow = ConsciousFlow.from_tensor(arr)\n            mConsciousFlows.append(mConsciousFlow)\n        mConsciousStream = ConsciousStream(consciousFlows=mConsciousFlows)\n        return mConsciousStream\n    def to_tensor(self) -> torch.Tensor:\n        mTensor, _ = einops.pack([c.to_tensor() for c in self.consciousFlows], \"* s d\")\n        return mTensor\n#####################\n# TRAINER & DATASET #\n#####################\nclass Trainer:\n    def __init__(self, model, loss_fn, optimiz"
        },
        {
            "comment": "This code defines a class that represents a conscious structure. It initializes the model, loss function, and optimizer in its constructor. The step method performs forward pass on the input data using the model's forward method, calculates the loss, and applies backpropagation to update the model's parameters. This conscious structure also includes an Enqueue class that can enqueue data and clear the queue.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":951-988",
            "content": "er):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer  # shall be registered to model parameters.\n    def step(self, batched_input, batched_output=None):\n        # BUG 8: model forward keyword error\n        # FIX 8: fixing keyword, adding default keyword argument\n        model_output = self.model.forward(batched_input, target_output=batched_output)\n        loss = self.loss_fn(model_output, batched_output)\n        logger.debug(\"LOSS? %s\", loss)\n        # this loss is incorrect. shall use some argmax stuff.\n        # to ensure that this thing is the thing that we want.\n        loss.backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\nfrom typing import Protocol\nclass Enqueue(Protocol):\n    def enqueue(self, data, /):\n        ...\n    def clear(self):\n        ...\nclass TestEnqueue(Enqueue):\n    def __init__(self):\n        ...\n        # self.queue = []\n    def enqueue(self, data):\n        logger.debug(\"DATA QUEUE: %s\", data)\n        # may you print th"
        },
        {
            "comment": "Checking data shape for action_data and image_data.\nCreating a SequentialTrainingQueue object with context_length, batch_size, and trainer.\nEnqueuing consciousBlock into the queue, optionally clearing it after enqueueing.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":988-1016",
            "content": "e data shape.\n        if data.action_data is not None:\n            logger.debug(\"ACTION DATA SHAPE: %s\", data.action_data.shape)\n        elif data.image_data is not None:\n            logger.debug(\"IMAGE DATA SHAPE: %s\", data.image_data.shape)\n        logger.debug(\"\")\n    def clear(self):\n        ...\nclass SequentialTrainingQueue:\n    def __init__(self, context_length: int, batch_size: int, trainer: Trainer):\n        self.context_length = context_length\n        self.batch_size = batch_size\n        self.trainer = trainer\n        self.max_critical_length = self.context_length + self.batch_size\n        self.min_critical_length = self.context_length + 1\n        self.consciousVectors = []\n    def enqueue(self, consciousBlock: ConsciousBlock, clear: bool = False):\n        # change that into some tensor first.\n        # BUG 3: consciousBlock has tensor output but not numpy ndarray\n        # FIX 3: find and replace all \"consciousBlock.to_nparray()\" with \"consciousBlock.to_tensor()\"\n        #         print(consciou"
        },
        {
            "comment": "This function is appending conscious vectors to a queue, and if the maximum critical length is reached, it trains the model. If \"clear\" is True, it clears the queue instead of training.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1016-1041",
            "content": "sBlock)\n        #         print(type(consciousBlock))\n        consciousVector = consciousBlock.to_tensor()\n        logger.debug(\n            \"CONSCIOUS VECTOR [TYPE: %s SHAPE: %s]\",\n            type(consciousVector),\n            consciousVector.shape,\n        )\n        self.consciousVectors.append(consciousVector)\n        if not clear:\n            # BUG 5: no \"max_critical_point\"\n            # FIX 5: replace it with \"max_critical_length\"\n            if len(self.consciousVectors) == self.max_critical_length:\n                self.train()\n        else:\n            self.clear()\n    def train(self, clear: bool = False):\n        # TODO: use `ConsciousFlow` to replace logic here.\n        # train the model and clear the queue.\n        # size of queue before: self.context_length+self.batch_size (should be? at least geq to self.length+1)\n        # size of queue after training: self.context_length\n        if len(self.consciousVectors) >= self.min_critical_length:\n            batch_size = len(self.consciousVectors) - s"
        },
        {
            "comment": "The code segment is part of a class that appears to be working with conscious vectors and flows. It initializes batched_input and batched_output by slicing self.consciousVectors into chunks based on the context length. These chunks are then passed to the trainer's step method for further processing. If clear is not set, it then removes some elements from self.consciousVectors.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1041-1072",
            "content": "elf.context_length\n            # BUG 6: missing self.\n            # FIX 6: adding self.\n            # BUG 7: torch.Tensor conversion error\n            # FIX 7: change \"torch.Tensor([])\" into einops.pack\n            #             print(self.consciousVectors)\n            batched_input = ConsciousStream(\n                consciousFlows=[\n                    ConsciousFlow(\n                        consciousBlocks=self.consciousVectors[\n                            i : i + self.context_length\n                        ]\n                    )\n                    for i in range(batch_size)\n                ]\n            ).to_tensor()\n            batched_output = ConsciousFlow(\n                consciousBlocks=[\n                    self.consciousVectors[self.context_length + i]\n                    for i in range(batch_size)\n                ]\n            )\n            self.trainer.step(batched_input, batched_output)\n            if not clear:\n                # now remove some elements.\n                self.consciousVectors = s"
        },
        {
            "comment": "The code defines a class `SequentialEvalQueue` that handles evaluation tasks sequentially, has methods to add data (images and hidden activations), clear the queue, and resize images. It also includes a utility function `resizeImage()` to resize input images to desired size by preserving aspect ratio.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1072-1110",
            "content": "elf.consciousVectors[-self.context_length :]\n            else:\n                self.consciousVectors = []\n    def clear(self):\n        # check if anything left in queue. call at the end of queue.\n        self.train(clear=True)\n###############\n# IMAGE UTILS #\n###############\ndef resizeImage(im, desired_size):\n    # im = cv2.imread(im_pth)\n    old_size = im.shape[:2]  # old_size is in (height, width) format\n    ratio = float(desired_size) / max(old_size)\n    new_size = tuple([int(x * ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = desired_size - new_size[1]\n    delta_h = desired_size - new_size[0]\n    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n    left, right = delta_w // 2, delta_w - (delta_w // 2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(\n        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n    )\n    return new_im\nclass SequentialEvalQueue:\n    # loop: init data (visual+hid ac"
        },
        {
            "comment": "This code appears to be part of a larger program that involves image processing and model training. It seems to handle visual data, human/bot action data, and machine predictions. The code is designed to limit the maximum number of tokens for machine predictions per interval to 5, forcing it to end if necessary. It also suggests the possibility of using this as a continuous training basis or redesigning the model for skipping visual data blocks and performing descent only on selected areas.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1110-1136",
            "content": "tions) -> predict till end or limit -> update visual and action data\n    # visual data are inserted in a regular basis.\n    # human action data/bot action data will be inserted in between visual data.\n    # limit max machine predict token count per interval to 5. forcing it to end anyway.\n    # what about the machine trying to spit out some visual prediction?\n    # we just shadow it. (do not act! just dream. compare to current visual ground truth and perform gradient descent. maybe you can use that as continuous training basis? or redesign the model so it can choose to skip (by retrievable positional encoding) some blocks of visual data and perform descent only on selected area?)\n    ...\n################\n# READING DATA #\n################\ndesired_size = 224 * 4\n# get perspective width/height with:\n# pyautogui.size()\n# must be on the same machine.\n# with VideoCaptureContextManager(videoPath) as cap:\nfrom recording_train_parse import getTrainingData\nimport json\nimport re\nimport parse\nimport random\n# this pro"
        },
        {
            "comment": "This function trains a model using data from a database path, with optional shuffling for testing. It reads perspective width and height from the file \"video_record_script.sh\" in the basePath. If shuffle_for_test is true, it sets a random seed for shuffling. It then parses perspective size information from the data read from the file and returns trainingData.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1136-1162",
            "content": "cess is actually training it.\ndef trainModelWithDataBasePath(\n    basePath: str,\n    sequentialTrainingQueue: Enqueue,\n    shuffle_for_test: bool = False,\n    random_seed: int = 43  # pass hypothesis param here!\n    # sequentialTrainingQueue: SequentialTrainingQueue\n):\n    if shuffle_for_test:\n        random.seed(random_seed)\n    # read perspective width & height from basepath.\n    fpath = os.path.join(basePath, \"video_record_script.sh\")\n    with open(fpath, \"r\") as f:\n        data = f.read()\n        # data = json.load(f)\n        parse_target = re.finditer(r\"\\b\\d+x\\d+\\b\", data).__next__().group()\n        parsed_data = parse.parse(\n            \"{perspective_width:d}x{perspective_height:d}\", parse_target\n        )\n        if parsed_data:\n            perspective_width, perspective_height = (\n                parsed_data[\"perspective_width\"],\n                parsed_data[\"perspective_height\"],\n            )\n        else:\n            raise Exception(f\"Cannot parse perspective size from file: {fpath}\")\n    trainingData"
        },
        {
            "comment": "Creates a generator for training data and shuffles if necessary. Then, iterates through the generated frames, and if it's a hidden action type, shuffles actions within that frame, decodes keyboard actions if applicable, and creates an instance of HIDAction class.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1162-1185",
            "content": "Generator = getTrainingData(basePath)\n    if shuffle_for_test:\n        myGenerator = list(trainingDataGenerator)\n        random.shuffle(myGenerator)\n    for trainingDataFrame in trainingDataGenerator:\n        if trainingDataFrame.datatype == \"hid\":\n            encoded_actions = []\n            actions = trainingDataFrame.data[\"HIDEvents\"]\n            if shuffle_for_test:\n                random.shuffle(actions)\n            for action in actions:\n                action_type, action_args = action\n                if action_type in HIDActionBase.keyboard_action_types:\n                    action_args = HIDActionBase.uncover_keycode(action_args)\n                    if action_args is None:\n                        logger.warning(\"Skipping: %s\", action)\n                        continue\n                mHIDAction = HIDAction.from_action_json(\n                    [action_type, action_args],\n                    max_x=perspective_width,\n                    max_y=perspective_height,\n                )  # related to mouse c"
        },
        {
            "comment": "Creating conscious blocks from HID actions and image data.\nThe code is creating 'consciousBlocks' for both HID action and image data, if the datatype of trainingDataFrame is either \"HIDAction\" or \"image\". It converts the HID action to a numpy array, checks if it's the last encoded action in the list, creates the ConsciousBlock with the appropriate special_token, appends the encoded action to the sequentialTrainingQueue. For image data, resizes and rearranges the image, slices the image into multiple blocks of size 224x224 for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1185-1205",
            "content": "oordinates.\n                mHIDActionNDArray = mHIDAction.to_ndarray()\n                logger.debug(\"HID NDArray shape: %s\", mHIDActionNDArray.shape)\n                encoded_actions.append(mHIDActionNDArray)\n            for index, EA in enumerate(encoded_actions):\n                st = None\n                if index + 1 == len(encoded_actions):\n                    st = \"action_end\"\n                consciousBlock = ConsciousBlock(\n                    data_type=\"HIDAction\", special_token=st, action_data=EA\n                )\n                sequentialTrainingQueue.enqueue(consciousBlock)\n        elif trainingDataFrame.datatype == \"image\":\n            image = trainingDataFrame.data\n            image_resized = resizeImage(image, desired_size)\n            image_reshaped = einops.rearrange(image_resized, \"h w c -> c h w\")\n            #             image_reshaped = np.rollaxis(image_resized, 2, 0) # (3, 896, 896)\n            image_sliced = [\n                image_reshaped[:, x * 224 : (x + 1) * 224, y * 224 : (y"
        },
        {
            "comment": "Reshaping image, slicing into 16 chunks, creating ConsciousBlock objects with image data and special tokens.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1205-1231",
            "content": " + 1) * 224]\n                for x in range(4)\n                for y in range(4)\n            ]  # ) # c h w\n            # IMAGE RESHAPED: (896, 3, 896)?\n            # IMAGE RESHAPED: (896, 896, 3)\n            #             print('IMAGE RESHAPED:', image_reshaped.shape)\n            #             print('IMAGE SLICED:', image_sliced.shape)\n            #     (16, 3, 224, 224)\n            # hell?\n            consciousBlocks = []\n            for index, im in enumerate(image_sliced):\n                im = einops.rearrange(im, \"c h w -> (c h w)\")\n                st = None\n                if index == 15:\n                    st = \"image_end\"\n                elif index != 0 and (index + 1) % 4 == 0:\n                    st = \"image_newline\"\n                # BUG 4: tuple\n                # FIX 4: remove .to_tensor() method call\n                consciousBlock = ConsciousBlock(\n                    data_type=\"image\", special_token=st, image_data=im\n                )\n                if shuffle_for_test:\n                    con"
        },
        {
            "comment": "Appending consciousBlock to consciousBlocks list if not else enqueueing it to sequentialTrainingQueue.\nChecks if the datatype is correct and clears sequentialTrainingQueue if shuffle_for_test is True.\nEnsures consistency with recorder and actor by setting process DPI awareness.\nDefines a class CustomModel for the model.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1231-1265",
            "content": "sciousBlocks.append(consciousBlock)\n                else:\n                    #                 print(consciousBlock)\n                    sequentialTrainingQueue.enqueue(consciousBlock)\n            #             last_output = torch.zeros(1, output_size)\n            if shuffle_for_test:\n                for consciousBlock in consciousBlocks:\n                    sequentialTrainingQueue.enqueue(consciousBlock)\n            del image\n            del image_reshaped\n            del image_resized\n        else:\n            assert False, f\"wrong datatype: {trainingDataFrame.datatype}\"\n    sequentialTrainingQueue.clear()\n#########################################\n#  CONSISTENCY WITH RECORDER AND ACTOR  #\n#########################################\n# import ctypes\n# PROCESS_PER_MONITOR_DPI_AWARE = 2\n# ctypes.windll.shcore.SetProcessDpiAwareness(PROCESS_PER_MONITOR_DPI_AWARE)\n#########\n# MODEL #\n#########\n# notice: when in online mode only image will be backpropagated.\n# like using some upside down mirror.\nclass CustomModel(to"
        },
        {
            "comment": "The code defines a class called `CustomModel` that inherits from `torch.nn.Module`. It has an initializer method that takes `vit_model`, `hidden_size_vit`, and `vit_block_size` as input parameters. The class also includes `HIDEncoder` and `HIDDecoder` linear layers, and a `ViTDecoder` convolutional layer with specific in_channels, out_channels, and kernel_size values.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1265-1286",
            "content": "rch.nn.Module):\n    def __init__(self, vit_model, hidden_size_vit=1000, vit_block_size=228):\n        #     def __init__(self, rwkv_model, vit_model, tokenizer, hidden_size_rwkv, hidden_size_vit, output_size, vit_times = 4, vit_block_size=228):\n        super(CustomModel, self).__init__()\n        #         self.rwkv_model = rwkv_model # processing language, generate actions.\n        self.vit_model = vit_model\n        self.vit_block_size = vit_block_size  # this is default.\n        #         self.vit_times = vit_times\n        # seq2seq alike.\n        #         self.hidden_size = hidden_size_rwkv+hidden_size_vit\n        self.hidden_size = hidden_size_vit\n        self.HIDEncoder = torch.nn.Linear(HIDActionBase.length, 1000)\n        self.HIDDecoder = torch.nn.Linear(\n            1000, HIDActionBase.length\n        )  # use torch.where or something\n        # sparse matrix?\n        # some magic going elsewhere.\n        self.ViTDecoder = torch.nn.Conv2d(\n            in_channels=9, out_channels=3, kernel_size=22, "
        },
        {
            "comment": "This code defines a class that contains an LSTM (Long Short-Term Memory) network. The LSTM network takes in a tensor of data called \"conscious_stream\" which has a specific shape and type defined in the ConsciousBase class. If a target output is provided, the network will only optimize the data type bits, making image/action bits identical. Otherwise, it calculates all bits from the model. There are also fixes for Bug 9 and Fix 13 mentioned in comments.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1286-1312",
            "content": "stride=4, dilation=5\n        )  # n c h w\n        # FIX 13: change 1000 to ConsciousBase.data_type_length+ConsciousBase.special_token_length+1000\n        io_h_size = (\n            ConsciousBase.data_type_length + ConsciousBase.special_token_length + 1000\n        )\n        self.rnn = torch.nn.LSTM(\n            input_size=io_h_size, hidden_size=io_h_size, batch_first=True\n        )\n        # use tensor.\n    def forward(\n        self,\n        conscious_stream: torch.Tensor,\n        target_output: Union[None, torch.Tensor] = None,\n    ):\n        # if have target, and our datatype bits are wrong, we only optimize the datatype bits, making image/action bits identical\n        # otherwise just calculate all bits from the model.\n        # conscious_stream: [batch_size, (ConsciousBase.length) data_type+special_tokens+image_bits+action_bits]\n        # you need another pydantic model for it.\n        # BUG 9: input shape error\n        # FIX 9: check input and output shape\n        #         print(conscious_stream.shape,"
        },
        {
            "comment": "This code appears to be part of a neural network model that is processing data and extracting information from it. It's reshaping the input data, unpacking it into different components, and then performing actions on these different components based on their indexes. The purpose of this specific section may be to identify specific types of data within the input, such as images, special tokens, and actions.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1312-1334",
            "content": " ConsciousBase.length, )\n        #         if target_output is not None:\n        #             print(target_output.shape)\n        batch_size, seq_length, dim_block = conscious_stream.shape\n        assert dim_block == ConsciousBase.length\n        conscious_stream_reshaped = einops.rearrange(\n            conscious_stream, \"b s d -> (b s) d\"\n        )\n        datatype_bits, special_bits, image_bits, action_bits = einops.unpack(\n            conscious_stream_reshaped, [[s] for s in ConsciousBase.split_sizes], \"b *\"\n        )\n        #         datatype_bits, special_bits, image_bits, action_bits = size_splits(conscious_stream_reshaped, ConsciousBase.split_sizes,dim=1)\n        #         desired_size = self.vit_block_size*self.vit_times\n        datatypes = torch.argmax(datatype_bits, dim=1)\n        image_indexs = datatypes == 0\n        action_indexs = datatypes == 1\n        special_tokens = torch.argmax(special_bits, dim=1)\n        image_newline_indexs = special_tokens == 0\n        image_end_indexs = special_to"
        },
        {
            "comment": "Code creates a tensor with initial values of zeros for batch_size x seq_length x (data_type_length + special_token_length + 1000). It then sets certain elements to 1 based on image_indexs, action_indexs, etc.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1334-1366",
            "content": "kens == 1\n        action_end_indexs = special_tokens == 2\n        nop_indexs = special_tokens == 3\n        # 4+2+1000\n        # you are gonna take actions.\n        # prepare some zeros.\n        # BUG 10: len() on int\n        # FIX 10: find and fix incorrect len() calls\n        batched_rnn_input = torch.zeros(\n            (\n                batch_size * seq_length,\n                ConsciousBase.data_type_length\n                + ConsciousBase.special_token_length\n                + 1000,\n            )\n        )\n        batched_rnn_input[image_indexs, 0] = 1\n        batched_rnn_input[action_indexs, 1] = 1\n        # BUG 11: indexs not defined\n        # FIX 11: indexes -> indexs\n        batched_rnn_input[image_newline_indexs, 2] = 1\n        batched_rnn_input[image_end_indexs, 3] = 1\n        batched_rnn_input[action_end_indexs, 4] = 1\n        batched_rnn_input[nop_indexs, 5] = 1\n        # process this.\n        datatype_and_special_token_length = (\n            ConsciousBase.data_type_length + ConsciousBase.special_toke"
        },
        {
            "comment": "This code is preparing the input data for a Recurrent Neural Network (RNN) by selecting image and action bits, transforming them, and then reshaping the batched input. The code annotates unknown axes to fix bug 12, selects image and action bits based on indexes, processes the selected bits using a Vision Transformer model and HID Encoder respectively, and finally reshapes the batched RNN input for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1366-1394",
            "content": "n_length\n        )\n        # BUG 12: unannotated unknown axes in einops.rearrange\n        # FIX 12: annotate these axes\n        selected_image_bits = image_bits[image_indexs, :]\n        transformed_image_bits = einops.rearrange(\n            selected_image_bits,\n            \"b (c h w) -> b c h w\",\n            h=ConsciousBase.image_dim,\n            w=ConsciousBase.image_dim,\n            c=ConsciousBase.image_channels,\n        )\n        processed_image_bits = self.vit_model(transformed_image_bits)\n        batched_rnn_input[\n            image_indexs, datatype_and_special_token_length:\n        ] = processed_image_bits\n        selected_action_bits = action_bits[action_indexs, :]\n        processed_action_bits = self.HIDEncoder(selected_action_bits)\n        batched_rnn_input[\n            action_indexs, datatype_and_special_token_length:\n        ] = processed_action_bits\n        batched_rnn_input_reshaped = einops.rearrange(\n            batched_rnn_input, \"(b s) d -> b s d\", b=batch_size, s=seq_length\n        )\n      "
        },
        {
            "comment": "Unpacks RNN output, reshapes and assigns values to specific variables for data types, special tokens, and other data.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1394-1421",
            "content": "  # BUG 13: mismatched shape for rnn\n        _, (h1, c1) = self.rnn(batched_rnn_input_reshaped)\n        # shape of h1: [batch_size, dim_block]\n        if target_output is not None:\n            (\n                target_datatype_bits,\n                target_special_bits,\n                target_image_bits,\n                target_action_bits,\n            ) = einops.unpack(\n                target_output, [[s] for s in ConsciousBase.split_sizes], \"b *\"\n            )\n        output_datatype_bits, output_special_bits, output_data_bits = einops.unpack(\n            einops.rearrange(h1, \"b s d -> (b s) d\"),\n            [\n                [s]\n                for s in [\n                    ConsciousBase.data_type_length,\n                    ConsciousBase.special_token_length,\n                    1000,\n                ]\n            ],\n            \"b *\",\n        )\n        #         output_datatype_bits, output_special_bits, output_data_bits = size_splits(h1, [ConsciousBase.data_type_length, ConsciousBase.special_token_length"
        },
        {
            "comment": "This code calculates the common output indexes between target and actual outputs for image and action indexes.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1421-1445",
            "content": ", 1000] ,dim=1)\n        output_datatypes = torch.argmax(output_datatype_bits, dim=1)\n        output_image_indexs = output_datatypes == 0\n        output_action_indexs = output_datatypes == 1\n        if target_output is not None:\n            target_output_datatypes = torch.argmax(target_datatype_bits, dim=1)\n            target_output_image_indexs = target_output_datatypes == 0\n            target_output_action_indexs = target_output_datatypes == 1\n            common_output_image_indexs = torch.logical_and(\n                target_output_image_indexs, output_image_indexs\n            )\n            common_output_action_indexs = torch.logical_and(\n                target_output_action_indexs, output_action_indexs\n            )\n            common_output_indexs = torch.logical_or(\n                common_output_image_indexs, common_output_action_indexs\n            )\n            target_exclusive_output_image_indexs = torch.logical_and(\n                target_output_image_indexs, torch.logical_not(output_image_indexs"
        },
        {
            "comment": "This code is part of a larger neural network. It checks if the target output is not None, and based on that, selects either common or specific output image indexes to process. Then it repeats and rearranges the selected image bits using Einops library. After this, it passes these processed image bits through a ViTDecoder module and continues processing in the next chunk of code.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1445-1469",
            "content": ")\n            )\n            target_exclusive_output_action_indexs = torch.logical_and(\n                target_output_action_indexs, torch.logical_not(output_action_indexs)\n            )\n            target_exclusive_output_indexs = torch.logical_or(\n                target_exclusive_output_image_indexs,\n                target_exclusive_output_action_indexs,\n            )\n        if target_output is not None:\n            selected_output_image_bits = output_data_bits[common_output_image_indexs, :]\n        else:\n            selected_output_image_bits = output_data_bits[output_image_indexs, :]\n        processed_output_image_bits = einops.repeat(\n            selected_output_image_bits, \"b d -> b d 9\"\n        )\n        processed_output_image_bits = einops.einsum(\n            processed_output_image_bits,\n            processed_output_image_bits,\n            \"b h c, b w c -> b c h w\",\n        )\n        processed_output_image_bits = self.ViTDecoder(processed_output_image_bits)\n        processed_output_image_bits = eino"
        },
        {
            "comment": "Rearranging processed output image bits and preparing blank output.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1469-1494",
            "content": "ps.rearrange(\n            processed_output_image_bits, \"b c h w -> b (c h w)\"\n        )\n        if target_output is not None:\n            selected_output_action_bits = output_data_bits[\n                common_output_action_indexs, :\n            ]\n        else:\n            selected_output_action_bits = output_data_bits[output_action_indexs, :]\n        processed_output_action_bits = self.HIDDecoder(selected_output_action_bits)\n        # preparing blank output\n        output_0 = torch.zeros((batch_size, ConsciousBase.length))\n        (\n            output_datatype_bits_0,\n            output_special_token_bits_0,\n            output_image_bits_0,\n            output_action_bits_0,\n        ) = einops.unpack(output_0, [[s] for s in ConsciousBase.split_sizes], \"b *\")\n        #         output_datatype_bits_0, output_special_token_bits_0, output_image_bits_0, output_action_bits_0 = size_splits(output, ConsciousBase.split_sizes, dim=1)\n        if target_output is not None:\n            output_datatype_bits_0[\n          "
        },
        {
            "comment": "Copying target data to output arrays based on exclusive and common indices.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1494-1515",
            "content": "      target_exclusive_output_indexs, :\n            ] = target_datatype_bits[target_exclusive_output_indexs, :]\n            output_datatype_bits_0[common_output_indexs, :] = output_datatype_bits[\n                common_output_indexs, :\n            ]\n            output_special_token_bits_0[\n                target_exclusive_output_indexs, :\n            ] = output_special_bits[target_exclusive_output_indexs, :]\n            output_special_token_bits_0[common_output_indexs, :] = output_special_bits[\n                common_output_indexs, :\n            ]\n            output_special_token_bits_0[common_output_image_indexs, 2] = 0\n            output_special_token_bits_0[common_output_action_indexs, :2] = 0\n            output_image_bits_0[\n                target_exclusive_output_image_indexs, :\n            ] = target_image_bits[target_exclusive_output_image_indexs, :]\n            output_action_bits_0[\n                target_exclusive_output_action_indexs, :\n            ] = target_action_bits[target_exclusive_outpu"
        },
        {
            "comment": "This code is assigning values to different parts of the output based on a condition. If the condition is true, it assigns processed_output_image_bits and processed_output_action_bits to their respective locations in the output arrays. Otherwise, it sets certain bits to 0 and then concatenates output_datatype_bits_0, output_special_token_bits_0, output_image_bits_0, and output_action_bits_0 into the final output using einops.pack.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1515-1543",
            "content": "t_action_indexs, :]\n            output_image_bits_0[\n                common_output_image_indexs, :\n            ] = processed_output_image_bits\n            output_action_bits_0[\n                common_output_action_indexs, :\n            ] = processed_output_action_bits\n        else:\n            output_datatype_bits_0 = output_datatype_bits\n            output_special_bits_0 = output_special_bits\n            output_special_bits_0[output_image_indexs, 2] = 0\n            output_special_bits_0[output_action_indexs, :2] = 0\n            output_image_bits_0[output_image_indexs, :] = processed_output_image_bits\n            output_action_bits_0[output_action_indexs, :] = processed_output_action_bits\n        output, _ = einops.pack(\n            (\n                output_datatype_bits_0,\n                output_special_token_bits_0,\n                output_image_bits_0,\n                output_action_bits_0,\n            ),\n            \"b *\",\n        )\n        #         output = torch.concat((output_datatype_bits_0, output_sp"
        },
        {
            "comment": "Performs 1D convolution with given filters and input data, returning the output.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/conscious_struct.py\":1543-1545",
            "content": "ecial_token_bits_0, output_image_bits_0, output_action_bits_0), dim=1)\n        return output"
        }
    ]
}