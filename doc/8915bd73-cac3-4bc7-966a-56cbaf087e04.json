{
    "summary": "The code imports necessary libraries, initializes a GPT2 model, defines functions for token building and generation, generates binary input, masks attention mask, adds custom special tokens to tokenizer, resizes token embeddings, trains the model by masking logits during forward pass, calculates loss using masked logits, computes gradients and optimizes them, and uses a symbol/reference variable for future reference.",
    "details": [
        {
            "comment": "This code imports necessary libraries, initializes a GPT2 language model, and defines functions for building special tokens and generating token pairs. It also generates a binary input and masks the attention mask to be all ones.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py\":0-38",
            "content": "#!/usr/bin/env python\n# coding: utf-8\n# In[34]:\n# a good reference:\n# https://blog.paperspace.com/generating-text-summaries-gpt-2/\nget_ipython().system('pip3 install einops')\nimport einops\nimport transformers\nimport torch\nMODEL_NAME = 'gpt2'\nmodel = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\ntokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\nlr = 0.0001\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nloss_fn = torch.nn.CrossEntropyLoss()\ndef build_special_token(location:str, name:str):\n    return f\"<|{location}_{name}|>\"\ndef generate_special_token_pair(name:str):\n    begin_token = build_special_token('begin', name)\n    end_token = build_special_token('end', name)\n    return begin_token, end_token\ntext = \"8e7d4f\"\n# text = \"0100100010010\"\nenc = tokenizer([text], return_tensors='pt')\ninput_ids = enc['input_ids'] # into three pieces only.\nattention_mask = torch.ones(input_ids.shape, dtype=torch.int64)\ninput_ids.shape\nbegin_bytes, end_bytes = generate_special_token_pair('bytes')\n"
        },
        {
            "comment": "This code is adding custom special tokens to a tokenizer, mapping binary and hexadecimal values to existing tokens in the tokenizer's vocabulary. This allows the model to process input data that includes binary or hexadecimal information. The code then resizes the token embeddings based on the updated vocabulary size and logs some shapes of output arrays for further analysis.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py\":38-93",
            "content": "# how to step in and out?\ntokenizer(begin_bytes)['input_ids'], tokenizer(end_bytes)['input_ids']\n# In[35]:\n# print(dir(tokenizer))\n# print(tokenizer.all_special_tokens)\n# help(tokenizer.add_tokens)\ntokenizer.add_tokens([begin_bytes, end_bytes]) # will not do this again.\n# tokenizer.add_special_tokens({\"begin_bytes\": begin_bytes, \"end_bytes\":end_bytes})\n# In[36]:\n# add new special token to tokenizer\nlen(tokenizer)\n# In[37]:\nmodel.resize_token_embeddings(len(tokenizer))\n# In[38]:\n# dir(tokenizer)\n# tokenizer.vocab\n# binary_vocab = {i: format(i, '04b') for i in range(16)}\n# binary_map = {v: tokenizer.vocab[v] for _, v in binary_vocab.items()}\n# missing: 0011\nhex_vocab = {i: format(i, '0x') for i in range(16)}\nhex_map = {v: tokenizer.vocab[v] for _, v in hex_vocab.items()}\n# hex_map\n# In[39]:\nbyte_vocab = {i: str(i) for i in range(256)}\nbyte_map = {v: tokenizer.vocab[v] for _, v in byte_vocab.items()}\n# In[46]:\noutput.logits.shape # now: 50259\n# <|begin_bytes|>feffd3d7ea<|end_bytes|>\n# <|begin_hex|>feffd3d7ea<|end_he"
        },
        {
            "comment": "This code is training a model by masking some logits during the forward pass. It first creates a tensor of zeros for the masked logits and selects specific elements to preserve from the original output logits. Then, it calculates the loss using these masked logits. The loss is then printed, and the gradients are computed and optimized.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py\":93-140",
            "content": "x|>\n# .............##########[#..........] <- in training we only mask some prob\n# .............####################### <- in inference/parsing there could be state rolling back\n# In[41]:\n# training\noutput = model(input_ids = input_ids, attention_mask = attention_mask)\n# output.logits[:,:,:] = 0 # this will not affect loss\nmasked_logits = torch.zeros(output.logits.shape)\nfocused_ids = [10,20,30]\nmasked_logits[:,:,focused_ids] = output.logits[:,:,focused_ids] # this will\nzero_input_ids = torch.zeros(input_ids.shape, dtype=input_ids.dtype)\n# output.logits\nreshaped_original_logits = einops.rearrange(output.logits, \"b s c -> b c s\")\nreshaped_logits = einops.rearrange(masked_logits, \"b s c -> b c s\")\nloss = loss_fn(reshaped_original_logits, zero_input_ids)\n# loss = loss_fn(reshaped_logits, zero_input_ids)\nprint(loss.item()) # it would be the same as long as setting to zero.\n# In[42]:\nmasked_logits[:,:,focused_ids]\n# In[43]:\nmodel.zero_grad()\n# In[44]:\nloss.backward()\noptimizer.step()\nmodel.zero_grad()\n# In[45]:\n# in"
        },
        {
            "comment": "This code snippet seems to be a reference variable or symbol, possibly indicating a memory location or value that will be used later in the program.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/binary_program_synthesis_cpu_assembly_execution/gpt-binary-training.py\":140-140",
            "content": "ference"
        }
    ]
}