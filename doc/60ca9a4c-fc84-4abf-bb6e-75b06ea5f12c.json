{
    "summary": "The code utilizes the LitEllm library for user input, incorporates AI model processing to generate replies, includes command parsing, retries, sleep delay, and executes commands from a specified port at random intervals.",
    "details": [
        {
            "comment": "Code is importing necessary libraries and defining a function to generate and refresh a command pool for executing various tasks. The TODO comments indicate potential future improvements or features to be added to the codebase.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":0-27",
            "content": "# TODO: replace this unstable api with another\n# TODO: make sure the terminal service is always alive, sometimes the service is not responsive because of commands like \">8\"\n# TODO: insert & execute random commands\n# TODO: build multi-agent framework\n# TODO: memory framework, cache & permanent storage\n# TODO: use image to ascii protocol (with ocr) for gui manipulation\n# TODO: get terminal size\n# TODO: specify which part of response is executed, and which is not\n# TODO: match error info with commands which cause problems\n# TODO: do not clear the previous command execution records, instead keep a limited few and refresh\n# TODO: create some interface to describe what commands does, or narriation.\nimport ollama_utils\n# llama2 is not intelligent enough to complete this task.\n# still, we can build framework upon this.\nfrom terminal_config import cols, rows\nimport ast\ngenerate_command_pool = lambda: {\n    \"executed\": [],\n    \"not_executed\": [],\n    \"error\": [],\n}\ndef refresh_command_pool(command_pool, limit=3):\n   "
        },
        {
            "comment": "This code generates a random command pool and escapes the text. It then returns a dictionary containing a limited number of commands from the pool. The code also prints the server port, defines functions to escape and unescape text, initializes an openrouter model, and generates single and multiple random commands.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":27-74",
            "content": " ret = {}\n    for k, v in command_pool.items():\n        new_v = v[-limit:]\n        ret[k] = new_v\n    return ret\nprev_command_pool = generate_command_pool()\ndef unescape(text: str):\n    text = ast.literal_eval(repr(text).replace(\"\\\\\\\\\", \"\\\\\"))\n    return text\ndef escape(text: str):\n    text = text.encode(\"unicode_escape\").decode()\n    return text\nimport litellm\nimport base64\nimport requests\nfrom port_util import port\nprint(\"using server on port %d\" % port)\nopenrouter_model_name = \"mistralai/mistral-7b-instruct\"\ncmd_prefix = \"type \"\nimport random\ndef generate_single_random_command(_min, _max):\n    cmd = \"\"\n    rng = lambda: random.randint(0, 255)\n    for _ in range(random.randint(_min, _max)):\n        cmd += chr(rng())\n    cmd = escape(cmd)\n    return cmd_prefix + cmd\ndef random_command_generator(_min=5, _max=10, min_count=1, max_count=3):\n    count = random.randint(min_count, max_count)\n    cmdlist = []\n    for _ in range(count):\n        cmd = generate_single_random_command(_min, _max)\n        cmdlist.append(cmd)\n    "
        },
        {
            "comment": "This code generates a prompt for a virtual editor, combining previous commands and random commands. It encourages experimentation while also displaying the consequences of previous actions.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":74-125",
            "content": "return cmdlist\n# it is bad to run random commands.\n# maybe you should listen to the advice at https://github.com/Significant-Gravitas/AutoGPT/issues/346\n# before it is too late.\n# we are just prototyping. why so serious.\n# trying random stuff!\n# let's create a virtual editor.\n# you just need to master the diff, the memory and the action\ndef prompt_gen(content, random_command_list):\n    random_command_repr = \"\\n\".join(random_command_list)\n    previous_executed_repr = \"\\n\".join(prev_command_pool[\"executed\"])\n    previous_error_repr = \"\\n\".join(prev_command_pool[\"error\"])\n    previous_not_executed_repr = \"\\n\".join(prev_command_pool[\"not_executed\"])\n    prompt = f\"\"\"\nTerminal environment:\n{content}\nTerminal size: {cols}x{rows}\nPrevious executed successfully:\n{previous_executed_repr}\nPrevious executed with error:\n{previous_error_repr}\nPrevious not executed:\n{previous_not_executed_repr}\nRandom commands:\n{random_command_repr}\nYour commands:\n\"\"\"\n    return prompt\nfrom diff_utils import diff_methods\nfrom typing import Lite"
        },
        {
            "comment": "This code defines a function `get_terminal_data` that retrieves data from a local server and applies a diff method to determine the changes. It also includes a `construct_prompt` function for generating prompts using random commands, and a `get_reply_from_chatgpt` function that uses an AI model to generate responses based on input content.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":125-164",
            "content": "ral\nprev_terminal_content = \"\"\ndef get_terminal_data(\n    port,\n    method: Literal[\n        \"git_style_diff\", \"char_diff\", \"line_indexed_diff\", \"no_diff\"\n    ] = \"line_indexed_diff\",\n):\n    global prev_terminal_content\n    r = requests.get(f\"http://localhost:{port}/display\")\n    terminal_content = r.text.strip()\n    procedure = diff_methods.get(method, lambda prev, _next: _next)\n    result = procedure(prev_terminal_content, terminal_content)\n    prev_terminal_content = terminal_content\n    return result\nEXEC_DELAY = 0.5\ndef construct_prompt(data):\n    random_command_list = random_command_generator()\n    prompt = prompt_gen(data, random_command_list)\n    return prompt, random_command_list\n# model_tag = \"openai/gpt-3.5-turbo\"\n# import functools\n# @functools.lru_cache(maxsize=100)\ndef get_reply_from_chatgpt(content: str, max_tokens=50):\n    # why you talk back to me! why are you so talktive!\n    messages = [{\"content\": content, \"role\": \"system\"}]\n    # messages = [{\"content\": content, \"role\": \"user\"}]\n    print(\"s"
        },
        {
            "comment": "This code snippet is handling user input, processing it with an AI model, and returning the output as a reply. It appears to be using the LitEllm library for handling the AI model calls. The code also includes functionality to parse command lists from user inputs and execute them.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":164-195",
            "content": "ending:\")\n    print(messages)\n    # openai call\n    # many info inside. you may want to take a look?\n    # response = litellm.completion(f\"openrouter/{openrouter_model_name}\", messages)\n    # response = litellm.completion(\"ollama/llama2\", messages, api_base=\"http://localhost:11434\")\n    response = litellm.completion(\n        \"ollama/autoexec\",\n        messages,\n        api_base=\"http://localhost:11434\",\n        max_tokens=max_tokens,\n    )\n    choices = response[\"choices\"]\n    reply_content = choices[0][\"message\"][\"content\"]\n    print(\"reply:\")\n    print(reply_content)\n    return reply_content\nimport ast\ndef parse_command_list(response):\n    command_list = []\n    for _line in response.split(\"\\n\"):\n        line = _line.lstrip()\n        if line.startswith(cmd_prefix):\n            command = line[len(cmd_prefix) :]\n            # command = ast.literal_eval(repr(line).replace(\"\\\\\\\\\",\"\\\\\"))\n            command = unescape(command)\n            command_list.append(command)\n            prev_command_pool[\"executed\"].append("
        },
        {
            "comment": "195-206: Generate command pool and prepare for executing commands\n207-218: Encode and execute a single command using requests module\n219-231: Execute command list with retries, print total commands, and sleep for execution delay",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":195-231",
            "content": "_line)\n        else:\n            prev_command_pool[\"not_executed\"].append(_line)\n    return command_list\ndef execute_command(command, port):\n    print(\"executing command:\", repr(command))\n    b64command = base64.b64encode(command.encode(\"utf-8\")).decode(\"utf-8\")\n    params = dict(b64type=b64command)\n    requests.get(f\"http://localhost:{port}/input\", params=params)\n    time.sleep(EXEC_DELAY)\ndef execute_command_list(command_list, port):\n    print(\"total commands:\", len(command_list))\n    for command in command_list:\n        execute_command(command, port)\nimport os\nprint(\"env:\", os.environ)\nimport time\nSLEEP_TIME = 3\nwhile True:\n    data = get_terminal_data(port)\n    prompt, random_commands = construct_prompt(data)\n    prev_command_pool = generate_command_pool()\n    # prev_command_pool = refresh_command_pool(prev_command_pool)\n    print(\"random commands:\", random_commands)\n    response = get_reply_from_chatgpt(prompt)\n    prev_command_pool[\"executed\"].extend(random_commands)\n    execute_command_list([c[len(\"type "
        },
        {
            "comment": "Reads random commands from file, parses command list, executes commands on specified port, and sleeps for a while before repeating.",
            "location": "\"/media/root/Toshiba XG3/works/cybergod_doc/src/containerized_chatgpt_agent/container_autoexec_example.py\":231-234",
            "content": "\") :] for c in random_commands], port)\n    command_list = parse_command_list(response)\n    execute_command_list(command_list, port)\n    time.sleep(SLEEP_TIME)"
        }
    ]
}